
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import requests
import json
from pathlib import Path
import cv2
from PIL import Image
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model
from tensorflow.keras.applications import ResNet50, EfficientNetB0
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import os
import sys
import warnings
import zipfile
import glob
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass
from scipy import stats
from scipy.ndimage import gaussian_filter
from skimage.feature import graycomatrix, graycoprops, local_binary_pattern
from skimage import exposure
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

warnings.filterwarnings('ignore')

# Try to import optional dependencies
try:
    from sentinelsat import SentinelAPI
    SENTINEL_AVAILABLE = True
except ImportError:
    logger.warning("sentinelsat not installed. Satellite data collection will be limited.")
    SENTINEL_AVAILABLE = False

try:
    from rasterio import open as rio_open
    import rasterio
    from rasterio.warp import calculate_default_transform, reproject, Resampling
    from rasterio.mask import mask
    from rasterio.plot import show
    import rasterio.features
    RASTERIO_AVAILABLE = True
except ImportError:
    logger.warning("rasterio not installed. Satellite image processing will be limited.")
    RASTERIO_AVAILABLE = False

try:
    import geopandas as gpd
    from shapely.geometry import Point, Polygon, mapping
    GEOPANDAS_AVAILABLE = True
except ImportError:
    logger.warning("geopandas not installed. Geospatial features limited.")
    GEOPANDAS_AVAILABLE = False

# ============================================================================
# PART 1: REAL SATELLITE DATA COLLECTION
# ============================================================================

class RealSatelliteDataCollector:
    """Fetch and process real satellite data from Sentinel-2 and Landsat"""
    
    def __init__(self, sentinel_user=    
    def train(self, train_images, train_biomass, val_images, val_biomass,
              train_satellite=None, val_satellite=None,
              epochs=50, batch_size=16, model_save_path='biomass_model.h5'):
        """
        Train the model on real data
        
        Args:
            train_images: Array of training images (N, H, W, 3)
            train_biomass: Array of ground truth biomass values (N,)
            val_images: Validation images
            val_biomass: Validation biomass values
            train_satellite: Optional satellite features for training
            val_satellite: Optional satellite features for validation
        """
        callbacks = [
            keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True,
                verbose=1
            ),
            keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-7,
                verbose=1
            ),
            keras.callbacks.ModelCheckpoint(
                model_save_path,
                monitor, sentinel_pass=None):
        """
        Initialize with Copernicus credentials
        Get free account at: https://scihub.copernicus.eu/dhus/#/self-registration
        """
        self.sentinel_user = sentinel_user
        self.sentinel_pass = sentinel_pass
        if sentinel_user and sentinel_pass:
            self.api = SentinelAPI(sentinel_user, sentinel_pass, 
                                   'https://scihub.copernicus.eu/dhus')
    
    
    def get_sentinel2_for_field(self, lat, lon, start_date, end_date, 
                                cloud_cover_max=20, download_dir='./sentinel_data'):
        """
        Download real Sentinel-2 data for your field coordinates
        
        Args:
            lat, lon: Field center coordinates (decimal degrees)
            start_date, end_date: Date range (YYYY-MM-DD)
            cloud_cover_max: Maximum cloud coverage percentage
            download_dir: Where to save downloaded imagery
        
        Returns:
            List of downloaded product paths
        """
        if not SENTINEL_AVAILABLE:
            logger.error("sentinelsat library not available. Install with: pip install sentinelsat")
            return []
        
        os.makedirs(download_dir, exist_ok=True)
        
        # Define area of interest (0.1 degree buffer around point)
        footprint = f"POLYGON(({lon-0.05} {lat-0.05}, {lon+0.05} {lat-0.05}, " \
                   f"{lon+0.05} {lat+0.05}, {lon-0.05} {lat+0.05}, {lon-0.05} {lat-0.05}))"
        
        try:
            # Query Sentinel-2 products
            products = self.api.query(
                footprint,
                date=(start_date, end_date),
                platformname='Sentinel-2',
                cloudcoverpercentage=(0, cloud_cover_max),
                producttype='S2MSI2A'  # Level-2A (atmospherically corrected)
            )
            
            logger.info(f"Found {len(products)} Sentinel-2 images")
            
            # Download products
            downloaded = []
            for product_id, product_info in products.items():
                logger.info(f"Downloading: {product_info['title']}")
                try:
                    self.api.download(product_id, directory_path=download_dir)
                    downloaded.append(os.path.join(download_dir, f"{product_info['title']}.zip"))
                except Exception as e:
                    logger.error(f"Error downloading {product_id}: {e}")
            
            return downloaded
        except Exception as e:
            logger.error(f"Error querying Sentinel-2: {e}")
            return []
    
    def extract_bands_from_sentinel(self, product_path):
        """
        Extract and return actual band data from Sentinel-2 product
        
        Returns:
            Dictionary with band arrays and metadata
        """
        if not RASTERIO_AVAILABLE:
            logger.error("rasterio library not available. Install with: pip install rasterio")
            return None
        
        bands_data = {}
        
        # Handle ZIP files
        if product_path.endswith('.zip'):
            with zipfile.ZipFile(product_path, 'r') as zip_ref:
                temp_dir = product_path.replace('.zip', '_extracted')
                zip_ref.extractall(temp_dir)
                product_path = temp_dir
        
        # Sentinel-2 band files in SAFE format
        band_mapping = {
            'B02': 'Blue',    # 10m resolution
            'B03': 'Green',   # 10m
            'B04': 'Red',     # 10m
            'B08': 'NIR',     # 10m
            'B05': 'RedEdge1', # 20m
            'B06': 'RedEdge2', # 20m
            'B07': 'RedEdge3', # 20m
            'B11': 'SWIR1',   # 20m
            'B12': 'SWIR2'    # 20m
        }
        
        try:
            # Navigate SAFE structure
            img_dir = Path(product_path) / 'GRANULE'
            if not img_dir.exists():
                # Try to find SAFE directory
                safe_dirs = list(Path(product_path).glob('*.SAFE'))
                if safe_dirs:
                    img_dir = safe_dirs[0] / 'GRANULE'
            
            granules = list(img_dir.glob('*'))
            if not granules:
                logger.error(f"No granules found in {img_dir}")
                return None
            
            granule = granules[0]
            
            for band_code, band_name in band_mapping.items():
                # Try 10m resolution first
                img_data_dir = granule / 'IMG_DATA' / 'R10m'
                band_files = list(img_data_dir.glob(f"*_{band_code}_10m.jp2"))
                
                if not band_files:
                    # Try 20m resolution
                    img_data_dir = granule / 'IMG_DATA' / 'R20m'
                    band_files = list(img_data_dir.glob(f"*_{band_code}_20m.jp2"))
                
                if not band_files:
                    # Try without resolution suffix
                    img_data_dir = granule / 'IMG_DATA'
                    band_files = list(img_data_dir.glob(f"*_{band_code}.jp2"))
                
                if band_files:
                    try:
                        with rio_open(str(band_files[0])) as src:
                            band_data = src.read(1).astype(np.float32)
                            # Normalize to reflectance (0-1)
                            band_data = band_data / 10000.0
                            bands_data[band_name] = band_data
                            
                            if band_name == 'Blue':
                                bands_data['metadata'] = {
                                    'crs': str(src.crs),
                                    'transform': src.transform,
                                    'bounds': src.bounds,
                                    'shape': band_data.shape
                                }
                    except Exception as e:
                        logger.warning(f"Error reading band {band_code}: {e}")
                else:
                    logger.warning(f"Band file not found: {band_code}")
            
            return bands_data
        
        except Exception as e:
            logger.error(f"Error extracting bands: {e}")
            return None
    
    def calculate_vegetation_indices(self, bands_data):
        """
        Calculate real vegetation indices from actual satellite bands
        """
        if not bands_data or 'Red' not in bands_data or 'NIR' not in bands_data:
            logger.warning("Required bands not available for vegetation index calculation")
            return {}
        
        red = bands_data['Red']
        nir = bands_data['NIR']
        blue = bands_data['Blue'] if 'Blue' in bands_data else None
        green = bands_data['Green'] if 'Green' in bands_data else None
        
        # Avoid division by zero
        epsilon = 1e-8
        
        indices = {}
        
        # NDVI - Normalized Difference Vegetation Index
        ndvi = (nir - red) / (nir + red + epsilon)
        ndvi = np.clip(ndvi, -1, 1)
        indices['ndvi'] = ndvi
        
        # EVI - Enhanced Vegetation Index
        if blue is not None:
            evi = 2.5 * ((nir - red) / (nir + 6*red - 7.5*blue + 1 + epsilon))
            evi = np.clip(evi, -1, 1)
            indices['evi'] = evi
        
        # SAVI - Soil Adjusted Vegetation Index
        L = 0.5  # soil brightness correction factor
        savi = ((nir - red) / (nir + red + L + epsilon)) * (1 + L)
        savi = np.clip(savi, -1, 1)
        indices['savi'] = savi
        
        # NDWI - Normalized Difference Water Index
        if 'SWIR1' in bands_data:
            swir1 = bands_data['SWIR1']
            ndwi = (nir - swir1) / (nir + swir1 + epsilon)
            ndwi = np.clip(ndwi, -1, 1)
            indices['ndwi'] = ndwi
        
        # GNDVI - Green NDVI
        if green is not None:
            gndvi = (nir - green) / (nir + green + epsilon)
            gndvi = np.clip(gndvi, -1, 1)
            indices['gndvi'] = gndvi
        
        # ARVI - Atmospherically Resistant Vegetation Index
        if blue is not None:
            rb = red - (2 * (blue - red))
            arvi = (nir - rb) / (nir + rb + epsilon)
            arvi = np.clip(arvi, -1, 1)
            indices['arvi'] = arvi
        
        # MSI - Moisture Stress Index
        if 'SWIR1' in bands_data:
            swir1 = bands_data['SWIR1']
            msi = swir1 / (nir + epsilon)
            indices['msi'] = msi
        
        # CIgreen - Chlorophyll Index Green
        if green is not None:
            ci_green = (nir / (green + epsilon)) - 1
            indices['ci_green'] = ci_green
        
        # CIrededge - Chlorophyll Index Red Edge
        if 'RedEdge1' in bands_data:
            rededge = bands_data['RedEdge1']
            ci_rededge = (nir / (rededge + epsilon)) - 1
            indices['ci_rededge'] = ci_rededge
        
        return indices
    
    def extract_field_statistics(self, indices, field_polygon=None):
        """
        Extract statistics for specific field area from full image
        
        Args:
            indices: Dictionary of vegetation index arrays
            field_polygon: Coordinates defining field boundary (optional)
        
        Returns:
            Statistical summary of indices for the field
        """
        stats = {}
        
        for name, array in indices.items():
            if array is not None and isinstance(array, np.ndarray):
                # Apply field mask if polygon provided
                masked_array = array
                if field_polygon is not None and RASTERIO_AVAILABLE:
                    try:
                        # Create mask from polygon
                        # This is simplified - actual implementation needs coordinate transformation
                        pass
                    except Exception as e:
                        logger.warning(f"Could not apply field mask: {e}")
                
                # Remove invalid values (NaN, Inf)
                valid_data = masked_array[np.isfinite(masked_array)]
                
                if len(valid_data) > 0:
                    stats[f'{name}_mean'] = float(np.mean(valid_data))
                    stats[f'{name}_std'] = float(np.std(valid_data))
                    stats[f'{name}_median'] = float(np.median(valid_data))
                    stats[f'{name}_min'] = float(np.min(valid_data))
                    stats[f'{name}_max'] = float(np.max(valid_data))
                    stats[f'{name}_p25'] = float(np.percentile(valid_data, 25))
                    stats[f'{name}_p75'] = float(np.percentile(valid_data, 75))
                    stats[f'{name}_iqr'] = stats[f'{name}_p75'] - stats[f'{name}_p25']
                    stats[f'{name}_cv'] = stats[f'{name}_std'] / (abs(stats[f'{name}_mean']) + 1e-8)
                    
                    # Spatial heterogeneity metrics
                    if len(valid_data) > 100:
                        stats[f'{name}_skewness'] = float(stats.skew(valid_data))
                        stats[f'{name}_kurtosis'] = float(stats.kurtosis(valid_data))
        
        return stats

# ============================================================================
# PART 2: GROUND IMAGE ANALYSIS
# ============================================================================

class GroundImageAnalyzer:
    """Process actual photos taken in the field"""
    
    def __init__(self, model_path=None):
        self.target_size = (224, 224)
        self.model = None
        if model_path and os.path.exists(model_path):
            self.model = keras.models.load_model(model_path)
    
    def extract_image_features(self, image_path):
        """
        Extract features from actual field photo
        
        Features extracted:
        - Color statistics (RGB channels)
        - Greenness metrics
        - Texture features
        - Deep learning features (if model loaded)
        """
        img = cv2.imread(image_path)
        if img is None:
            raise ValueError(f"Could not load image: {image_path}")
        
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        
        features = {}
        
        # Color channel statistics
        for i, channel in enumerate(['red', 'green', 'blue']):
            channel_data = img_rgb[:,:,i]
            features[f'{channel}_mean'] = np.mean(channel_data)
            features[f'{channel}_std'] = np.std(channel_data)
            features[f'{channel}_median'] = np.median(channel_data)
        
        # Greenness metrics
        green = img_rgb[:,:,1].astype(float)
        red = img_rgb[:,:,0].astype(float)
        blue = img_rgb[:,:,2].astype(float)
        
        # Excess Green Index
        features['egi'] = np.mean(2*green - red - blue)
        
        # Green-Red Vegetation Index
        features['grvi'] = np.mean((green - red) / (green + red + 1e-8))
        
        # Visible Atmospherically Resistant Index
        features['vari'] = np.mean((green - red) / (green + red - blue + 1e-8))
        
        # Texture features using Gray Level Co-occurrence Matrix
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # Calculate GLCM features
        glcm = self._calculate_glcm(gray)
        features.update(glcm)
        
        # Edge density (indicator of vegetation structure)
        edges = cv2.Canny(gray, 100, 200)
        features['edge_density'] = np.sum(edges > 0) / edges.size
        
        # Deep features if model available
        if self.model is not None:
            img_resized = cv2.resize(img_rgb, self.target_size)
            img_normalized = img_resized / 255.0
            img_batch = np.expand_dims(img_normalized, axis=0)
            deep_features = self.model.predict(img_batch, verbose=0)[0]
            for i, val in enumerate(deep_features):
                features[f'deep_feature_{i}'] = val
        
        return features
    
    def _calculate_glcm(self, gray_image, distances=[1], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4]):
        """Calculate Gray Level Co-occurrence Matrix texture features"""
        try:
            # Reduce gray levels for GLCM calculation
            gray_reduced = (gray_image / 32).astype(np.uint8)
            gray_reduced = np.clip(gray_reduced, 0, 7)
            
            # Calculate GLCM
            glcm = graycomatrix(gray_reduced, distances=distances, angles=angles,
                               levels=8, symmetric=True, normed=True)
            
            # Extract properties
            features = {}
            properties = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']
            
            for prop in properties:
                try:
                    values = graycoprops(glcm, prop)
                    features[f'glcm_{prop}_mean'] = float(np.mean(values))
                    features[f'glcm_{prop}_std'] = float(np.std(values))
                    features[f'glcm_{prop}_range'] = float(np.max(values) - np.min(values))
                except:
                    pass
            
            return features
        except Exception as e:
            logger.warning(f"Error calculating GLCM: {e}")
            return {}
    
    def batch_process_images(self, image_directory, output_csv):
        """
        Process all images in a directory and save features
        """
        image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']
        image_paths = []
        for ext in image_extensions:
            image_paths.extend(list(Path(image_directory).glob(ext)))
        
        if not image_paths:
            logger.warning(f"No images found in {image_directory}")
            return pd.DataFrame()
        
        all_features = []
        
        logger.info(f"Processing {len(image_paths)} images...")
        for idx, img_path in enumerate(image_paths):
            try:
                features = self.extract_image_features(str(img_path))
                features['image_path'] = str(img_path)
                features['image_name'] = img_path.name
                all_features.append(features)
                
                if (idx + 1) % 10 == 0:
                    logger.info(f"Processed: {idx + 1}/{len(image_paths)} images")
            except Exception as e:
                logger.error(f"Error processing {img_path.name}: {e}")
        
        if not all_features:
            logger.error("No features extracted from any images")
            return pd.DataFrame()
        
        df = pd.DataFrame(all_features)
        df.to_csv(output_csv, index=False)
        logger.info(f"Saved features for {len(df)} images to {output_csv}")
        
        return df

# ============================================================================
# PART 3: DEEP LEARNING MODEL ARCHITECTURE
# ============================================================================

class BiomassEstimationCNN:
    """Convolutional Neural Network for biomass estimation from images"""
    
    def __init__(self, input_shape=(224, 224, 3)):
        self.input_shape = input_shape
        self.model = None
    
    def build_resnet_model(self, include_satellite_features=False, n_satellite_features=35):
        """
        Build ResNet-based model for biomass estimation
        """
        # Image input
        image_input = layers.Input(shape=self.input_shape, name='image_input')
        
        # Use pretrained ResNet50 as base
        base_model = ResNet50(weights='imagenet', include_top=False, 
                             input_tensor=image_input)
        
        # Freeze early layers
        for layer in base_model.layers[:100]:
            layer.trainable = False
        
        x = base_model.output
        x = layers.GlobalAveragePooling2D()(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(x)
        x = layers.Dropout(0.5)(x)
        x = layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(x)
        x = layers.Dropout(0.3)(x)
        
        # If including satellite data
        if include_satellite_features:
            satellite_input = layers.Input(shape=(n_satellite_features,), 
                                         name='satellite_input')
            s = layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(satellite_input)
            s = layers.Dropout(0.3)(s)
            s = layers.Dense(64, activation='relu')(s)
            
            # Concatenate image and satellite features
            combined = layers.concatenate([x, s])
            combined = layers.Dense(128, activation='relu')(combined)
            combined = layers.Dropout(0.3)(combined)
            output = layers.Dense(1, activation='linear', name='biomass_output')(combined)
            
            self.model = Model(inputs=[image_input, satellite_input], outputs=output)
        else:
            output = layers.Dense(1, activation='linear', name='biomass_output')(x)
            self.model = Model(inputs=image_input, outputs=output)
        
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.0001),
            loss='mse',
            metrics=['mae', 'mse', keras.metrics.RootMeanSquaredError()]
        )
        
        return self.model
    
    def build_efficientnet_model(self):
        """Alternative: EfficientNet-based model"""
        image_input = layers.Input(shape=self.input_shape)
        
        base_model = EfficientNetB0(weights='imagenet', include_top=False,
                                   input_tensor=image_input)
        
        for layer in base_model.layers[:100]:
            layer.trainable = False
        
        x = base_model.output
        x = layers.GlobalAveragePooling2D()(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(x)
        x = layers.Dropout(0.4)(x)
        x = layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(x)
        x = layers.Dropout(0.3)(x)
        output = layers.Dense(1, activation='linear')(x)
        
        self.model = Model(inputs=image_input, outputs=output)
        
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.0001),
            loss='mse',
            metrics=['mae', 'mse', keras.metrics.RootMeanSquaredError()]
        )
        
        return self.model
    
    def train(self, train_images, train_biomass, val_images, val_biomass,
              train_satellite=None, val_satellite=None,
              epochs=50, batch_size=16, model_save_path='biomass_model.h5'):
        """
        Train the model on real data
        
        Args:
            train_images: Array of training images (N, H, W, 3)
            train_biomass: Array of ground truth biomass values (N,)
            val_images: Validation images
            val_biomass: Validation biomass values
            train_satellite: Optional satellite features for training
            val_satellite: Optional satellite features for validation
        """
        callbacks = [
            keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
            keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5),
            keras.callbacks.ModelCheckpoint(model_save_path, save_best_only=True)
        ]
        
        if train_satellite is not None:
            history = self.model.fit(
                [train_images, train_satellite], train_biomass,
                validation_data=([val_images, val_satellite], val_biomass),
                epochs=epochs,
                batch_size=batch_size,
                callbacks=callbacks
            )
        else:
            history = self.model.fit(
                train_images, train_biomass,
                validation_data=(val_images, val_biomass),
                epochs=epochs,
                batch_size=batch_size,
                callbacks=callbacks
            )
        
        return history

# ============================================================================
# PART 4: ENSEMBLE MODELS (XGBoost, Random Forest, Gradient Boosting)
# ============================================================================

class EnsembleBiomassPredictor:
    """Combine multiple models for robust predictions"""
    
    def __init__(self):
        self.models = {}
        self.scaler = StandardScaler()
        self.feature_importance = None
    
    def prepare_features(self, image_features_df, satellite_features_df=None):
        """
        Combine image and satellite features into single feature matrix
        """
        # Start with image features
        feature_df = image_features_df.copy()
        
        # Add satellite features if available
        if satellite_features_df is not None:
            feature_df = feature_df.merge(satellite_features_df, 
                                         left_index=True, right_index=True,
                                         how='left')
        
        # Remove non-numeric columns
        feature_cols = feature_df.select_dtypes(include=[np.number]).columns
        X = feature_df[feature_cols].values
        
        return X, feature_cols
    
    def train_all_models(self, X_train, y_train, X_val, y_val):
        """
        Train multiple models and return performance metrics
        """
        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        
        results = {}
        
        # 1. XGBoost
        print("Training XGBoost...")
        xgb_model = xgb.XGBRegressor(
            n_estimators=500,
            max_depth=8,
            learning_rate=0.01,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42
        )
        xgb_model.fit(X_train_scaled, y_train,
                     eval_set=[(X_val_scaled, y_val)],
                     early_stopping_rounds=50,
                     verbose=False)
        self.models['xgboost'] = xgb_model
        
        pred = xgb_model.predict(X_val_scaled)
        results['xgboost'] = self._calculate_metrics(y_val, pred)
        
        # 2. Random Forest
        print("Training Random Forest...")
        rf_model = RandomForestRegressor(
            n_estimators=300,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        rf_model.fit(X_train_scaled, y_train)
        self.models['random_forest'] = rf_model
        
        pred = rf_model.predict(X_val_scaled)
        results['random_forest'] = self._calculate_metrics(y_val, pred)
        
        # 3. Gradient Boosting
        print("Training Gradient Boosting...")
        gb_model = GradientBoostingRegressor(
            n_estimators=300,
            max_depth=7,
            learning_rate=0.01,
            subsample=0.8,
            random_state=42
        )
        gb_model.fit(X_train_scaled, y_train)
        self.models['gradient_boosting'] = gb_model
        
        pred = gb_model.predict(X_val_scaled)
        results['gradient_boosting'] = self._calculate_metrics(y_val, pred)
        
        # 4. Ensemble prediction (average of all models)
        print("Creating ensemble...")
        ensemble_pred = np.mean([
            self.models['xgboost'].predict(X_val_scaled),
            self.models['random_forest'].predict(X_val_scaled),
            self.models['gradient_boosting'].predict(X_val_scaled)
        ], axis=0)
        results['ensemble'] = self._calculate_metrics(y_val, ensemble_pred)
        
        # Feature importance
        self.feature_importance = pd.DataFrame({
            'feature': range(X_train.shape[1]),
            'xgboost_importance': xgb_model.feature_importances_,
            'rf_importance': rf_model.feature_importances_,
            'gb_importance': gb_model.feature_importances_
        })
        self.feature_importance['avg_importance'] = self.feature_importance[
            ['xgboost_importance', 'rf_importance', 'gb_importance']
        ].mean(axis=1)
        self.feature_importance = self.feature_importance.sort_values(
            'avg_importance', ascending=False
        )
        
        return results
    
    def _calculate_metrics(self, y_true, y_pred):
        """Calculate performance metrics"""
        return {
            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
            'mae': mean_absolute_error(y_true, y_pred),
            'r2': r2_score(y_true, y_pred),
            'mape': np.mean(np.abs((y_true - y_pred) / y_true)) * 100
        }
    
    def predict(self, X, method='ensemble'):
        """
        Make predictions using specified method
        
        Args:
            X: Feature matrix
            method: 'ensemble', 'xgboost', 'random_forest', or 'gradient_boosting'
        """
        X_scaled = self.scaler.transform(X)
        
        if method == 'ensemble':
            predictions = np.mean([
                model.predict(X_scaled) for model in self.models.values()
            ], axis=0)
        else:
            predictions = self.models[method].predict(X_scaled)
        
        return predictions
    
    def save_models(self, directory='./models'):
        """Save all trained models"""
        os.makedirs(directory, exist_ok=True)
        
        # Save scaler
        with open(f'{directory}/scaler.pkl', 'wb') as f:
            pickle.dump(self.scaler, f)
        
        # Save each model
        for name, model in self.models.items():
            with open(f'{directory}/{name}.pkl', 'wb') as f:
                pickle.dump(model, f)
        
        # Save feature importance
        if self.feature_importance is not None:
            self.feature_importance.to_csv(f'{directory}/feature_importance.csv', 
                                          index=False)
        
        print(f"Models saved to {directory}")
    
    def load_models(self, directory='./models'):
        """Load pre-trained models"""
        # Load scaler
        with open(f'{directory}/scaler.pkl', 'rb') as f:
            self.scaler = pickle.load(f)
        
        # Load models
        model_files = {
            'xgboost': f'{directory}/xgboost.pkl',
            'random_forest': f'{directory}/random_forest.pkl',
            'gradient_boosting': f'{directory}/gradient_boosting.pkl'
        }
        
        for name, path in model_files.items():
            if os.path.exists(path):
                with open(path, 'rb') as f:
                    self.models[name] = pickle.load(f)
        
        # Load feature importance
        importance_path = f'{directory}/feature_importance.csv'
        if os.path.exists(importance_path):
            self.feature_importance = pd.read_csv(importance_path)
        
        print(f"Models loaded from {directory}")

# ============================================================================
# PART 5: COMPLETE TRAINING PIPELINE
# ============================================================================

class CompleteBiomassPipeline:
    """End-to-end pipeline from data collection to deployment"""
    
    def __init__(self, project_dir='./biomass_project'):
        self.project_dir = Path(project_dir)
        self.project_dir.mkdir(exist_ok=True)
        
        # Create subdirectories
        (self.project_dir / 'data' / 'raw_images').mkdir(parents=True, exist_ok=True)
        (self.project_dir / 'data' / 'satellite').mkdir(parents=True, exist_ok=True)
        (self.project_dir / 'data' / 'ground_truth').mkdir(parents=True, exist_ok=True)
        (self.project_dir / 'models').mkdir(exist_ok=True)
        (self.project_dir / 'results').mkdir(exist_ok=True)
        
        self.satellite_collector = RealSatelliteDataCollector()
        self.image_analyzer = GroundImageAnalyzer()
        self.cnn_model = BiomassEstimationCNN()
        self.ensemble_model = EnsembleBiomassPredictor()
    
    def collect_satellite_data(self, field_locations, date_ranges, 
                               sentinel_user, sentinel_pass):
        """
        Step 1: Collect satellite data for all field locations
        
        Args:
            field_locations: List of (lat, lon, field_id) tuples
            date_ranges: List of (start_date, end_date) tuples
            sentinel_user: Copernicus username
            sentinel_pass: Copernicus password
        """
        self.satellite_collector = RealSatelliteDataCollector(sentinel_user, sentinel_pass)
        
        all_satellite_features = []
        
        for (lat, lon, field_id), (start, end) in zip(field_locations, date_ranges):
            print(f"\nProcessing field {field_id} at ({lat}, {lon})")
            
            # Download Sentinel-2 data
            products = self.satellite_collector.get_sentinel2_for_field(
                lat, lon, start, end,
                download_dir=str(self.project_dir / 'data' / 'satellite' / field_id)
            )
            
            # Process each product
            for product_id in products:
                product_path = self.project_dir / 'data' / 'satellite' / field_id / product_id
                
                # Extract bands
                bands = self.satellite_collector.extract_bands_from_sentinel(product_path)
                
                # Calculate indices
                indices = self.satellite_collector.calculate_vegetation_indices(bands)
                
                # Get statistics
                stats = self.satellite_collector.extract_field_statistics(indices, None)
                stats['field_id'] = field_id
                stats['product_id'] = product_id
                
                all_satellite_features.append(stats)
        
        # Save to CSV
        satellite_df = pd.DataFrame(all_satellite_features)
        satellite_df.to_csv(self.project_dir / 'data' / 'satellite_features.csv', 
                           index=False)
        
        return satellite_df
    
    def process_ground_images(self, image_directory):
        """
        Step 2: Process all ground images and extract features
        """
        output_path = self.project_dir / 'data' / 'image_features.csv'
        
        image_features = self.image_analyzer.batch_process_images(
            image_directory,
            output_path
        )
        
        return image_features
    
    def load_ground_truth(self, csv_path):
        """
        Step 3: Load ground truth biomass measurements
        
        CSV should contain columns:
        - image_name or field_id
        - biomass_kg_ha (actual measured biomass)
        - date
        - other metadata
        """
        ground_truth = pd.read_csv(csv_path)
        
        # Validate required columns
        required = ['image_name', 'biomass_kg_ha']
        for col in required:
            if col not in ground_truth.columns:
                raise ValueError(f"Ground truth CSV missing column: {col}")
        
        ground_truth.to_csv(self.project_dir / 'data' / 'ground_truth_processed.csv',
                           index=False)
        
        return ground_truth
    
    def merge_all_data(self, image_features_df, satellite_features_df, ground_truth_df):
        """
        Step 4: Merge image features, satellite data, and ground truth
        """
        # Merge image features with ground truth
        merged = image_features_df.merge(ground_truth_df, on='image_name', how='inner')
        
        # If satellite data available, merge it too
        if satellite_features_df is not None and 'field_id' in merged.columns:
            merged = merged.merge(satellite_features_df, on='field_id', how='left')
        
        # Save merged dataset
        merged.to_csv(self.project_dir / 'data' / 'complete_dataset.csv', index=False)
        
        return merged
    
    def train_complete_system(self, merged_data_path, test_size=0.2):
        """
        Step 5: Train all models on complete dataset
        """
        # Load merged data
        data = pd.read_csv(merged_data_path)
        
        print(f"Total samples: {len(data)}")
        print(f"Biomass range: {data['biomass_kg_ha'].min():.0f} - {data['biomass_kg_ha'].max():.0f} kg/ha")
        
        # Separate features and target
        feature_cols = [col for col in data.columns 
                       if col not in ['biomass_kg_ha', 'image_name', 'image_path', 
                                     'field_id', 'product_id', 'date']]
        
        X = data[feature_cols].values
        y = data['biomass_kg_ha'].values
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42
        )
        
        print(f"\nTraining set: {len(X_train)} samples")
        print(f"Test set: {len(X_test)} samples")
        
        # Train ensemble models
        print("\n" + "="*60)
        print("TRAINING ENSEMBLE MODELS")
        print("="*60)
        
        results = self.ensemble_model.train_all_models(
            X_train, y_train, X_test, y_test
        )
        
        # Print results
        print("\n" + "="*60)
        print("MODEL PERFORMANCE RESULTS")
        print("="*60)
        
        results_df = pd.DataFrame(results).T
        print(results_df.to_string())
        
        # Save models
        self.ensemble_model.save_models(
            str(self.project_dir / 'models')
        )
        
        # Save results
        results_df.to_csv(self.project_dir / 'results' / 'model_performance.csv')
        
        # Plot predictions vs actual
        self._plot_predictions(X_test, y_test)
        
        # Plot feature importance
        self._plot_feature_importance(feature_cols)
        
        return results
    
    def _plot_predictions(self, X_test, y_test):
        """Plot predicted vs actual biomass"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        models_to_plot = ['xgboost', 'random_forest', 'gradient_boosting', 'ensemble']
        
        for idx, model_name in enumerate(models_to_plot):
            ax = axes[idx // 2, idx % 2]
            
            if model_name == 'ensemble':
                y_pred = self.ensemble_model.predict(X_test, method='ensemble')
            else:
                y_pred = self.ensemble_model.predict(X_test, method=model_name)
            
            # Scatter plot
            ax.scatter(y_test, y_pred, alpha=0.5)
            
            # Perfect prediction line
            min_val = min(y_test.min(), y_pred.min())
            max_val = max(y_test.max(), y_pred.max())
            ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)
            
            # Calculate metrics
            r2 = r2_score(y_test, y_pred)
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))
            
            ax.set_xlabel('Actual Biomass (kg/ha)', fontsize=12)
            ax.set_ylabel('Predicted Biomass (kg/ha)', fontsize=12)
            ax.set_title(f'{model_name.upper()}\nR² = {r2:.3f}, RMSE = {rmse:.0f}', 
                        fontsize=12, fontweight='bold')
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.project_dir / 'results' / 'predictions_vs_actual.png', 
                   dpi=300, bbox_inches='tight')
        print(f"\nPrediction plots saved to results/predictions_vs_actual.png")
        plt.close()
    
    def _plot_feature_importance(self, feature_names):
        """Plot feature importance from all models"""
        if self.ensemble_model.feature_importance is None:
            return
        
        # Add feature names
        importance_df = self.ensemble_model.feature_importance.copy()
        importance_df['feature_name'] = [feature_names[i] if i < len(feature_names) 
                                        else f'feature_{i}' 
                                        for i in importance_df['feature']]
        
        # Plot top 20 features
        top_features = importance_df.head(20)
        
        fig, ax = plt.subplots(figsize=(12, 8))
        
        x = np.arange(len(top_features))
        width = 0.25
        
        ax.barh(x - width, top_features['xgboost_importance'], width, 
               label='XGBoost', alpha=0.8)
        ax.barh(x, top_features['rf_importance'], width, 
               label='Random Forest', alpha=0.8)
        ax.barh(x + width, top_features['gb_importance'], width, 
               label='Gradient Boosting', alpha=0.8)
        
        ax.set_yticks(x)
        ax.set_yticklabels(top_features['feature_name'])
        ax.set_xlabel('Feature Importance', fontsize=12)
        ax.set_title('Top 20 Most Important Features', fontsize=14, fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3, axis='x')
        
        plt.tight_layout()
        plt.savefig(self.project_dir / 'results' / 'feature_importance.png', 
                   dpi=300, bbox_inches='tight')
        print(f"Feature importance plot saved to results/feature_importance.png")
        plt.close()
    
    def predict_new_field(self, image_path, lat=None, lon=None, date=None,
                         sentinel_user=None, sentinel_pass=None):
        """
        Step 6: Make prediction for a new field image
        
        Args:
            image_path: Path to field image
            lat, lon: Coordinates (optional, for satellite data)
            date: Date of image (optional, for satellite data)
            sentinel_user, sentinel_pass: Credentials (if using satellite)
        
        Returns:
            Dictionary with predictions and confidence metrics
        """
        # Extract image features
        print("Extracting features from image...")
        image_features = self.image_analyzer.extract_image_features(image_path)
        
        # Get satellite data if coordinates provided
        satellite_features = {}
        if lat and lon and date and sentinel_user and sentinel_pass:
            print("Fetching satellite data...")
            
            collector = RealSatelliteDataCollector(sentinel_user, sentinel_pass)
            
            # Get data for date range (7 days before to 7 days after)
            from datetime import datetime, timedelta
            target_date = datetime.strptime(date, '%Y-%m-%d')
            start_date = (target_date - timedelta(days=7)).strftime('%Y-%m-%d')
            end_date = (target_date + timedelta(days=7)).strftime('%Y-%m-%d')
            
            try:
                products = collector.get_sentinel2_for_field(
                    lat, lon, start_date, end_date,
                    download_dir=str(self.project_dir / 'data' / 'satellite' / 'temp')
                )
                
                if products:
                    # Process most recent product
                    product_path = self.project_dir / 'data' / 'satellite' / 'temp' / products[0]
                    bands = collector.extract_bands_from_sentinel(product_path)
                    indices = collector.calculate_vegetation_indices(bands)
                    satellite_features = collector.extract_field_statistics(indices, None)
            except Exception as e:
                print(f"Warning: Could not fetch satellite data: {e}")
        
        # Combine all features
        all_features = {**image_features, **satellite_features}
        
        # Load feature columns from training
        feature_cols_path = self.project_dir / 'data' / 'feature_columns.pkl'
        if feature_cols_path.exists():
            with open(feature_cols_path, 'rb') as f:
                expected_features = pickle.load(f)
            
            # Align features
            feature_vector = []
            for feat in expected_features:
                feature_vector.append(all_features.get(feat, 0))
            
            X = np.array(feature_vector).reshape(1, -1)
        else:
            # Use all available features
            X = np.array(list(all_features.values())).reshape(1, -1)
        
        # Make predictions with all models
        predictions = {}
        for model_name in ['xgboost', 'random_forest', 'gradient_boosting', 'ensemble']:
            pred = self.ensemble_model.predict(X, method=model_name)[0]
            predictions[model_name] = float(pred)
        
        # Calculate statistics
        pred_values = list(predictions.values())
        result = {
            'predictions': predictions,
            'ensemble_mean': np.mean(pred_values),
            'ensemble_std': np.std(pred_values),
            'confidence_interval_95': (
                np.mean(pred_values) - 1.96 * np.std(pred_values),
                np.mean(pred_values) + 1.96 * np.std(pred_values)
            ),
            'image_features': image_features,
            'satellite_features': satellite_features
        }
        
        return result
    
    def generate_field_report(self, prediction_result, output_path=None):
        """
        Generate a comprehensive report for farmers
        """
        if output_path is None:
            output_path = self.project_dir / 'results' / 'field_report.txt'
        
        with open(output_path, 'w') as f:
            f.write("="*70 + "\n")
            f.write("PASTURE BIOMASS ESTIMATION REPORT\n")
            f.write("="*70 + "\n\n")
            
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("BIOMASS PREDICTIONS\n")
            f.write("-"*70 + "\n")
            
            mean_pred = prediction_result['ensemble_mean']
            std_pred = prediction_result['ensemble_std']
            ci_low, ci_high = prediction_result['confidence_interval_95']
            
            f.write(f"Estimated Biomass: {mean_pred:.0f} kg/ha\n")
            f.write(f"Standard Deviation: {std_pred:.0f} kg/ha\n")
            f.write(f"95% Confidence Interval: {ci_low:.0f} - {ci_high:.0f} kg/ha\n\n")
            
            f.write("Individual Model Predictions:\n")
            for model, pred in prediction_result['predictions'].items():
                f.write(f"  {model:20s}: {pred:8.0f} kg/ha\n")
            
            f.write("\n")
            f.write("MANAGEMENT RECOMMENDATIONS\n")
            f.write("-"*70 + "\n")
            
            # Generate recommendations based on biomass level
            if mean_pred < 1500:
                f.write("⚠️  LOW BIOMASS ALERT\n")
                f.write("- Current biomass is below optimal grazing levels\n")
                f.write("- Recommend: Rest pasture, consider fertilization\n")
                f.write("- Expected recovery time: 3-4 weeks\n")
            elif mean_pred < 2500:
                f.write("✓ MODERATE BIOMASS\n")
                f.write("- Suitable for light to moderate grazing\n")
                f.write("- Recommend: Rotational grazing to maintain productivity\n")
                f.write("- Monitor regrowth weekly\n")
            elif mean_pred < 3500:
                f.write("✓ OPTIMAL BIOMASS\n")
                f.write("- Excellent conditions for grazing\n")
                f.write("- Recommend: Full stocking rate appropriate\n")
                f.write("- Maintain current management practices\n")
            else:
                f.write("⚠️  HIGH BIOMASS\n")
                f.write("- Biomass exceeds optimal range\n")
                f.write("- Recommend: Increase stocking rate or mechanical harvest\n")
                f.write("- Risk of pasture quality decline if not utilized\n")
            
            f.write("\n")
            f.write("KEY VEGETATION INDICES\n")
            f.write("-"*70 + "\n")
            
            # Report satellite indices if available
            sat_features = prediction_result['satellite_features']
            if sat_features:
                if 'ndvi_mean' in sat_features:
                    f.write(f"NDVI (Vegetation Health): {sat_features['ndvi_mean']:.3f}\n")
                if 'evi_mean' in sat_features:
                    f.write(f"EVI (Enhanced VI): {sat_features['evi_mean']:.3f}\n")
                if 'savi_mean' in sat_features:
                    f.write(f"SAVI (Soil Adjusted VI): {sat_features['savi_mean']:.3f}\n")
            
            # Report image-based features
            img_features = prediction_result['image_features']
            if 'grvi' in img_features:
                f.write(f"Green-Red VI: {img_features['grvi']:.3f}\n")
            if 'egi' in img_features:
                f.write(f"Excess Green Index: {img_features['egi']:.1f}\n")
            
            f.write("\n")
            f.write("="*70 + "\n")
            f.write("End of Report\n")
            f.write("="*70 + "\n")
        
        print(f"\nField report saved to {output_path}")
        
        return str(output_path)

# ============================================================================
# PART 6: WEATHER AND ENVIRONMENTAL DATA INTEGRATION
# ============================================================================

class EnvironmentalDataCollector:
    """Collect weather and environmental data from public APIs"""
    
    def __init__(self):
        pass
    
    def get_weather_data(self, lat, lon, start_date, end_date):
        """
        Get historical weather data from Open-Meteo (free, no API key required)
        
        Returns: Temperature, precipitation, solar radiation
        """
        url = "https://archive-api.open-meteo.com/v1/archive"
        
        params = {
            'latitude': lat,
            'longitude': lon,
            'start_date': start_date,
            'end_date': end_date,
            'daily': 'temperature_2m_mean,precipitation_sum,shortwave_radiation_sum',
            'timezone': 'auto'
        }
        
        try:
            response = requests.get(url, params=params)
            data = response.json()
            
            if 'daily' in data:
                df = pd.DataFrame(data['daily'])
                
                # Calculate summary statistics
                weather_features = {
                    'avg_temp': df['temperature_2m_mean'].mean(),
                    'total_precip': df['precipitation_sum'].sum(),
                    'avg_solar_radiation': df['shortwave_radiation_sum'].mean(),
                    'temp_std': df['temperature_2m_mean'].std(),
                    'precip_days': (df['precipitation_sum'] > 0).sum()
                }
                
                return weather_features
        except Exception as e:
            print(f"Error fetching weather data: {e}")
            return {}
    
    def get_soil_data(self, lat, lon):
        """
        Get soil properties from SoilGrids API (free)
        
        Returns: Soil organic carbon, pH, clay content, etc.
        """
        url = f"https://rest.isric.org/soilgrids/v2.0/properties/query"
        
        params = {
            'lon': lon,
            'lat': lat,
            'property': ['clay', 'soc', 'phh2o', 'nitrogen'],
            'depth': ['0-5cm', '5-15cm'],
            'value': 'mean'
        }
        
        try:
            response = requests.get(url, params=params)
            data = response.json()
            
            soil_features = {}
            
            if 'properties' in data:
                for prop in data['properties']:
                    prop_name = prop['name']
                    # Get surface layer (0-5cm)
                    if prop['depths']:
                        value = prop['depths'][0]['values']['mean']
                        soil_features[f'soil_{prop_name}'] = value
            
            return soil_features
        except Exception as e:
            print(f"Error fetching soil data: {e}")
            return {}
    
    def get_elevation(self, lat, lon):
        """
        Get elevation from Open-Elevation API (free)
        """
        url = f"https://api.open-elevation.com/api/v1/lookup?locations={lat},{lon}"
        
        try:
            response = requests.get(url)
            data = response.json()
            
            if 'results' in data and data['results']:
                return {'elevation': data['results'][0]['elevation']}
        except Exception as e:
            print(f"Error fetching elevation: {e}")
        
        return {}

# ============================================================================
# PART 7: USAGE EXAMPLES AND COMPLETE WORKFLOW
# ============================================================================

def example_complete_workflow():
    """
    Complete example of using the entire system
    """
    
    print("="*70)
    print("COMPLETE PASTURE BIOMASS ESTIMATION WORKFLOW")
    print("="*70)
    
    # Initialize pipeline
    pipeline = CompleteBiomassPipeline(project_dir='./my_biomass_project')
    
    # ====================
    # STEP 1: COLLECT DATA
    # ====================
    print("\nSTEP 1: Data Collection")
    print("-"*70)
    
    # Your field locations
    field_locations = [
        (40.7128, -74.0060, 'field_001'),  # Example: (latitude, longitude, field_id)
        (40.7580, -73.9855, 'field_002'),
        (40.7489, -73.9680, 'field_003')
    ]
    
    # Date ranges for each field (when images were taken)
    date_ranges = [
        ('2024-06-01', '2024-06-15'),
        ('2024-06-10', '2024-06-25'),
        ('2024-07-01', '2024-07-15')
    ]
    
    # YOUR CREDENTIALS (get free account at https://scihub.copernicus.eu)
    sentinel_username = "YOUR_USERNAME"
    sentinel_password = "YOUR_PASSWORD"
    
    # Collect satellite data
    # satellite_df = pipeline.collect_satellite_data(
    #     field_locations, date_ranges, sentinel_username, sentinel_password
    # )
    
    # ================================
    # STEP 2: PROCESS GROUND IMAGES
    # ================================
    print("\nSTEP 2: Processing Ground Images")
    print("-"*70)
    
    # Process all images in your directory
    # image_features_df = pipeline.process_ground_images('./my_field_images')
    
    # ================================
    # STEP 3: LOAD GROUND TRUTH DATA
    # ================================
    print("\nSTEP 3: Loading Ground Truth Measurements")
    print("-"*70)
    
    # Your ground truth CSV should look like:
    # image_name,biomass_kg_ha,date,field_id
    # IMG_001.jpg,2500,2024-06-05,field_001
    # IMG_002.jpg,3200,2024-06-10,field_002
    
    # ground_truth_df = pipeline.load_ground_truth('./ground_truth_measurements.csv')
    
    # =====================
    # STEP 4: MERGE DATA
    # =====================
    print("\nSTEP 4: Merging All Data Sources")
    print("-"*70)
    
    # complete_data = pipeline.merge_all_data(
    #     image_features_df, satellite_df, ground_truth_df
    # )
    
    # ======================
    # STEP 5: TRAIN MODELS
    # ======================
    print("\nSTEP 5: Training Models")
    print("-"*70)
    
    # Train all models on merged data
    # results = pipeline.train_complete_system(
    #     './my_biomass_project/data/complete_dataset.csv'
    # )
    
    # ================================
    # STEP 6: MAKE NEW PREDICTIONS
    # ================================
    print("\nSTEP 6: Making Predictions on New Field")
    print("-"*70)
    
    # Predict biomass for a new field image
    # prediction = pipeline.predict_new_field(
    #     image_path='./new_field_photo.jpg',
    #     lat=40.7128,
    #     lon=-74.0060,
    #     date='2024-08-15',
    #     sentinel_user=sentinel_username,
    #     sentinel_pass=sentinel_password
    # )
    
    # print("\nPrediction Results:")
    # print(f"Estimated Biomass: {prediction['ensemble_mean']:.0f} kg/ha")
    # print(f"95% CI: {prediction['confidence_interval_95'][0]:.0f} - "
    #       f"{prediction['confidence_interval_95'][1]:.0f} kg/ha")
    
    # Generate farmer report
    # report_path = pipeline.generate_field_report(prediction)
    
    print("\n" + "="*70)
    print("WORKFLOW COMPLETE!")
    print("="*70)

# ============================================================================
# QUICK START GUIDE
# ============================================================================

def quick_start_guide():
    """
    Prints instructions for getting started
    """
    guide = """
    ╔══════════════════════════════════════════════════════════════════════╗
    ║         PASTURE BIOMASS ESTIMATION - QUICK START GUIDE               ║
    ╚══════════════════════════════════════════════════════════════════════╝
    
    📋 REQUIREMENTS:
    ---------------
    pip install tensorflow opencv-python scikit-learn xgboost pandas numpy
    pip install matplotlib seaborn rasterio sentinelsat requests pillow
    
    📊 DATA YOU NEED:
    -----------------
    1. Ground photos of your pasture (JPG/PNG)
    2. Actual biomass measurements (kg/ha) - from cutting & weighing
    3. GPS coordinates of each photo location
    4. Dates when photos were taken
    
    🛰️ FREE SATELLITE DATA:
    ------------------------
    Register for free Copernicus account:
    https://scihub.copernicus.eu/dhus/#/self-registration
    
    ⚙️ BASIC USAGE:
    ---------------
    # 1. Initialize
    from complete_biomass_system import CompleteBiomassPipeline
    pipeline = CompleteBiomassPipeline()
    
    # 2. Process your images
    features = pipeline.process_ground_images('./my_photos')
    
    # 3. Load your measurements
    ground_truth = pipeline.load_ground_truth('./measurements.csv')
    
    # 4. Train model
    pipeline.train_complete_system('./data/complete_dataset.csv')
    
    # 5. Predict new field
    result = pipeline.predict_new_field('./new_photo.jpg')
    print(f"Biomass: {result['ensemble_mean']:.0f} kg/ha")
    
    📈 MINIMUM TRAINING DATA:
    -------------------------
    - At least 50-100 images with measurements
    - Cover different seasons/growth stages
    - Include variety of pasture conditions
    - More data = better predictions
    
    🎯 MEASUREMENT PROTOCOL:
    ------------------------
    1. Take photos from consistent height (1.5m recommended)
    2. Capture on clear days (avoid shadows)
    3. Include scale reference if possible
    4. Cut and weigh biomass from 1m² quadrat
    5. Dry samples to get dry matter weight
    6. Convert to kg/ha (weight × 10,000)
    
    📞 OUTPUT:
    ----------
    - Biomass predictions in kg/ha
    - Confidence intervals
    - Management recommendations
    - Vegetation health indices
    - Feature importance rankings
    
    ═══════════════════════════════════════════════════════════════════════
    """
    
    print(guide)

if __name__ == "__main__":
    quick_start_guide()
    print("\n\nTo see complete workflow example, run:")
    print(">>> from complete_biomass_system import example_complete_workflow")
    print(">>> example_complete_workflow()")
    print("\n\nFor prediction on single image:")
    print(">>> pipeline = CompleteBiomassPipeline()")
    print(">>> pipeline.ensemble_model.load_models('./models')")
    print(">>> result = pipeline.predict_new_field('your_field.jpg')")