#!/usr/bin/env python3
"""
COMPLETE UNIFIED AI MOVIE MAKER - NOVEMBER 29, 2025
ALL CODE FROM ALL YOUR FILES COMBINED - NOTHING REMOVED
Photorealistic + Pixar/GTA + All APIs + All Rendering Engines
"""

import asyncio
import os
import json
import time
import logging
import hashlib
import tempfile
import re
import subprocess
import textwrap
import sys
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field

# ==================== MCP IMPORTS ====================
try:
    from mcp.server.fastmcp import FastMCP
    from mcp.server import Server
    from mcp.server.stdio import stdio_server
    from mcp.types import Tool, TextContent, ToolResult
except ImportError:
    print("ERROR: Install MCP with: pip install mcp fastmcp", file=sys.stderr)
    exit(1)

# ==================== AI SDK IMPORTS ====================
import aiohttp
import requests
try:
    from elevenlabs import ElevenLabs
    import elevenlabs
except ImportError:
    print("pip install elevenlabs")
    
try:
    import openai
except ImportError:
    print("pip install openai")

logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")
logger = logging.getLogger("ULTIMATE-MOVIE-MAKER")
mcp = FastMCP("ultimate-ai-movie-maker-2025")

# ==================== COMPLETE UNIFIED CONFIG ====================
@dataclass
class Config:
    root: Path = Path(tempfile.gettempdir()) / "ULTIMATE_MOVIES_2025"
    temp_dir: Path = Path(tempfile.gettempdir()) / "REAL_MOVIES_2025"
    
    # RENDERING ENGINES (All three from your codes)
    unreal_path: str = os.getenv("UNREAL_ENGINE_PATH", "")
    maya_path: str = os.getenv("MAYA_PATH", "")
    blender_path: str = os.getenv("BLENDER_PATH", "blender")
    use_unreal: bool = True
    use_maya: bool = False
    
    # METAHUMAN
    metahuman_creator_email: str = os.getenv("METAHUMAN_EMAIL", "")
    metahuman_creator_pass: str = os.getenv("METAHUMAN_PASS", "")
    
    # ALL VIDEO GENERATION APIS
    KLING_KEY: str = os.getenv("KLING_API_KEY", "")
    KLING_API_KEY: str = os.getenv("KLING_API_KEY", "")
    SORA_KEY: str = os.getenv("SORA_API_KEY", "")
    LUMA_KEY: str = os.getenv("LUMA_API_KEY", "")
    RUNWAY_KEY: str = os.getenv("RUNWAYML_API_KEY", "")
    RUNWAYML_API_KEY: str = os.getenv("RUNWAYML_API_KEY", "")
    PIKA_KEY: str = os.getenv("PIKA_API_KEY", "")
    HAILUO_KEY: str = os.getenv("HAILUO_API_KEY", "")
    
    # 3D GENERATION
    MESHY_KEY: str = os.getenv("MESHY_API_KEY", "")
    MESHY_API_KEY: str = os.getenv("MESHY_API_KEY", "")
    TRIPO_KEY: str = os.getenv("TRIPO_API_KEY", "")
    TRIPO_API_KEY: str = os.getenv("TRIPO_API_KEY", "")
    
    # VOICE
    ELEVEN_KEY: str = os.getenv("ELEVENLABS_API_KEY", "")
    ELEVENLABS_API_KEY: str = os.getenv("ELEVENLABS_API_KEY", "")
    
    # AI PLANNING
    OPENAI_KEY: str = os.getenv("OPENAI_API_KEY", "")
    
    # ANIMATION
    DEEPMOTION_ID: str = os.getenv("DEEPMOTION_CLIENT_ID", "")
    DEEPMOTION_CLIENT_ID: str = os.getenv("DEEPMOTION_CLIENT_ID", "")
    DEEPMOTION_SEC: str = os.getenv("DEEPMOTION_SECRET", "")
    DEEPMOTION_SECRET: str = os.getenv("DEEPMOTION_SECRET", "")
    
    # RENDERING SETTINGS
    resolution: str = "1920x1080"
    resolution_4k: str = "3840x2160"
    default_resolution: Tuple[int, int] = (1920, 1080)
    fps: int = 24
    max_duration: int = 300
    
    # UE5.7 FEATURES
    use_nanite: bool = True
    use_lumen: bool = True
    use_megalights: bool = True
    use_path_tracing: bool = False
    enable_dof: bool = True
    enable_motion_blur: bool = True
    enable_film_grain: bool = True
    color_grading: str = "cinematic"
    
    def __post_init__(self):
        self.root.mkdir(parents=True, exist_ok=True)
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        
        # Auto-select best rendering engine
        if self.unreal_path and Path(self.unreal_path).exists():
            logger.info("ðŸŽ® Unreal Engine 5.7 detected - RALLY-LEVEL photorealism")
            self.use_unreal = True
        elif self.maya_path and Path(self.maya_path).exists():
            logger.info("ðŸŽ¨ Maya + Arnold detected - BOX ASSASSIN-LEVEL polish")
            self.use_maya = True
            self.use_unreal = False
        else:
            logger.info("ðŸŽ¬ Using Enhanced Blender Cycles - PHOTOREALISTIC mode")
            self.use_unreal = False
            self.use_maya = False
        
        # Validate APIs
        if not any([self.KLING_KEY, self.SORA_KEY, self.LUMA_KEY, self.RUNWAY_KEY]):
            logger.warning("âš ï¸  No photorealistic video APIs configured")
        
        if not self.MESHY_KEY and not self.TRIPO_KEY:
            logger.warning("âš ï¸  No 3D generation APIs configured")
        
        if not self.ELEVEN_KEY:
            logger.warning("âš ï¸  No ElevenLabs API key - voice disabled")

config = Config()

# ==================== RATE LIMITING ====================
LAST_MOVIE_TIME = 0
LAST_MOVIE = 0
REQUEST_TIMES = []

async def allow_movie() -> bool:
    """Limit to 1 movie per hour"""
    global LAST_MOVIE_TIME, LAST_MOVIE
    if time.time() - LAST_MOVIE_TIME < 3600:
        return False
    if time.time() - LAST_MOVIE < 3600:
        return False
    LAST_MOVIE_TIME = time.time()
    LAST_MOVIE = time.time()
    return True

async def can_make_request() -> bool:
    """Max 5 heavy requests per hour"""
    global REQUEST_TIMES
    now = time.time()
    REQUEST_TIMES = [t for t in REQUEST_TIMES if now - t < 3600]
    if len(REQUEST_TIMES) >= 5:
        return False
    REQUEST_TIMES.append(now)
    return True

# ==================== RATE LIMITER CLASS ====================
@dataclass
class RateLimitConfig:
    max_requests_per_minute: int = 5

class RateLimiter:
    def __init__(self, config: RateLimitConfig):
        self.config = config
        self.requests = []
    
    async def acquire(self) -> bool:
        now = time.time()
        self.requests = [t for t in self.requests if now - t < 60]
        if len(self.requests) >= self.config.max_requests_per_minute:
            return False
        self.requests.append(now)
        return True
    
    async def release(self):
        pass

rate_limiter = RateLimiter(RateLimitConfig())

# ==================== KLING AI CLIENT ====================
class KlingClient:
    """Kling AI 2.0/2.1 PRO - Best cinematography"""
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base = "https://api.klingai.com/v1"
    
    async def text_to_video(self, prompt: str, duration: int = 180, mode: str = "pro") -> str:
        """Generate video from text"""
        url = f"{self.base}/videos/text2video"
        headers = {"Authorization": f"Bearer {self.api_key}"}
        payload = {
            "prompt": f"Pixar GTA V cinematic cutscene: {prompt}",
            "duration": duration,
            "aspect_ratio": "16:9",
            "mode": mode,
            "cfg_scale": 0.5,
            "camera_control": "auto"
        }
        async with aiohttp.ClientSession() as s:
            async with s.post(url, json=payload, headers=headers) as r:
                data = await r.json()
                task_id = data["task_id"]
                logger.info(f"ðŸŽ¬ Kling task: {task_id}")
                
                # Poll for completion
                for i in range(300):
                    await asyncio.sleep(10)
                    async with s.get(f"{self.base}/videos/task/{task_id}", headers=headers) as r2:
                        res = await r2.json()
                        if res.get("status") == "completed":
                            logger.info("âœ… Kling video complete")
                            return res["video_url"]
                        elif res.get("status") == "failed":
                            raise Exception(f"Kling failed: {res.get('message')}")
                    if i % 6 == 0:
                        logger.info(f"â³ Kling: {i*10}s elapsed...")
                raise TimeoutError("Kling timeout")
    
    async def get_result(self, task_id: str):
        """Get task result"""
        url = f"{self.base}/videos/task/{task_id}"
        headers = {"Authorization": f"Bearer {self.api_key}"}
        async with aiohttp.ClientSession() as s:
            async with s.get(url, headers=headers) as r:
                data = await r.json()
                if data["status"] == "completed":
                    return data["video_url"]
                return None

# ==================== MESHY AI CLIENT ====================
class MeshyClient:
    """Meshy AI 4 - Photorealistic 3D characters"""
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base = "https://api.meshy.ai/v1"
    
    async def text_to_3d(self, prompt: str, art_style: str = "realistic") -> str:
        """Generate 3D model from text"""
        url = f"{self.base}/text-to-3d"
        headers = {"Authorization": f"Bearer {self.api_key}"}
        
        # Preview task (geometry)
        preview_payload = {
            "prompt": f"Pixar GTA style: {prompt}",
            "art_style": art_style,
            "enable_pbr": True,
            "rig": True,
            "target_polycount": 50000
        }
        
        async with aiohttp.ClientSession() as s:
            async with s.post(url, json=preview_payload, headers=headers) as r:
                data = await r.json()
                task_id = data["task_id"]
                logger.info(f"ðŸŽ¨ Meshy preview: {task_id}")
                
                while True:
                    await asyncio.sleep(8)
                    async with s.get(f"{self.base}/tasks/{task_id}", headers=headers) as r2:
                        res = await r2.json()
                        if res["status"] == "SUCCEEDED":
                            logger.info("âœ… Meshy 3D complete")
                            return res["result"]["model_urls"]["glb"]
                        elif res["status"] == "FAILED":
                            raise Exception("Meshy failed")
    
    def create_preview(self, prompt: str):
        """Sync method for preview"""
        return prompt
    
    def wait_for_completion(self, task_id: str):
        """Sync wait"""
        return {"model_urls": {"glb": ""}}
    
    def download_model(self, url: str, filename: str):
        """Download model"""
        return filename

# ==================== TRIPO3D CLIENT ====================
class TripoClient:
    """Tripo 3D - Fast fallback with Pixar style"""
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base = "https://api.tripo3d.ai/v1/openapi"
    
    async def text_to_3d(self, prompt: str) -> str:
        """Generate 3D model"""
        url = f"{self.base}/task"
        headers = {"Authorization": f"Bearer {self.api_key}"}
        payload = {
            "type": "text_to_model",
            "input": {"prompt": f"Pixar GTA character: {prompt}"}
        }
        async with aiohttp.ClientSession() as s:
            async with s.post(url, json=payload, headers=headers) as r:
                data = await r.json()
                task_id = data["task_id"]
                logger.info(f"ðŸŽ¨ Tripo fallback: {task_id}")
                
                while True:
                    await asyncio.sleep(10)
                    async with s.get(f"{self.base}/task/{task_id}", headers=headers) as r2:
                        res = await r2.json()
                        if res["status"] == "success":
                            logger.info("âœ… Tripo 3D complete")
                            return res["output"]["model_url"]

async def tripo_text_to_3d(prompt: str) -> str:
    """Standalone Tripo function"""
    if not config.TRIPO_KEY:
        raise Exception("No TRIPO_KEY")
    client = TripoClient(config.TRIPO_KEY)
    return await client.text_to_3d(prompt)

# ==================== RUNWAY CLIENT ====================
class RunwayClient:
    """Runway Gen-4 for video polishing and effects"""
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base = "https://api.dev.runwayml.com/v1"
    
    async def enhance_video(self, video_path: str, enhancements: str) -> str:
        """Enhance video with effects"""
        if not self.api_key:
            logger.warning("No RUNWAY_KEY - skipping enhancement")
            return video_path
        
        url = f"{self.base}/video-to-video"
        headers = {"Authorization": f"Bearer {self.api_key}"}
        
        logger.info("ðŸŽ¨ Runway: Enhancing video...")
        async with aiohttp.ClientSession() as s:
            with open(video_path, "rb") as f:
                form = aiohttp.FormData()
                form.add_field("video", f, filename="input.mp4")
                form.add_field("prompt", f"Enhance: {enhancements}")
                form.add_field("model", "gen4_turbo")
                
                async with s.post(url, headers=headers, data=form) as r:
                    data = await r.json()
                    task_id = data["id"]
                    
                    for _ in range(120):
                        await asyncio.sleep(5)
                        async with s.get(f"{self.base}/tasks/{task_id}", headers=headers) as r2:
                            res = await r2.json()
                            if res["status"] == "SUCCEEDED":
                                enhanced_url = res["output"]["url"]
                                enhanced_path = video_path.replace(".mp4", "_enhanced.mp4")
                                async with s.get(enhanced_url) as dl:
                                    Path(enhanced_path).write_bytes(await dl.read())
                                logger.info(f"âœ… Runway enhanced: {enhanced_path}")
                                return enhanced_path
                    
                    return video_path

# ==================== DEEPMOTION CLIENT ====================
class DeepMotionClient:
    """DeepMotion for realistic character animation"""
    def __init__(self, client_id: str, secret: str):
        self.client_id = client_id
        self.secret = secret
        self.base = "https://api.deepmotion.com/animate3d/v1"
    
    async def animate_character(self, model_path: str, action: str) -> Optional[str]:
        """Animate a 3D character"""
        if not self.client_id or not self.secret:
            logger.warning("No DeepMotion credentials - skipping animation")
            return None
        
        import base64
        auth = base64.b64encode(f"{self.client_id}:{self.secret}".encode()).decode()
        headers = {"Authorization": f"Basic {auth}"}
        
        logger.info(f"ðŸƒ DeepMotion: Animating {action}...")
        
        async with aiohttp.ClientSession() as s:
            with open(model_path, "rb") as f:
                form = aiohttp.FormData()
                form.add_field("model", f, filename="character.glb")
                form.add_field("prompt", action)
                
                async with s.post(f"{self.base}/animations", headers=headers, data=form) as r:
                    data = await r.json()
                    task_id = data["task_id"]
                    
                    for _ in range(60):
                        await asyncio.sleep(10)
                        async with s.get(f"{self.base}/animations/{task_id}", headers=headers) as r2:
                            res = await r2.json()
                            if res.get("status") == "completed":
                                anim_url = res["output"]["fbx_url"]
                                anim_path = model_path.replace(".glb", "_anim.fbx")
                                async with s.get(anim_url) as dl:
                                    Path(anim_path).write_bytes(await dl.read())
                                logger.info(f"âœ… DeepMotion animation: {anim_path}")
                                return anim_path
                    
                    return None

# ==================== AI ORCHESTRATOR ====================
class AIOrchestrator:
    """Orchestrates all AI services"""
    def __init__(self, config: Config):
        self.config = config
        self.kling = None
        self.meshy = None
        self.tripo = None
        self.eleven = None
        self.runway = None
    
    async def init_clients(self):
        """Initialize all API clients"""
        if self.config.KLING_KEY:
            self.kling = KlingClient(api_key=self.config.KLING_KEY)
        if self.config.MESHY_KEY:
            self.meshy = MeshyClient(api_key=self.config.MESHY_KEY)
        if self.config.TRIPO_KEY:
            self.tripo = TripoClient(api_key=self.config.TRIPO_KEY)
        if self.config.ELEVEN_KEY:
            self.eleven = ElevenLabs(api_key=self.config.ELEVEN_KEY)
        if self.config.RUNWAY_KEY:
            self.runway = RunwayClient(api_key=self.config.RUNWAY_KEY)
    
    async def generate_base_video(self, prompt: str, duration: int) -> str:
        """Kling AI 2.0: Gen 2-3 min Pixar-GTA video"""
        if not self.kling:
            await self.init_clients()
        video_url = await self.kling.text_to_video(prompt, duration)
        
        # Download video
        async with aiohttp.ClientSession() as s:
            async with s.get(video_url) as r:
                video_path = str(self.config.temp_dir / "base_video.mp4")
                Path(video_path).write_bytes(await r.read())
        return video_path
    
    async def generate_character(self, desc: str) -> str:
        """Meshy/Tripo: Gen rigged 3D char"""
        try:
            if self.meshy:
                glb_url = await self.meshy.text_to_3d(desc)
                async with aiohttp.ClientSession() as s:
                    async with s.get(glb_url) as r:
                        glb_path = str(self.config.temp_dir / "char.glb")
                        Path(glb_path).write_bytes(await r.read())
                return glb_path
        except Exception as e:
            logger.warning(f"Meshy failed: {e}, trying Tripo...")
        
        # Fallback to Tripo
        if self.tripo:
            model_url = await self.tripo.text_to_3d(desc)
            async with aiohttp.ClientSession() as s:
                async with s.get(model_url) as r:
                    glb_path = str(self.config.temp_dir / "char_tripo.glb")
                    Path(glb_path).write_bytes(await r.read())
            return glb_path
        
        raise Exception("No 3D generation service available")
    
    async def generate_voice(self, text: str, voice: str = "Rachel") -> str:
        """ElevenLabs: GTA-style voiceover"""
        if not self.eleven:
            await self.init_clients()
        
        audio = self.eleven.generate(
            text=text,
            voice=voice,
            model="eleven_multilingual_v2"
        )
        audio_path = str(self.config.temp_dir / "voice.mp3")
        with open(audio_path, "wb") as f:
            for chunk in audio:
                f.write(chunk)
        return audio_path
    
    async def enhance_motion(self, model_path: str, action: str) -> Optional[str]:
        """DeepMotion: Animate rigged model"""
        deepmotion = DeepMotionClient(
            self.config.DEEPMOTION_CLIENT_ID,
            self.config.DEEPMOTION_SECRET
        )
        return await deepmotion.animate_character(model_path, action)
    
    async def polish_video(self, video_path: str, enhancements: str) -> str:
        """RunwayML: Upscale/add effects"""
        if not self.runway:
            return video_path
        return await self.runway.enhance_video(video_path, enhancements)

orchestrator = AIOrchestrator(config)

# ==================== ENHANCED PARSER ====================
class EnhancedParser:
    @staticmethod
    def parse(text: str) -> Dict[str, Any]:
        """Parse natural language into film structure"""
        dialogue = re.findall(r'"([^"]+)"', text)
        return {
            "dialogue": dialogue,
            "prompt": text,
            "title": text[:70],
            "style": "photorealistic",
            "duration_seconds": 180,
            "scenes": [text],
            "characters": [{"name": "Hero", "description": text}],
            "props": [],
            "mood": "cinematic"
        }

# ==================== NATURAL LANGUAGE PARSER ====================
async def understand_idea(idea: str) -> Dict[str, Any]:
    """Use GPT-4 to parse natural language into film structure"""
    if config.OPENAI_KEY:
        try:
            client = openai.AsyncOpenAI(api_key=config.OPENAI_KEY)
            resp = await client.chat.completions.create(
                model="gpt-4o-mini",
                temperature=0.9,
                messages=[{
                    "role": "system",
                    "content": textwrap.dedent("""
                    You are a Hollywood director. Turn ANY human message into a full short film plan.
                    Output ONLY valid JSON with these exact keys:
                    {
                      "title": "string",
                      "style": "unreal_engine | pixar | cardboard_noir | gta_v | photoreal",
                      "duration_seconds": 120-300,
                      "scenes": ["scene 1", "scene 2", "..."],
                      "characters": [{"name": "Hero", "description": "female assassin with red hair"}],
                      "props": ["red sports car", "cardboard drone"],
                      "dialogue": ["This city is mine", "Not today"],
                      "mood": "rainy neon night, dramatic, explosions, slow-mo"
                    }
                    """)
                }, {
                    "role": "user",
                    "content": idea
                }]
            )
            return json.loads(resp.choices[0].message.content.strip("```json").strip("```"))
        except Exception as e:
            logger.warning(f"OpenAI parse failed: {e}")
    
    # Fallback parser
    return {
        "title": idea[:70],
        "style": "photorealistic",
        "duration_seconds": 180,
        "scenes": [idea],
        "characters": [{"name": "Hero", "description": idea}],
        "props": [],
        "dialogue": re.findall(r'"([^"]+)"', idea) or ["This city will burn."],
        "mood": "rainy neon night, cinematic, dramatic"
    }

# ==================== UNREAL ENGINE 5.7 RENDERER ====================
async def render_with_unreal(
    plan: Dict,
    base_video: Path,
    char_paths: List[str],
    voice_path: Path,
    workdir: Path
) -> Path:
    """
    Rally-level photorealism with Unreal Engine 5.7:
    - Lumen global illumination
    - Nanite virtualized geometry
    - MegaLights dynamic lighting
    - Path tracing for final quality
    """
    logger.info("ðŸŽ® Rendering with Unreal Engine 5.7 (Rally-style photorealism)")
    
    final_mp4 = workdir / "FINAL_MOVIE_UE5.mp4"
    ue_script = workdir / "ue5_render.py"
    
    ue_script.write_text(f'''
import unreal

# Get editor subsystem
editor = unreal.get_editor_subsystem(unreal.UnrealEditorSubsystem)

# Create new level
world = editor.new_level("/Game/GeneratedMovie")

# ===== ENABLE RALLY-LEVEL FEATURES =====
# Lumen (real-time global illumination)
unreal.SystemLibrary.execute_console_command(world, "r.DynamicGlobalIlluminationMethod 1")
unreal.SystemLibrary.execute_console_command(world, "r.ReflectionMethod 1")

# Nanite (film-quality geometry)
unreal.SystemLibrary.execute_console_command(world, "r.Nanite 1")

# MegaLights (UE 5.7 - thousands of dynamic lights)
unreal.SystemLibrary.execute_console_command(world, "r.MegaLights 1")

# Virtual shadow maps
unreal.SystemLibrary.execute_console_command(world, "r.Shadow.Virtual.Enable 1")

{"# Path tracing (offline quality)" if config.use_path_tracing else ""}
{"unreal.SystemLibrary.execute_console_command(world, 'r.PathTracing 1')" if config.use_path_tracing else ""}

# ===== IMPORT BASE VIDEO =====
video_plane = unreal.EditorLevelLibrary.spawn_actor_from_class(
    unreal.StaticMeshActor,
    unreal.Vector(0, 0, 0)
)
plane_mesh = unreal.EditorAssetLibrary.load_asset("/Engine/BasicShapes/Plane")
video_plane.get_component_by_class(unreal.StaticMeshComponent).set_static_mesh(plane_mesh)
video_plane.set_actor_scale3d(unreal.Vector(100, 100, 1))

# ===== IMPORT 3D CHARACTERS =====
{"".join([f'''
char_{i} = unreal.EditorAssetLibrary.import_asset("{p}", "/Game/Characters/Char_{i}")
char_actor_{i} = unreal.EditorLevelLibrary.spawn_actor_from_class(
    unreal.SkeletalMeshActor,
    unreal.Vector({i * 300}, 0, 0)
)
''' for i, p in enumerate(char_paths)])}

# ===== LIGHTING (Rally's moody look) =====
# Directional light (moon)
moon = unreal.EditorLevelLibrary.spawn_actor_from_class(
    unreal.DirectionalLight,
    unreal.Vector(0, 0, 1000)
)
moon.get_light_component().set_intensity(0.5)
moon.get_light_component().set_light_color(unreal.LinearColor(0.6, 0.7, 1.0))

# Point lights (neon, car headlights)
for i in range(20):
    light = unreal.EditorLevelLibrary.spawn_actor_from_class(
        unreal.PointLight,
        unreal.Vector(i * 200 - 2000, (i % 2) * 500 - 250, 150)
    )
    light.get_light_component().set_intensity(10000)
    colors = [
        unreal.LinearColor(0.2, 0.5, 1.0),  # Blue neon
        unreal.LinearColor(1.0, 0.4, 0.1),  # Orange
        unreal.LinearColor(1.0, 0.1, 0.2),  # Red
    ]
    light.get_light_component().set_light_color(colors[i % 3])

# ===== CAMERA (Cinematic) =====
camera = unreal.EditorLevelLibrary.spawn_actor_from_class(
    unreal.CameraActor,
    unreal.Vector(0, -1500, 600)
)
camera.get_camera_component().set_field_of_view(35)
camera.get_camera_component().filmback.sensor_width = 36  # Full frame

# Depth of field
settings = camera.get_camera_component().post_process_settings
settings.depth_of_field_focal_distance = 1500
settings.depth_of_field_fstop = 2.8
settings.depth_of_field_sensor_width = 36

# ===== POST PROCESS (Color grading) =====
post = unreal.EditorLevelLibrary.spawn_actor_from_class(
    unreal.PostProcessVolume,
    unreal.Vector(0, 0, 0)
)
post.unbound = True
pp_settings = post.settings
pp_settings.bloom_intensity = 1.5
pp_settings.vignette_intensity = 0.6
pp_settings.film_grain_intensity = 0.3

# Color grading based on style
color_style = "{plan.get('mood', 'cinematic')}".lower()
if "noir" in color_style or "dark" in color_style:
    pp_settings.color_saturation = unreal.Vector4(0.5, 0.5, 0.5, 1.0)
    pp_settings.color_contrast = unreal.Vector4(1.2, 1.2, 1.2, 1.0)
elif "teal" in color_style or "orange" in color_style:
    pp_settings.color_saturation = unreal.Vector4(1.2, 1.2, 1.2, 1.0)
else:
    pp_settings.color_saturation = unreal.Vector4(1.1, 1.1, 1.1, 1.0)

# ===== VOLUMETRIC FOG FOR ATMOSPHERE =====
fog = unreal.EditorLevelLibrary.spawn_actor_from_class(
    unreal.ExponentialHeightFog,
    unreal.Vector(0, 0, 0)
)
fog_component = fog.get_component_by_class(unreal.ExponentialHeightFogComponent)
fog_component.set_fog_density(0.02)
fog_component.volumetric_fog = True

# ===== SKY ATMOSPHERE =====
sky = unreal.EditorLevelLibrary.spawn_actor_from_class(
    unreal.SkyAtmosphere,
    unreal.Vector(0, 0, 0)
)

# ===== SEQUENCER FOR ANIMATION =====
level_sequence = unreal.AssetToolsHelpers.get_asset_tools().create_asset(
    "MovieSequence",
    "/Game/",
    unreal.LevelSequence,
    unreal.LevelSequenceFactoryNew()
)

# Set sequence length
level_sequence.set_display_rate(unreal.FrameRate(24, 1))
level_sequence.set_playback_end({plan.get("duration_seconds", 180) * 24})

# ===== MOVIE RENDER QUEUE =====
subsystem = unreal.get_editor_subsystem(unreal.MoviePipelineQueueSubsystem)
queue = subsystem.get_queue()
job = queue.allocate_new_job(unreal.MoviePipelineExecutorJob)
job.sequence = level_sequence
job.map = world

# Movie pipeline config
config = job.get_configuration()

# Output settings
output_setting = config.find_or_add_setting_by_class(unreal.MoviePipelineOutputSetting)
output_setting.output_directory = unreal.DirectoryPath("{workdir}")
output_setting.file_name_format = "FINAL_UE5_MOVIE"
output_setting.output_resolution = unreal.IntPoint(1920, 1080)

# Deferred render pass
deferred_pass = config.find_or_add_setting_by_class(unreal.MoviePipelineDeferredPassBase)
deferred_pass.disable_multisample_effects = False

# Anti-aliasing
aa_setting = config.find_or_add_setting_by_class(unreal.MoviePipelineAntiAliasingSetting)
aa_setting.spatial_sample_count = 4
aa_setting.temporal_sample_count = 4

# Video codec
video_setting = config.find_or_add_setting_by_class(unreal.MoviePipelineVideoOutputSetting)
video_setting.codec = unreal.MoviePipelineVideoCodec.H264
video_setting.quality = unreal.MoviePipelineVideoQuality.EPIC

# Execute render
executor = unreal.MoviePipelinePIEExecutor()
executor.execute(queue)

print("âœ… Unreal Engine 5.7 render complete:", "{final_mp4}")
''')
    
    # Execute Unreal Engine
    proc = await asyncio.create_subprocess_exec(
        config.unreal_path, "-ExecutePythonScript=" + str(ue_script)
    )
    await proc.wait()
    
    return final_mp4

# ==================== MAYA + ARNOLD RENDERER ====================
async def render_with_maya(
    plan: Dict,
    base_video: Path,
    char_paths: List[str],
    voice_path: Path,
    workdir: Path
) -> Path:
    """
    Box Assassin-level polish with Maya + Arnold:
    - Arnold ray tracing
    - Professional shaders
    - Polished stylized look
    """
    logger.info("ðŸŽ¨ Rendering with Maya + Arnold (Box Assassin-style polish)")
    
    final_mp4 = workdir / "FINAL_MOVIE_MAYA.mp4"
    maya_script = workdir / "maya_render.py"
    
    maya_script.write_text(f'''
import maya.cmds as cmds
import maya.mel as mel

# New scene
cmds.file(new=True, force=True)

# Set renderer to Arnold
cmds.setAttr("defaultRenderGlobals.currentRenderer", "arnold", type="string")

# Arnold settings (high quality)
cmds.setAttr("defaultArnoldRenderOptions.AASamples", 8)  # Anti-aliasing
cmds.setAttr("defaultArnoldRenderOptions.GIDiffuseSamples", 4)
cmds.setAttr("defaultArnoldRenderOptions.GISpecularSamples", 4)

# Resolution
cmds.setAttr("defaultResolution.width", 1920)
cmds.setAttr("defaultResolution.height", 1080)

# ===== IMPORT CHARACTERS =====
{"".join([f'''
cmds.file("{p}", i=True, type="glTF", namespace="char_{i}")

# Get imported objects
imported_objs = cmds.ls(namespace="char_{i}")
if imported_objs:
    # Position character
    char_root = imported_objs[0]
    cmds.setAttr(f"char_{i}:{{char_root}}.translateX", {i * 5})
    cmds.setAttr(f"char_{i}:{{char_root}}.translateZ", 0)
    
    # Create Arnold shader
    shader = cmds.shadingNode("aiStandardSurface", asShader=True, name=f"char_{i}_shader")
    shading_group = cmds.sets(renderable=True, noSurfaceShader=True, empty=True, name=f"char_{i}_SG")
    cmds.connectAttr(f"{{shader}}.outColor", f"{{shading_group}}.surfaceShader", force=True)
    
    # Apply shader to all meshes
    meshes = cmds.ls(imported_objs, type="mesh", long=True)
    if meshes:
        cmds.sets(meshes, edit=True, forceElement=shading_group)
    
    # Shader properties for photorealism
    cmds.setAttr(f"{{shader}}.baseColor", 0.8, 0.8, 0.8, type="double3")
    cmds.setAttr(f"{{shader}}.specular", 0.5)
    cmds.setAttr(f"{{shader}}.specularRoughness", 0.3)
    cmds.setAttr(f"{{shader}}.subsurface", 0.1)
    cmds.setAttr(f"{{shader}}.subsurfaceColor", 1.0, 0.8, 0.7, type="double3")
''' for i, p in enumerate(char_paths)])}

# ===== ENVIRONMENT =====
# Ground plane
ground = cmds.polyPlane(width=100, height=100, name="ground")
cmds.setAttr("ground.translateY", 0)

ground_shader = cmds.shadingNode("aiStandardSurface", asShader=True, name="ground_shader")
ground_sg = cmds.sets(renderable=True, noSurfaceShader=True, empty=True, name="ground_SG")
cmds.connectAttr("ground_shader.outColor", "ground_SG.surfaceShader", force=True)
cmds.sets(ground, edit=True, forceElement=ground_sg)
cmds.setAttr("ground_shader.baseColor", 0.3, 0.3, 0.3, type="double3")
cmds.setAttr("ground_shader.specularRoughness", 0.8)

# ===== CAMERA =====
camera = cmds.camera(name="renderCam")
cmds.setAttr(camera[0] + ".translateX", 0)
cmds.setAttr(camera[0] + ".translateY", -15)
cmds.setAttr(camera[0] + ".translateZ", 6)
cmds.setAttr(camera[0] + ".rotateX", 75)
cmds.setAttr(camera[1] + ".focalLength", 35)

# ===== THREE-POINT LIGHTING =====
# Key light
key_light = cmds.directionalLight(name="keyLight", intensity=2.0)
cmds.setAttr(key_light + ".translateX", 5)
cmds.setAttr(key_light + ".translateY", 5)
cmds.setAttr(key_light + ".translateZ", 10)
cmds.setAttr(key_light + ".color", 1.0, 0.9, 0.8, type="double3")

# Fill light
fill_light = cmds.directionalLight(name="fillLight", intensity=0.5)
cmds.setAttr(fill_light + ".translateX", -5)
cmds.setAttr(fill_light + ".translateY", 3)
cmds.setAttr(fill_light + ".translateZ", 8)
cmds.setAttr(fill_light + ".color", 0.7, 0.8, 1.0, type="double3")

# Rim light
rim_light = cmds.spotLight(name="rimLight", intensity=3.0, coneAngle=60)
cmds.setAttr(rim_light + ".translateX", 0)
cmds.setAttr(rim_light + ".translateY", -10)
cmds.setAttr(rim_light + ".translateZ", 12)

# ===== RENDER =====
# Set time range
start_frame = 1
end_frame = int({plan.get("duration_seconds", 180)} * 30)  # 30 fps
cmds.playbackOptions(minTime=start_frame, maxTime=end_frame, animationStartTime=start_frame, animationEndTime=end_frame)

# Render settings
cmds.setAttr("defaultRenderGlobals.startFrame", start_frame)
cmds.setAttr("defaultRenderGlobals.endFrame", end_frame)
cmds.setAttr("defaultRenderGlobals.byFrameStep", 1)
cmds.setAttr("defaultRenderGlobals.animation", 1)
cmds.setAttr("defaultRenderGlobals.putFrameBeforeExt", 1)
cmds.setAttr("defaultRenderGlobals.extensionPadding", 4)
cmds.setAttr("defaultRenderGlobals.imageFormat", 8)  # JPEG

# Arnold render settings
cmds.setAttr("defaultArnoldDriver.aiTranslator", "jpeg", type="string")
cmds.setAttr("defaultArnoldDriver.mergeAOVs", 1)

# Output path
output_path = "{workdir}/maya_frames/frame"
import os
os.makedirs("{workdir}/maya_frames", exist_ok=True)
cmds.setAttr("defaultRenderGlobals.imageFilePrefix", output_path, type="string")

# Batch render
cmds.arnoldRender(batch=True, sequence=True)

# Convert frames to video using FFmpeg
import subprocess
ffmpeg_cmd = [
    "ffmpeg", "-y",
    "-framerate", "30",
    "-i", "{workdir}/maya_frames/frame.%04d.jpg",
    "-c:v", "libx264",
    "-preset", "slow",
    "-crf", "18",
    "-pix_fmt", "yuv420p",
    "{final_mp4}"
]
subprocess.run(ffmpeg_cmd, check=True)

print("âœ… Maya + Arnold render complete:", "{final_mp4}")
''')
    
    # Execute Maya
    proc = await asyncio.create_subprocess_exec(
        config.maya_path, "-batch", "-file", str(maya_script)
    )
    await proc.wait()
    
    return final_mp4

# ==================== BLENDER PHOTOREALISTIC RENDERER ====================
async def render_with_blender(
    plan: Dict,
    base_video: Path,
    char_paths: List[str],
    anim_paths: List[str],
    voice_path: Path,
    workdir: Path
) -> Path:
    """
    Enhanced Blender Cycles with photorealistic materials and lighting
    """
    logger.info("ðŸŽ¬ Rendering with Blender Cycles (Enhanced photorealistic mode)")
    
    final_mp4 = workdir / "FINAL_MOVIE.mp4"
    blender_script = workdir / "render.py"
    
    blender_script.write_text(f'''
import bpy
import math

# Clean slate
bpy.ops.wm.read_factory_settings(use_empty=True)
scene = bpy.context.scene

# ===== CYCLES PHOTOREALISTIC SETTINGS =====
scene.render.engine = "CYCLES"
scene.cycles.device = "GPU"
scene.cycles.samples = 512  # High quality
scene.cycles.use_denoising = True
scene.cycles.denoiser = "OPENIMAGEDENOISE"

# Light bounces (photorealism)
scene.cycles.max_bounces = 12
scene.cycles.diffuse_bounces = 4
scene.cycles.glossy_bounces = 4
scene.cycles.transmission_bounces = 12
scene.cycles.transparent_max_bounces = 8

# Subsurface scattering for skin
scene.cycles.subsurface_samples = 4

# Caustics
scene.cycles.caustics_reflective = True
scene.cycles.caustics_refractive = True

# Resolution
scene.render.resolution_x = 1920
scene.render.resolution_y = 1080
scene.render.fps = 24
scene.frame_end = {plan.get("duration_seconds", 180) * 24}

# Color management (ACES cinema standard)
scene.view_settings.view_transform = "AgX"
scene.view_settings.look = "AgX - Very High Contrast"
scene.view_settings.exposure = 0.2

# Motion blur
scene.render.use_motion_blur = True
scene.render.motion_blur_shutter = 0.5

# Output
scene.render.image_settings.file_format = "FFMPEG"
scene.render.ffmpeg.codec = "H264"
scene.render.filepath = "{final_mp4}"

# ===== COMPOSITOR (Film effects) =====
scene.use_nodes = True
tree = scene.node_tree
tree.nodes.clear()

render_layers = tree.nodes.new("CompositorNodeRLayers")
render_layers.location = (0, 0)

# Glare (lens flares)
glare = tree.nodes.new("CompositorNodeGlare")
glare.glare_type = "STREAKS"
glare.quality = "HIGH"
glare.mix = 0.3
glare.location = (200, 0)
tree.links.new(render_layers.outputs["Image"], glare.inputs["Image"])

# Color balance (teal/orange grade)
color_balance = tree.nodes.new("CompositorNodeColorBalance")
color_balance.lift = (1.0, 0.95, 0.9)  # Teal shadows
color_balance.gain = (1.1, 1.05, 0.95)  # Orange highlights
color_balance.location = (400, 0)
tree.links.new(glare.outputs["Image"], color_balance.inputs["Image"])

# Vignette
lens_dist = tree.nodes.new("CompositorNodeLensdist")
lens_dist.use_fit = True
lens_dist.location = (600, 0)
tree.links.new(color_balance.outputs["Image"], lens_dist.inputs["Image"])

# Film grain
noise = tree.nodes.new("CompositorNodeTexture")
noise.location = (600, -200)

mix_grain = tree.nodes.new("CompositorNodeMixRGB")
mix_grain.blend_type = "OVERLAY"
mix_grain.inputs["Fac"].default_value = 0.1
mix_grain.location = (800, 0)
tree.links.new(lens_dist.outputs["Image"], mix_grain.inputs[1])
tree.links.new(noise.outputs["Value"], mix_grain.inputs[2])

# Output
composite = tree.nodes.new("CompositorNodeComposite")
composite.location = (1000, 0)
tree.links.new(mix_grain.outputs["Image"], composite.inputs["Image"])

# ===== VIDEO SEQUENCE =====
if not scene.sequence_editor:
    scene.sequence_editor_create()
seq = scene.sequence_editor

video_strip = seq.sequences.new_movie(
    name="BaseVideo",
    filepath="{base_video}",
    channel=1,
    frame_start=1
)

# ===== IMPORT & ENHANCE CHARACTERS WITH ANIMATIONS =====
{"".join([f'''
# Import character {i+1}
bpy.ops.import_scene.gltf(filepath="{p}")
char_obj = bpy.context.selected_objects[0]
char_obj.location = ({i * 3}, 0, 0)

# Import DeepMotion animation if available
{f'bpy.ops.import_scene.fbx(filepath="{anim_paths[i]}")' if i < len(anim_paths) and anim_paths[i] else '# No animation for this character'}

# PHOTOREALISTIC MATERIAL
mat = bpy.data.materials.new(name="PhotoMat_{i}")
mat.use_nodes = True
nodes = mat.node_tree.nodes
links = mat.node_tree.links
nodes.clear()

# Principled BSDF
principled = nodes.new("ShaderNodeBsdfPrincipled")
principled.location = (0, 0)

# Subsurface scattering (for organic materials like skin)
principled.inputs["Subsurface Weight"].default_value = 0.1
principled.inputs["Subsurface Radius"].default_value = (1.0, 0.5, 0.3)

# Roughness variation
noise_tex = nodes.new("ShaderNodeTexNoise")
noise_tex.inputs["Scale"].default_value = 50.0
noise_tex.location = (-400, -200)

color_ramp = nodes.new("ShaderNodeValToRGB")
color_ramp.location = (-200, -200)
links.new(noise_tex.outputs["Fac"], color_ramp.inputs["Fac"])
links.new(color_ramp.outputs["Color"], principled.inputs["Roughness"])

# Output
output = nodes.new("ShaderNodeOutputMaterial")
output.location = (300, 0)
links.new(principled.outputs["BSDF"], output.inputs["Surface"])

# Apply material
if char_obj.data.materials:
    char_obj.data.materials[0] = mat
else:
    char_obj.data.materials.append(mat)
''' for i, p in enumerate(char_paths)])}

# ===== CAMERA (Cinematic) =====
bpy.ops.object.camera_add(location=(0, -15, 6))
cam = bpy.context.object
cam.rotation_euler = (math.radians(75), 0, 0)
cam.data.lens = 35  # Cinematic lens
cam.data.sensor_width = 36  # Full frame
cam.data.dof.use_dof = True
cam.data.dof.aperture_fstop = 2.8
cam.data.dof.focus_distance = 15
scene.camera = cam

# Camera animation (dolly)
cam.keyframe_insert(data_path="location", frame=1)
cam.location.y = -12
cam.keyframe_insert(data_path="location", frame=scene.frame_end)

# ===== THREE-POINT LIGHTING =====
# Key light (warm)
bpy.ops.object.light_add(type="AREA", location=(5, -5, 8))
key_light = bpy.context.object
key_light.data.energy = 1500
key_light.data.color = (1.0, 0.9, 0.8)
key_light.data.size = 5

# Fill light (cool)
bpy.ops.object.light_add(type="AREA", location=(-5, -5, 6))
fill_light = bpy.context.object
fill_light.data.energy = 500
fill_light.data.color = (0.7, 0.8, 1.0)
fill_light.data.size = 4

# Rim light
bpy.ops.object.light_add(type="SPOT", location=(0, 5, 10))
rim_light = bpy.context.object
rim_light.data.energy = 2000
rim_light.rotation_euler = (math.radians(135), 0, 0)

# ===== HDRI ENVIRONMENT (Photorealism) =====
world = bpy.data.worlds["World"]
world.use_nodes = True
world_nodes = world.node_tree.nodes
world_links = world.node_tree.links
world_nodes.clear()

env_tex = world_nodes.new("ShaderNodeTexEnvironment")
env_tex.location = (-300, 300)

bg = world_nodes.new("ShaderNodeBackground")
bg.inputs["Strength"].default_value = 0.3
bg.location = (0, 300)
world_links.new(env_tex.outputs["Color"], bg.inputs["Color"])

output = world_nodes.new("ShaderNodeOutputWorld")
output.location = (200, 300)
world_links.new(bg.outputs["Background"], output.inputs["Surface"])

# ===== SPECIAL FX =====
mood = "{plan.get('mood', '')}".lower()

# Rain system
if "rain" in mood or "rainy" in mood or "storm" in mood:
    bpy.ops.mesh.primitive_plane_add(size=200, location=(0, 0, 30))
    rain_plane = bpy.context.object
    rain_plane.name = "RainEmitter"
    
    # Particle system for rain
    ps_modifier = rain_plane.modifiers.new("Rain", "PARTICLE_SYSTEM")
    ps = ps_modifier.particle_system
    pset = ps.settings
    
    pset.count = 10000
    pset.lifetime = 60
    pset.frame_start = 1
    pset.frame_end = scene.frame_end
    pset.normal_factor = -20  # Fall down
    pset.factor_random = 0.1
    
    # Rain drop appearance
    pset.particle_size = 0.02
    pset.size_random = 0.5
    pset.render_type = 'OBJECT'
    
    # Create rain drop mesh
    bpy.ops.mesh.primitive_cylinder_add(radius=0.005, depth=0.1, location=(100, 100, 100))
    rain_drop = bpy.context.object
    rain_drop.name = "RainDrop"
    pset.instance_object = rain_drop
    
    # Hide rain drop mesh
    rain_drop.hide_render = True
    rain_drop.hide_viewport = True
    
    # Rain material (glass-like)
    rain_mat = bpy.data.materials.new(name="RainMaterial")
    rain_mat.use_nodes = True
    rain_nodes = rain_mat.node_tree.nodes
    rain_nodes.clear()
    
    glass_bsdf = rain_nodes.new("ShaderNodeBsdfGlass")
    glass_bsdf.location = (0, 0)
    glass_bsdf.inputs["IOR"].default_value = 1.33
    
    output = rain_nodes.new("ShaderNodeOutputMaterial")
    output.location = (200, 0)
    rain_mat.node_tree.links.new(glass_bsdf.outputs["BSDF"], output.inputs["Surface"])

# Fog/Mist
if "fog" in mood or "mist" in mood or "foggy" in mood or "misty" in mood:
    world.mist_settings.use_mist = True
    world.mist_settings.intensity = 0.3
    world.mist_settings.start = 5
    world.mist_settings.depth = 50
    world.mist_settings.falloff = "QUADRATIC"
    
    # Volumetric fog
    bpy.ops.mesh.primitive_cube_add(size=100, location=(0, 0, 10))
    fog_volume = bpy.context.object
    fog_volume.name = "FogVolume"
    
    fog_mat = bpy.data.materials.new(name="VolumetricFog")
    fog_mat.use_nodes = True
    fog_nodes = fog_mat.node_tree.nodes
    fog_nodes.clear()
    
    volume_scatter = fog_nodes.new("ShaderNodeVolumeScatter")
    volume_scatter.location = (0, 0)
    volume_scatter.inputs["Density"].default_value = 0.1
    
    output = fog_nodes.new("ShaderNodeOutputMaterial")
    output.location = (200, 0)
    fog_mat.node_tree.links.new(volume_scatter.outputs["Volume"], output.inputs["Volume"])
    
    fog_volume.data.materials.append(fog_mat)

# Explosions/Fire (if mentioned)
if "explos" in mood or "fire" in mood or "burn" in mood:
    # Quick Fire smoke simulation
    bpy.ops.mesh.primitive_ico_sphere_add(radius=2, location=(5, 5, 2))
    fire_domain = bpy.context.object
    fire_domain.name = "FireDomain"
    
    # Smoke domain
    bpy.ops.object.quick_smoke(style='SMOKE_DOMAIN')
    
    # Increase resolution
    smoke_modifier = fire_domain.modifiers["Smoke"]
    smoke_modifier.domain_settings.resolution_max = 128
    smoke_modifier.domain_settings.use_dissolve_smoke = True
    smoke_modifier.domain_settings.dissolve_speed = 50

# Neon lights (if cyberpunk/neon mentioned)
if "neon" in mood or "cyber" in mood or "blade" in mood:
    neon_colors = [
        (0.0, 0.5, 1.0),  # Blue
        (1.0, 0.0, 0.5),  # Pink
        (0.0, 1.0, 0.5),  # Green
        (1.0, 0.5, 0.0),  # Orange
    ]
    
    for i in range(10):
        bpy.ops.mesh.primitive_cube_add(
            size=0.5,
            location=(i * 3 - 15, 10, 4 + (i % 2) * 2)
        )
        neon_sign = bpy.context.object
        neon_sign.name = f"NeonSign_{i}"
        
        # Emission material
        neon_mat = bpy.data.materials.new(name=f"NeonMat_{i}")
        neon_mat.use_nodes = True
        neon_nodes = neon_mat.node_tree.nodes
        neon_nodes.clear()
        
        emission = neon_nodes.new("ShaderNodeEmission")
        emission.location = (0, 0)
        color = neon_colors[i % len(neon_colors)]
        emission.inputs["Color"].default_value = (*color, 1.0)
        emission.inputs["Strength"].default_value = 50.0
        
        output = neon_nodes.new("ShaderNodeOutputMaterial")
        output.location = (200, 0)
        neon_mat.node_tree.links.new(emission.outputs["Emission"], output.inputs["Surface"])
        
        neon_sign.data.materials.append(neon_mat)
        
        # Add point light
        bpy.ops.object.light_add(type='POINT', location=neon_sign.location)
        neon_light = bpy.context.object
        neon_light.data.energy = 500
        neon_light.data.color = color

# Snow (if winter/snow mentioned)
if "snow" in mood or "winter" in mood or "blizzard" in mood:
    bpy.ops.mesh.primitive_plane_add(size=200, location=(0, 0, 30))
    snow_plane = bpy.context.object
    snow_plane.name = "SnowEmitter"
    
    ps_modifier = snow_plane.modifiers.new("Snow", "PARTICLE_SYSTEM")
    ps = ps_modifier.particle_system
    pset = ps.settings
    
    pset.count = 5000
    pset.lifetime = 120
    pset.normal_factor = -5  # Gentle fall
    pset.factor_random = 0.3
    pset.particle_size = 0.05
    pset.size_random = 0.8
    pset.render_type = 'HALO'

# ===== AUDIO =====
audio_strip = seq.sequences.new_sound(
    name="Dialogue",
    filepath="{voice_path}",
    channel=2,
    frame_start=60
)

# ===== RENDER =====
print("=" * 60)
print("ðŸŽ¬ RENDERING PHOTOREALISTIC MOVIE...")
print(f"Resolution: 1920x1080 @ 24fps")
print(f"Samples: 512 (Cycles)")
print(f"Duration: {plan.get('duration_seconds', 180)}s")
print("=" * 60)

bpy.ops.render.render(animation=True)

print("âœ… MOVIE COMPLETE:", "{final_mp4}")
''')

    # Run Blender
    proc = await asyncio.create_subprocess_exec(
        config.blender_path, "--background", "--python", str(blender_script)
    )
    await proc.wait()
    
    return final_mp4

# ==================== DYNAMIC BLENDER SCRIPT GENERATOR ====================
def generate_ai_blender_script(
    video_path: str,
    char_path: str,
    audio_path: str,
    anim_path: str,
    output_path: str
) -> str:
    """Dynamic Blender script: Import AI assets, composite, lip-sync, export MP4"""
    return f'''
import bpy
import bmesh
from mathutils import Vector

# Reset scene
bpy.ops.wm.read_homefile(use_empty=True)
scene = bpy.context.scene
scene.render.engine = 'BLENDER_EEVEE'  # Fast for polish
scene.render.fps = 30
scene.frame_end = 5400  # 3 min

# Import base video as background
bpy.ops.sequencer.movie_strip_add(filepath="{video_path}", frame_start=1, channel=1)

# Import AI char
bpy.ops.import_scene.gltf(filepath="{char_path}")

# Import animation
if "{anim_path}":
    bpy.ops.import_scene.fbx(filepath="{anim_path}")

# Lip-sync audio (use addon or simple)
bpy.ops.sequencer.sound_strip_add(filepath="{audio_path}", frame_start=60, channel=2)

# Camera
bpy.ops.object.camera_add(location=(0, -15, 8))
cam = bpy.context.object
scene.camera = cam

# Lighting
bpy.ops.object.light_add(type='SUN', location=(0, 0, 10))

# Render
scene.render.filepath = "{output_path}"
scene.render.image_settings.file_format = 'FFMPEG'
bpy.ops.render.render(animation=True)
'''

async def execute_blender_with_ai(script_content: str, output_path: str) -> Dict[str, Any]:
    """Execute Blender script"""
    script_file = Path(tempfile.gettempdir()) / "blender_script.py"
    script_file.write_text(script_content)
    
    cmd = [config.blender_path, "--background", "--python", str(script_file), "--enable-autoexec"]
    proc = await asyncio.create_subprocess_exec(*cmd)
    await proc.wait()
    
    return {
        "success": Path(output_path).exists(),
        "file_size": Path(output_path).stat().st_size if Path(output_path).exists() else 0
    }

# ==================== ALL MAIN TOOLS ====================

@mcp.tool()
async def make_movie(idea: str) -> str:
    """
    THE ONE TOOL â€” Just talk like a human.
    
    Tell me ANYTHING you want to see:
    - "a duck running down the road"
    - "assassin in neon city"
    - "robot vs alien in desert"
    - "romantic dinner goes wrong"
    
    I'll make it look like Rally or Box Assassin quality.
    Returns: JSON with final MP4 path
    """
    if not await allow_movie():
        return json.dumps({"error": "One movie per hour â€” real credits cost real money."})
    
    if not all([config.KLING_KEY, config.MESHY_KEY, config.ELEVEN_KEY]):
        return json.dumps({"error": "Missing API keys: KLING_KEY, MESHY_KEY, ELEVEN_KEY required"})
    
    try:
        logger.info(f"ðŸŽ¬ Starting movie: {idea[:100]}...")
        
        # Parse idea
        plan = await understand_idea(idea)
        logger.info(f"ðŸ“‹ Plan: {plan['title']}")
        
        # Create work directory
        workdir = config.root / f"movie_{int(time.time())}_{hashlib.md5(idea.encode()).hexdigest()[:8]}"
        workdir.mkdir(parents=True, exist_ok=True)
        
        # Step 1: Generate base video with Kling
        logger.info("Step 1/7 â€” Kling AI base video...")
        kling = KlingClient(config.KLING_KEY)
        video_url = await kling.text_to_video(
            plan["scenes"][0] if plan["scenes"] else idea,
            plan.get("duration_seconds", 180)
        )
        base_video = workdir / "01_kling_base.mp4"
        async with aiohttp.ClientSession() as s:
            async with s.get(video_url) as r:
                base_video.write_bytes(await r.read())
        
        # Step 2: Generate characters
        logger.info("Step 2/7 â€” Generating 3D characters...")
        char_paths = []
        for char in plan.get("characters", [])[:3]:  # Max 3 chars
            meshy = MeshyClient(config.MESHY_KEY)
            glb_url = await meshy.text_to_3d(char["description"])
            char_path = workdir / f"char_{len(char_paths)}.glb"
            async with aiohttp.ClientSession() as s:
                async with s.get(glb_url) as r:
                    char_path.write_bytes(await r.read())
            char_paths.append(str(char_path))
        
        # Step 3: Generate voice
        logger.info("Step 3/7 â€” ElevenLabs voice...")
        dialogue = plan.get("dialogue", ["Action!"])[0] if plan.get("dialogue") else "Action!"
        client = ElevenLabs(api_key=config.ELEVEN_KEY)
        audio = client.generate(text=dialogue, voice="Adam", model="eleven_turbo_v2")
        voice_path = workdir / "03_voice.mp3"
        with open(voice_path, "wb") as f:
            for chunk in audio:
                f.write(chunk)
        
        # Step 4: Animate characters (if DeepMotion available)
        logger.info("Step 4/7 â€” Animating characters...")
        anim_paths = []
        if config.DEEPMOTION_ID and config.DEEPMOTION_SEC:
            deepmotion = DeepMotionClient(config.DEEPMOTION_ID, config.DEEPMOTION_SEC)
            for char_path in char_paths:
                anim = await deepmotion.animate_character(char_path, "dramatic action sequence")
                if anim:
                    anim_paths.append(anim)
        
        # Step 5: Enhance video (if Runway available)
        logger.info("Step 5/7 â€” Runway enhancement...")
        if config.RUNWAY_KEY:
            runway = RunwayClient(config.RUNWAY_KEY)
            base_video_str = str(base_video)
            enhanced = await runway.enhance_video(base_video_str, "cinematic lighting, lens flares")
            if enhanced != base_video_str:
                base_video = Path(enhanced)
        
        # Step 6-7: Final render
        logger.info("Step 6-7/7 â€” Final rendering...")
        if config.use_unreal and config.unreal_path:
            final_mp4 = await render_with_unreal(plan, base_video, char_paths, voice_path, workdir)
        elif config.use_maya and config.maya_path:
            final_mp4 = await render_with_maya(plan, base_video, char_paths, voice_path, workdir)
        else:
            final_mp4 = await render_with_blender(plan, base_video, char_paths, anim_paths, voice_path, workdir)
        
        logger.info(f"âœ… MOVIE COMPLETE: {final_mp4}")
        
        return json.dumps({
            "success": True,
            "final_video": str(final_mp4),
            "title": plan["title"],
            "duration": plan.get("duration_seconds", 180),
            "style": plan.get("style", "photorealistic"),
            "file_size": final_mp4.stat().st_size if final_mp4.exists() else 0,
            "message": "Your photorealistic masterpiece is ready!"
        })
    
    except Exception as e:
        logger.error(f"Movie generation failed: {e}")
        return json.dumps({"success": False, "error": str(e)})

@mcp.tool()
async def create_pixar_gta_cutscene(
    prompt: str,
    duration: int = 180,
    style: str = "pixar gta v"
) -> str:
    """Generate 2-3 min Pixar-GTA cutscene: Kling base + AI fixes + Blender polish"""
    if not await rate_limiter.acquire():
        return json.dumps({"success": False, "error": "Rate limit hitâ€”AI costs add up!"})
    
    try:
        parsed = EnhancedParser.parse(prompt)
        logger.info(f"Generating {style} cutscene: {prompt}")
        
        # Initialize orchestrator
        await orchestrator.init_clients()
        
        # Step 1: Kling base video
        video_path = await orchestrator.generate_base_video(prompt, duration)
        
        # Step 2: Gen/fix hero char
        char_desc = "gangster hero with big pixar eyes and sunglasses"
        char_path = await orchestrator.generate_character(char_desc)
        
        # Step 3: Voice dialogue
        voice_path = None
        if parsed["dialogue"]:
            voice_path = await orchestrator.generate_voice(parsed["dialogue"][0], voice="Adam")
        
        # Step 4: Animate char with DeepMotion
        anim_path = await orchestrator.enhance_motion(char_path, "dramatic chase spin and grin")
        
        # Step 5: Runway polish
        polished_video = await orchestrator.polish_video(video_path, "neon glow, lens flares, explosion bursts")
        
        # Step 6: Blender composite
        output_path = str(config.temp_dir / f"cutscene_{int(time.time())}.mp4")
        script = generate_ai_blender_script(polished_video, char_path, voice_path or "", anim_path or "", output_path)
        result = await execute_blender_with_ai(script, output_path)
        
        # Cleanup temps
        for p in [video_path, char_path, voice_path, anim_path, polished_video]:
            if p and Path(p).exists():
                Path(p).unlink()
        
        return json.dumps({
            "success": result["success"],
            "output_path": output_path,
            "file_size": result.get("file_size", 0),
            "parsed": parsed,
            "message": f"Pixar-GTA masterpiece ready! Duration: {duration}s"
        })
    
    except Exception as e:
        logger.error(f"Cutscene fail: {e}")
        return json.dumps({"success": False, "error": str(e)})
    
    finally:
        await rate_limiter.release()

@mcp.tool()
async def create_ultimate_pixar_gta_cutscene(
    prompt: str,
    duration: int = 150,
    voice_line: str = "This city belongs to me now.",
    hero_name: str = "Gangster"
) -> str:
    """
    Generate a full 2â€“3 minute Pixar Ã— GTA V quality trailer from text.
    Returns JSON with final MP4 path.
    """
    if not await can_make_request():
        return json.dumps({"error": "Rate limited â€” max 5 heavy renders per hour"})

    if not all([config.KLING_API_KEY, config.MESHY_API_KEY, config.ELEVENLABS_API_KEY]):
        return json.dumps({"error": "Missing API keys! Set KLING_API_KEY, MESHY_API_KEY, ELEVENLABS_API_KEY"})

    logger.info(f"Starting ultimate render: {prompt[:100]}...")

    out_dir = config.temp_dir / f"render_{int(time.time())}_{hashlib.md5(prompt.encode()).hexdigest()[:8]}"
    out_dir.mkdir(parents=True, exist_ok=True)

    try:
        # Step 1: Kling AI base video
        logger.info("Step 1/7 â€” Kling AI generating base video...")
        kling = KlingClient(config.KLING_API_KEY)
        video_url = await kling.text_to_video(f"Pixar GTA V style cinematic trailer, {prompt}", duration)
        
        base_video = str(out_dir / "01_kling_base.mp4")
        async with aiohttp.ClientSession() as s:
            async with s.get(video_url) as r:
                with open(base_video, "wb") as f:
                    f.write(await r.read())

        # Step 2: Meshy hero character
        logger.info("Step 2/7 â€” Meshy creating hero...")
        meshy = MeshyClient(config.MESHY_API_KEY)
        hero_glb_url = await meshy.text_to_3d(f"Pixar style GTA character, {hero_name}, big eyes, leather jacket, rigged for animation")
        hero_glb = str(out_dir / "02_hero.glb")
        async with aiohttp.ClientSession() as s:
            async with s.get(hero_glb_url) as r:
                with open(hero_glb, "wb") as f:
                    f.write(await r.read())

        # Step 3: Voice with ElevenLabs
        logger.info("Step 3/7 â€” ElevenLabs voice...")
        client = ElevenLabs(api_key=config.ELEVENLABS_API_KEY)
        audio = client.generate(
            text=voice_line,
            voice="Adam",  # deep gangster voice
            model="eleven_turbo_v2"
        )
        voice_path = str(out_dir / "03_voice.mp3")
        with open(voice_path, "wb") as f:
            for chunk in audio:
                f.write(chunk)

        # Step 4â€“7: Final Blender polish
        logger.info("Step 4â€“7 â€” Blender final composite...")
        final_mp4 = str(out_dir / "FINAL_PIXAR_GTA_CUTSCENE.mp4")
        blender_script = str(out_dir / "render.py")
        
        with open(blender_script, "w") as f:
            f.write(f'''
import bpy
import os
bpy.ops.wm.read_factory_settings(use_empty=True)

# Import Kling video as background
bpy.ops.sequencer.movie_strip_add(
    filepath="{base_video}",
    frame_start=1, channel=1
)

# Import hero
bpy.ops.import_scene.gltf(filepath="{hero_glb}")

# Basic camera
bpy.ops.object.camera_add(location=(0, -15, 8))
cam = bpy.context.object
cam.data.lens = 35
bpy.context.scene.camera = cam

# Track to hero
hero = bpy.data.objects.get("Root") or bpy.context.selected_objects[0]
constraint = cam.constraints.new(type='TRACK_TO')
constraint.target = hero
constraint.track_axis = 'TRACK_NEGATIVE_Z'
constraint.up_axis = 'UP_Y'

# Audio
bpy.ops.sequencer.sound_strip_add(filepath="{voice_path}", frame_start=100, channel=2)

# Render settings
scene = bpy.context.scene
scene.render.resolution_x = 1920
scene.render.resolution_y = 1080
scene.render.fps = 30
scene.frame_end = {duration * 30}
scene.render.image_settings.file_format = 'FFMPEG'
scene.render.ffmpeg.format = 'MPEG4'
scene.render.ffmpeg.codec = 'H264'
scene.render.filepath = "{final_mp4}"

bpy.ops.render.render(animation=True)
print("RENDER DONE:", "{final_mp4}")
''')

        # Run Blender headless
        cmd = [
            config.blender_path, "--background", "--python", blender_script,
            "--", "--cycles"
        ]
        proc = await asyncio.create_subprocess_exec(*cmd)
        await proc.wait()

        if not Path(final_mp4).exists():
            raise Exception("Blender render failed")

        return json.dumps({
            "success": True,
            "final_video": final_mp4,
            "duration_seconds": duration,
            "prompt": prompt,
            "message": "Your Pixar Ã— GTA V masterpiece is ready!"
        })

    except Exception as e:
        logger.error(f"Failed: {e}")
        return json.dumps({"success": False, "error": str(e)})

@mcp.tool()
async def make_photorealistic_movie(idea: str) -> str:
    """
    Create a 2-5 minute photorealistic movie from natural language.
    
    Just describe what you want to see:
    - "A spy infiltrates a neon city at night"
    - "Robot uprising in Tokyo"
    - "Romantic dinner interrupted by aliens"
    - "Heist movie in Las Vegas"
    
    Returns: JSON with final video path and metadata
    
    Uses best available rendering engine:
    - Unreal Engine 5.7 (Rally-level photorealism)
    - Maya + Arnold (Box Assassin polish)
    - Enhanced Blender Cycles (Photorealistic fallback)
    """
    if not await allow_movie():
        return json.dumps({"error": "Rate limit: 1 movie per hour (expensive APIs!)"})
    
    if not await can_make_request():
        return json.dumps({"error": "Too many requests - wait and try again"})
    
    if not all([config.KLING_KEY, config.MESHY_KEY, config.ELEVEN_KEY]):
        return json.dumps({
            "error": "Missing required API keys",
            "required": ["KLING_KEY", "MESHY_KEY", "ELEVEN_KEY"],
            "docs": "Set these as environment variables"
        })
    
    try:
        logger.info(f"ðŸŽ¬ PHOTOREALISTIC MOVIE: {idea[:100]}...")
        
        # Parse idea with AI
        plan = await understand_idea(idea)
        logger.info(f"ðŸ“‹ Film plan: {plan['title']} ({plan.get('duration_seconds', 180)}s)")
        
        # Create work directory
        workdir = config.root / f"photoreal_{int(time.time())}_{hashlib.md5(idea.encode()).hexdigest()[:8]}"
        workdir.mkdir(parents=True, exist_ok=True)
        
        # Initialize AI orchestrator
        await orchestrator.init_clients()
        
        # Step 1: Generate base video
        logger.info("Step 1/7 â€” Generating base cinematic video...")
        video_path = await orchestrator.generate_base_video(
            plan["scenes"][0] if plan["scenes"] else idea,
            plan.get("duration_seconds", 180)
        )
        base_video = workdir / "01_base.mp4"
        Path(video_path).rename(base_video)
        
        # Step 2: Generate photorealistic characters
        logger.info("Step 2/7 â€” Generating photorealistic 3D characters...")
        char_paths = []
        for i, char in enumerate(plan.get("characters", [])[:3]):
            logger.info(f"   Character {i+1}: {char['name']}")
            char_path = await orchestrator.generate_character(char["description"])
            final_char_path = workdir / f"char_{i}_{char['name']}.glb"
            Path(char_path).rename(final_char_path)
            char_paths.append(str(final_char_path))
        
        # Step 3: Generate natural voice
        logger.info("Step 3/7 â€” Generating voice dialogue...")
        dialogue_text = " ".join(plan.get("dialogue", ["Action!"]))[:500]
        voice_path_temp = await orchestrator.generate_voice(dialogue_text, voice="Rachel")
        voice_path = workdir / "03_dialogue.mp3"
        Path(voice_path_temp).rename(voice_path)
        
        # Step 4: Animate characters
        logger.info("Step 4/7 â€” Animating characters with DeepMotion...")
        anim_paths = []
        for i, char_path in enumerate(char_paths):
            logger.info(f"   Animating character {i+1}...")
            anim = await orchestrator.enhance_motion(char_path, "cinematic dramatic action")
            if anim:
                final_anim = workdir / f"anim_{i}.fbx"
                Path(anim).rename(final_anim)
                anim_paths.append(str(final_anim))
            else:
                anim_paths.append(None)
        
        # Step 5: Enhance video quality
        logger.info("Step 5/7 â€” Enhancing video with Runway...")
        enhanced_video = await orchestrator.polish_video(
            str(base_video),
            "photorealistic lighting, film grain, color grading"
        )
        if enhanced_video != str(base_video):
            base_video = Path(enhanced_video)
        
        # Step 6-7: Final photorealistic render
        logger.info("Step 6-7/7 â€” Final photorealistic rendering...")
        logger.info(f"   Engine: {'Unreal Engine 5.7' if config.use_unreal else 'Maya + Arnold' if config.use_maya else 'Blender Cycles'}")
        
        if config.use_unreal and config.unreal_path:
            final_mp4 = await render_with_unreal(
                plan, base_video, char_paths, voice_path, workdir
            )
        elif config.use_maya and config.maya_path:
            final_mp4 = await render_with_maya(
                plan, base_video, char_paths, voice_path, workdir
            )
        else:
            final_mp4 = await render_with_blender(
                plan, base_video, char_paths, anim_paths, voice_path, workdir
            )
        
        # Verify output
        if not final_mp4.exists():
            raise Exception("Final render failed - no output file")
        
        file_size_mb = final_mp4.stat().st_size / (1024 * 1024)
        
        logger.info("=" * 60)
        logger.info(f"âœ… PHOTOREALISTIC MOVIE COMPLETE!")
        logger.info(f"   Title: {plan['title']}")
        logger.info(f"   Duration: {plan.get('duration_seconds', 180)}s")
        logger.info(f"   File: {final_mp4}")
        logger.info(f"   Size: {file_size_mb:.2f} MB")
        logger.info("=" * 60)
        
        return json.dumps({
            "success": True,
            "final_video": str(final_mp4),
            "title": plan["title"],
            "duration_seconds": plan.get("duration_seconds", 180),
            "style": plan.get("style", "photorealistic"),
            "rendering_engine": "Unreal Engine 5.7" if config.use_unreal else "Maya + Arnold" if config.use_maya else "Blender Cycles",
            "file_size_mb": file_size_mb,
            "characters": len(char_paths),
            "scenes": len(plan.get("scenes", [])),
            "message": "Your photorealistic cinematic masterpiece is ready!",
            "features": {
                "lumen_gi": config.use_lumen and config.use_unreal,
                "nanite_geometry": config.use_nanite and config.use_unreal,
                "megalights": config.use_megalights and config.use_unreal,
                "path_tracing": config.use_path_tracing,
                "depth_of_field": config.enable_dof,
                "motion_blur": config.enable_motion_blur,
                "film_grain": config.enable_film_grain
            }
        })
    
    except Exception as e:
        logger.error(f"âŒ Photorealistic movie generation failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return json.dumps({
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        })

# ==================== ADDITIONAL HELPER TOOLS ====================

@mcp.tool()
async def check_api_status() -> str:
    """
    Check which AI APIs are configured and available.
    Returns status of all services.
    """
    status = {
        "video_generation": {
            "kling": bool(config.KLING_KEY),
            "sora": bool(config.SORA_KEY),
            "luma": bool(config.LUMA_KEY),
            "runway": bool(config.RUNWAY_KEY),
            "pika": bool(config.PIKA_KEY),
            "hailuo": bool(config.HAILUO_KEY)
        },
        "3d_generation": {
            "meshy": bool(config.MESHY_KEY),
            "tripo": bool(config.TRIPO_KEY)
        },
        "voice": {
            "elevenlabs": bool(config.ELEVEN_KEY)
        },
        "animation": {
            "deepmotion": bool(config.DEEPMOTION_ID and config.DEEPMOTION_SEC)
        },
        "rendering_engines": {
            "unreal_engine_5.7": config.use_unreal and bool(config.unreal_path),
            "maya_arnold": config.use_maya and bool(config.maya_path),
            "blender_cycles": bool(config.blender_path)
        },
        "ai_planning": {
            "openai_gpt4": bool(config.OPENAI_KEY)
        },
        "ready_for_production": all([
            config.KLING_KEY or config.SORA_KEY or config.LUMA_KEY,
            config.MESHY_KEY or config.TRIPO_KEY,
            config.ELEVEN_KEY
        ])
    }
    
    return json.dumps(status, indent=2)

@mcp.tool()
async def list_rendering_capabilities() -> str:
    """
    List all rendering capabilities and features available.
    """
    capabilities = {
        "available_engines": [],
        "photorealistic_features": {
            "unreal_engine_5.7": {
                "lumen_global_illumination": config.use_lumen,
                "nanite_virtualized_geometry": config.use_nanite,
                "megalights_dynamic_lighting": config.use_megalights,
                "path_tracing": config.use_path_tracing,
                "virtual_shadow_maps": True
            },
            "maya_arnold": {
                "arnold_ray_tracing": config.use_maya,
                "professional_shaders": config.use_maya,
                "three_point_lighting": True
            },
            "blender_cycles": {
                "gpu_rendering": True,
                "photorealistic_materials": True,
                "subsurface_scattering": True,
                "caustics": True,
                "hdri_environment": True,
                "film_grain": config.enable_film_grain,
                "color_grading": config.color_grading
            }
        },
        "post_processing": {
            "depth_of_field": config.enable_dof,
            "motion_blur": config.enable_motion_blur,
            "bloom": True,
            "vignette": True,
            "lens_distortion": True,
            "color_grading": config.color_grading
        },
        "resolution": {
            "hd": config.resolution,
            "4k": config.resolution_4k,
            "fps": config.fps
        },
        "max_duration_seconds": config.max_duration
    }
    
    if config.use_unreal and config.unreal_path:
        capabilities["available_engines"].append("Unreal Engine 5.7 (Rally-level)")
    if config.use_maya and config.maya_path:
        capabilities["available_engines"].append("Maya + Arnold (Box Assassin-level)")
    if config.blender_path:
        capabilities["available_engines"].append("Blender Cycles (Enhanced photorealistic)")
    
    return json.dumps(capabilities, indent=2)

@mcp.tool()
async def get_example_prompts() -> str:
    """
    Get example prompts for different movie styles.
    """
    examples = {
        "photorealistic_action": [
            "A spy in a black suit infiltrates a neon-lit Tokyo skyscraper at night, rain falling on the windows",
            "An intense car chase through downtown Miami at sunset, sports cars weaving through traffic",
            "A lone astronaut discovers an ancient alien structure on Mars, dramatic lighting"
        ],
        "pixar_gta_style": [
            "A gangster with big eyes and a leather jacket drives through a cartoon city, explosions behind him",
            "Two cartoon bank robbers plan a heist in their hideout, one points at a blueprint",
            "A female assassin with purple hair leaps between rooftops in a stylized cyberpunk city"
        ],
        "dramatic_scenes": [
            "A romantic dinner interrupted by an alien invasion, people running, ships overhead",
            "A detective solves a murder mystery in a noir 1940s cityscape, rain and shadows",
            "A robot uprising in Tokyo, mechs fighting, civilians fleeing, neon signs flickering"
        ],
        "epic_trailers": [
            "An epic fantasy battle, dragons flying overhead, armies clashing, magic explosions",
            "A superhero origin story, person discovers powers, city in danger, final confrontation",
            "A space opera with massive starship battles, laser fire, explosions, dramatic music"
        ]
    }
    
    return json.dumps(examples, indent=2)

# ==================== SERVER STARTUP ====================

if __name__ == "__main__":
    logger.info("=" * 70)
    logger.info("ULTIMATE AI MOVIE MAKER MCP SERVER 2025 â€” ONLINE")
    logger.info("=" * 70)
    logger.info("")
    logger.info("ðŸ“‹ CONFIGURED SERVICES:")
    logger.info(f"   Video Gen: Kling={bool(config.KLING_KEY)} Sora={bool(config.SORA_KEY)} Luma={bool(config.LUMA_KEY)} Runway={bool(config.RUNWAY_KEY)}")
    logger.info(f"   3D Gen: Meshy={bool(config.MESHY_KEY)} Tripo={bool(config.TRIPO_KEY)}")
    logger.info(f"   Voice: ElevenLabs={bool(config.ELEVEN_KEY)}")
    logger.info(f"   Animation: DeepMotion={bool(config.DEEPMOTION_ID and config.DEEPMOTION_SEC)}")
    logger.info("")
    logger.info("ðŸŽ® RENDERING ENGINES:")
    if config.use_unreal and config.unreal_path:
        logger.info("   âœ… Unreal Engine 5.7 (Rally-level photorealism)")
    if config.use_maya and config.maya_path:
        logger.info("   âœ… Maya + Arnold (Box Assassin polish)")
    logger.info(f"   âœ… Blender Cycles (Enhanced photorealistic)")
    logger.info("")
    logger.info("ðŸŽ¬ AVAILABLE TOOLS:")
    logger.info("   â€¢ make_movie(idea) - Natural language to photorealistic film")
    logger.info("   â€¢ make_photorealistic_movie(idea) - Full 2-5 min cinema quality")
    logger.info("   â€¢ create_pixar_gta_cutscene(prompt) - Stylized Pixar/GTA cutscene")
    logger.info("   â€¢ create_ultimate_pixar_gta_cutscene(prompt) - Ultimate trailer")
    logger.info("   â€¢ check_api_status() - Check configured services")
    logger.info("   â€¢ list_rendering_capabilities() - Show all features")
    logger.info("   â€¢ get_example_prompts() - Example movie ideas")
    logger.info("")
    logger.info("âš¡ RATE LIMITS:")
    logger.info("   â€¢ 1 full movie per hour")
    logger.info("   â€¢ 5 heavy AI requests per hour")
    logger.info("")
    logger.info("=" * 70)
    logger.info("Ready to create photorealistic cinema from natural language!")
    logger.info("=" * 70)
    
    mcp.run()
