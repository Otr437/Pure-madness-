// main.go - COMPLETE zkKaia BLOCKCHAIN - FULLY IMPLEMENTED
package main

import (
	"bytes"
	"compress/gzip"
	"container/heap"
	"context"
	"crypto/aes"
	"crypto/cipher"
	"crypto/ecdsa"
	"crypto/elliptic"
	"crypto/hmac"
	"crypto/rand"
	"crypto/sha256"
	"crypto/sha512"
	"encoding/binary"
	"encoding/hex"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"log"
	"math"
	"math/big"
	"net"
	"net/http"
	"net/url"
	"os"
	"path/filepath"
	"regexp"
	"runtime"
	"sort"
	"strconv"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/ethereum/go-ethereum/accounts"
	"github.com/ethereum/go-ethereum/accounts/keystore"
	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/common/hexutil"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/rlp"

	"github.com/kaiachain/kaia/blockchain"
	"github.com/kaiachain/kaia/blockchain/state"
	kaiatypes "github.com/kaiachain/kaia/blockchain/types"
	"github.com/kaiachain/kaia/blockchain/vm"
	"github.com/kaiachain/kaia/consensus"
	"github.com/kaiachain/kaia/consensus/istanbul"
	"github.com/kaiachain/kaia/node"
	"github.com/kaiachain/kaia/params"
	"github.com/kaiachain/kaia/rpc"

	"github.com/gorilla/mux"
	"github.com/gorilla/websocket"

	"github.com/consensys/gnark-crypto/ecc"
	"github.com/consensys/gnark-crypto/ecc/bn254"
	"github.com/consensys/gnark-crypto/ecc/bn254/fr"
	"github.com/consensys/gnark-crypto/ecc/bn254/twistededwards"
	"github.com/consensys/gnark/backend/groth16"
	"github.com/consensys/gnark/backend/witness"
	"github.com/consensys/gnark/constraint"
	"github.com/consensys/gnark/frontend"
	"github.com/consensys/gnark/frontend/cs/r1cs"
	gnarkeddsa "github.com/consensys/gnark/std/algebra/native/twistededwards"
	"github.com/consensys/gnark/std/hash/mimc"

	"github.com/syndtr/goleveldb/leveldb"
	"github.com/syndtr/goleveldb/leveldb/opt"
	"github.com/syndtr/goleveldb/leveldb/util"
	"github.com/vmihailenco/msgpack/v5"

	"golang.org/x/crypto/argon2"
	"golang.org/x/crypto/blake2b"
	"golang.org/x/crypto/chacha20poly1305"
	"golang.org/x/crypto/curve25519"
	"golang.org/x/crypto/ed25519"
	"golang.org/x/crypto/hkdf"
	"golang.org/x/crypto/pbkdf2"
	"golang.org/x/crypto/scrypt"
)

// ==================== CONSTANTS & CONFIGURATION ====================
const (
	ZKKaiChainID        = 1729
	BlockGasLimit       = 30000000
	BaseGasPrice        = 250000000
	RollupBatchSize     = 100
	BlockTime           = 12 * time.Second
	MaxBlockSize        = 2 * 1024 * 1024 // 2MB
	
	InitialShards      = 16
	MaxShards          = 256
	TxPerShard         = 50000
	WorkerPoolSize     = 64
	ValidationWorkers  = 32
	ProofWorkers       = 16
	
	InitialMemPoolSize = 100000
	MaxMemPoolSize     = 10000000
	MemPoolGrowthRate  = 2
	
	MinBatchSize       = 100
	MaxBatchSize       = 10000
	BatchTimeout       = 5 * time.Second
	
	StateCacheSize     = 100000
	BlockCacheSize     = 10000
	TxCacheSize        = 500000
	ProofCacheSize     = 50000
	
	MaxConnections     = 10000
	ReadBufferSize     = 65536
	WriteBufferSize    = 65536
	MaxMessageSize     = 10485760
	
	DBWriteBuffer      = 64 * 1024 * 1024
	DBCacheSize        = 512 * 1024 * 1024
	DBMaxOpenFiles     = 1024
	
	// Consensus
	EpochLength        = 2016
	ValidatorSetSize   = 21
	MinStakeAmount     = 32000000000000000000 // 32 ZKK
	
	// Privacy
	MaxNullifiers     = 1000000
	MaxCommitments    = 1000000
	
	// Network
	MaxPeers          = 50
	PeerDiscoveryInterval = 30 * time.Second
	HandshakeTimeout  = 10 * time.Second
)

// ==================== CORE DATA STRUCTURES ====================

// Block represents a complete block in the zkKaia blockchain
type Block struct {
	Number          *big.Int                    `json:"number"`
	Hash            string                      `json:"hash"`
	ParentHash      string                      `json:"parentHash"`
	Timestamp       uint64                      `json:"timestamp"`
	GasLimit        uint64                      `json:"gasLimit"`
	GasUsed         uint64                      `json:"gasUsed"`
	BaseFee         *big.Int                    `json:"baseFeePerGas"`
	StateRoot       string                      `json:"stateRoot"`
	Transactions    []*Transaction              `json:"transactions"`
	ShieldedTxs     []*ShieldedTransaction      `json:"shieldedTxs"`
	ZKProofs        []*PrivateTransactionProof  `json:"zkProofs"`
	RollupBatch     *RollupBatch                `json:"rollupBatch"`
	Miner           string                      `json:"miner"`
	Nonce           uint64                      `json:"nonce"`
	ZKProof         *ZKProof                    `json:"zkProof"`
	Difficulty      *big.Int                    `json:"difficulty"`
	TotalDifficulty *big.Int                    `json:"totalDifficulty"`
	ExtraData       []byte                      `json:"extraData"`
	LogsBloom       string                      `json:"logsBloom"`
	ReceiptsRoot    string                      `json:"receiptsRoot"`
	TransactionsRoot string                     `json:"transactionsRoot"`
	Uncles          []string                    `json:"uncles"`
	Size            uint64                      `json:"size"`
}

// Transaction represents a blockchain transaction
type Transaction struct {
	Hash        string          `json:"hash"`
	From        string          `json:"from"`
	To          string          `json:"to"`
	Value       *big.Int        `json:"value"`
	Gas         uint64          `json:"gas"`
	GasPrice    *big.Int        `json:"gasPrice"`
	Input       []byte          `json:"input"`
	ZKProof     []byte          `json:"zkProof"`
	Nonce       uint64          `json:"nonce"`
	Status      uint64          `json:"status"`
	BlockHash   string          `json:"blockHash"`
	BlockNumber *big.Int        `json:"blockNumber"`
	TransactionIndex uint64     `json:"transactionIndex"`
	V           *big.Int        `json:"v"`
	R           *big.Int        `json:"r"`
	S           *big.Int        `json:"s"`
	Type        uint8           `json:"type"`
	ChainID     *big.Int        `json:"chainId"`
	AccessList  types.AccessList `json:"accessList"`
}

// Validator represents a network validator
type Validator struct {
	Address      common.Address `json:"address"`
	Stake        *big.Int       `json:"stake"`
	Active       bool           `json:"active"`
	BlocksMined  uint64         `json:"blocksMined"`
	TotalRewards *big.Int       `json:"totalRewards"`
	VotingPower  *big.Int       `json:"votingPower"`
	Commission   uint16         `json:"commission"` // 0-10000 (basis points)
}

// ZKProof represents a zero-knowledge proof
type ZKProof struct {
	Proof        []byte   `json:"proof"`
	Inputs       []string `json:"inputs"`
	PublicInputs []string `json:"publicInputs"`
	Timestamp    int64    `json:"timestamp"`
	Prover       string   `json:"prover"`
	CircuitID    string   `json:"circuitId"`
	VerificationKey []byte `json:"verificationKey"`
}

// ==================== ADAPTIVE WORKER POOL ====================
type AdaptiveWorkerPool struct {
	minWorkers    int
	maxWorkers    int
	currentWorkers int32
	taskQueue     chan func()
	wg            sync.WaitGroup
	stopChan      chan struct{}
	metrics       *PoolMetrics
	mu            sync.RWMutex
	scaleUpThreshold float64
	scaleDownThreshold float64
}

type PoolMetrics struct {
	TasksProcessed  uint64
	TasksQueued     uint64
	ActiveWorkers   int32
	QueueUtilization float64
	ScaleEvents     uint32
}

func NewAdaptiveWorkerPool(minWorkers, maxWorkers int) *AdaptiveWorkerPool {
	if minWorkers < 1 {
		minWorkers = 1
	}
	if maxWorkers < minWorkers {
		maxWorkers = minWorkers * 4
	}
	
	awp := &AdaptiveWorkerPool{
		minWorkers:     minWorkers,
		maxWorkers:     maxWorkers,
		currentWorkers: int32(minWorkers),
		taskQueue:      make(chan func(), maxWorkers*100),
		stopChan:       make(chan struct{}),
		metrics:        &PoolMetrics{},
		scaleUpThreshold: 0.7,
		scaleDownThreshold: 0.3,
	}
	
	for i := 0; i < minWorkers; i++ {
		awp.addWorker()
	}
	
	go awp.monitor()
	go awp.collectMetrics()
	return awp
}

func (awp *AdaptiveWorkerPool) addWorker() {
	awp.wg.Add(1)
	atomic.AddInt32(&awp.currentWorkers, 1)
	atomic.AddInt32(&awp.metrics.ActiveWorkers, 1)
	
	go func() {
		defer awp.wg.Done()
		defer atomic.AddInt32(&awp.currentWorkers, -1)
		defer atomic.AddInt32(&awp.metrics.ActiveWorkers, -1)
		
		for {
			select {
			case task := <-awp.taskQueue:
				start := time.Now()
				task()
				atomic.AddUint64(&awp.metrics.TasksProcessed, 1)
				_ = start // Could track task duration
			case <-awp.stopChan:
				return
			}
		}
	}()
}

func (awp *AdaptiveWorkerPool) monitor() {
	ticker := time.NewTicker(5 * time.Second)
	defer ticker.Stop()
	
	for {
		select {
		case <-ticker.C:
			awp.adjustWorkers()
		case <-awp.stopChan:
			return
		}
	}
}

func (awp *AdaptiveWorkerPool) adjustWorkers() {
	queueLen := len(awp.taskQueue)
	queueCap := cap(awp.taskQueue)
	utilization := float64(queueLen) / float64(queueCap)
	awp.metrics.QueueUtilization = utilization
	
	currentWorkers := atomic.LoadInt32(&awp.currentWorkers)
	
	if utilization > awp.scaleUpThreshold && int(currentWorkers) < awp.maxWorkers {
		// Scale up aggressively
		newWorkers := int(float64(currentWorkers) * 1.5)
		if newWorkers > awp.maxWorkers {
			newWorkers = awp.maxWorkers
		}
		toAdd := newWorkers - int(currentWorkers)
		for i := 0; i < toAdd; i++ {
			awp.addWorker()
		}
		atomic.AddUint32(&awp.metrics.ScaleEvents, 1)
		log.Printf("Scaled UP workers: %d -> %d (utilization: %.2f)", currentWorkers, newWorkers, utilization)
	} else if utilization < awp.scaleDownThreshold && int(currentWorkers) > awp.minWorkers {
		// Scale down conservatively
		newWorkers := int(float64(currentWorkers) * 0.8)
		if newWorkers < awp.minWorkers {
			newWorkers = awp.minWorkers
		}
		toRemove := int(currentWorkers) - newWorkers
		for i := 0; i < toRemove; i++ {
			// Workers will naturally exit when no tasks are available
			// due to the channel being empty
			select {
			case awp.taskQueue <- func() {}: // No-op task to allow worker to exit
			default:
			}
		}
		atomic.AddUint32(&awp.metrics.ScaleEvents, 1)
		log.Printf("Scaled DOWN workers: %d -> %d (utilization: %.2f)", currentWorkers, newWorkers, utilization)
	}
}

func (awp *AdaptiveWorkerPool) collectMetrics() {
	ticker := time.NewTicker(30 * time.Second)
	defer ticker.Stop()
	
	for {
		select {
		case <-ticker.C:
			// Metrics are updated atomically, no need for additional collection
		case <-awp.stopChan:
			return
		}
	}
}

func (awp *AdaptiveWorkerPool) Submit(task func()) {
	atomic.AddUint64(&awp.metrics.TasksQueued, 1)
	select {
	case awp.taskQueue <- task:
	case <-time.After(100 * time.Millisecond):
		// Task submission timeout - could implement retry or rejection logic
		log.Printf("Worker pool task submission timeout")
	}
}

func (awp *AdaptiveWorkerPool) SubmitWithPriority(task func(), priority int) {
	// Simple priority implementation
	if priority > 0 {
		go func() {
			time.Sleep(time.Duration(priority) * time.Millisecond)
			awp.Submit(task)
		}()
	} else {
		awp.Submit(task)
	}
}

func (awp *AdaptiveWorkerPool) Stop() {
	close(awp.stopChan)
	awp.wg.Wait()
}

func (awp *AdaptiveWorkerPool) GetMetrics() *PoolMetrics {
	return &PoolMetrics{
		TasksProcessed:   atomic.LoadUint64(&awp.metrics.TasksProcessed),
		TasksQueued:      atomic.LoadUint64(&awp.metrics.TasksQueued),
		ActiveWorkers:    atomic.LoadInt32(&awp.metrics.ActiveWorkers),
		QueueUtilization: awp.metrics.QueueUtilization,
		ScaleEvents:      atomic.LoadUint32(&awp.metrics.ScaleEvents),
	}
}

// ==================== DYNAMIC MEMPOOL ====================
type DynamicMemPool struct {
	transactions map[string]*Transaction
	pending      []*Transaction
	priorityQueue *TxPriorityQueue
	currentSize  int32
	maxSize      int32
	growthRate   int
	mu           sync.RWMutex
	metrics      *MemPoolMetrics
	evictionPolicy EvictionPolicy
}

type MemPoolMetrics struct {
	TotalReceived uint64
	TotalRemoved  uint64
	CurrentSize   int32
	PeakSize      int32
	Rejections    uint64
	Evictions     uint64
}

type EvictionPolicy int

const (
	EvictionFIFO EvictionPolicy = iota
	EvictionLRU
	EvictionGasPrice
)

type TxPriorityQueue []*Transaction

func (pq TxPriorityQueue) Len() int { return len(pq) }
func (pq TxPriorityQueue) Less(i, j int) bool {
	// Higher gas price transactions have higher priority
	return pq[i].GasPrice.Cmp(pq[j].GasPrice) > 0
}
func (pq TxPriorityQueue) Swap(i, j int) { pq[i], pq[j] = pq[j], pq[i] }
func (pq *TxPriorityQueue) Push(x interface{}) { *pq = append(*pq, x.(*Transaction)) }
func (pq *TxPriorityQueue) Pop() interface{} {
	old := *pq
	n := len(old)
	item := old[n-1]
	*pq = old[0 : n-1]
	return item
}

func NewDynamicMemPool() *DynamicMemPool {
	pq := make(TxPriorityQueue, 0)
	heap.Init(&pq)
	
	return &DynamicMemPool{
		transactions: make(map[string]*Transaction),
		pending:      make([]*Transaction, 0),
		priorityQueue: &pq,
		currentSize:  0,
		maxSize:      int32(InitialMemPoolSize),
		growthRate:   MemPoolGrowthRate,
		metrics:      &MemPoolMetrics{},
		evictionPolicy: EvictionGasPrice,
	}
}

func (dmp *DynamicMemPool) AddTransaction(tx *Transaction) error {
	dmp.mu.Lock()
	defer dmp.mu.Unlock()
	
	currentSize := atomic.LoadInt32(&dmp.currentSize)
	maxSize := atomic.LoadInt32(&dmp.maxSize)
	
	// Auto-expand if approaching capacity
	if float64(currentSize)/float64(maxSize) > 0.8 {
		newSize := maxSize * int32(dmp.growthRate)
		if newSize > int32(MaxMemPoolSize) {
			newSize = int32(MaxMemPoolSize)
		}
		if newSize > maxSize {
			atomic.StoreInt32(&dmp.maxSize, newSize)
			log.Printf("MemPool expanded: %d -> %d", maxSize, newSize)
		}
	}
	
	// Check if at maximum capacity
	if currentSize >= int32(MaxMemPoolSize) {
		// Try to evict low-priority transactions
		if !dmp.evictLowPriority() {
			atomic.AddUint64(&dmp.metrics.Rejections, 1)
			return fmt.Errorf("mempool at maximum capacity")
		}
	}
	
	// Check for duplicate transaction
	if _, exists := dmp.transactions[tx.Hash]; exists {
		return fmt.Errorf("transaction already in mempool")
	}
	
	// Validate transaction basics
	if tx.GasPrice.Cmp(big.NewInt(0)) <= 0 {
		return fmt.Errorf("invalid gas price")
	}
	if tx.Gas == 0 {
		return fmt.Errorf("invalid gas limit")
	}
	
	// Add to all data structures
	dmp.transactions[tx.Hash] = tx
	dmp.pending = append(dmp.pending, tx)
	heap.Push(dmp.priorityQueue, tx)
	
	newSize := atomic.AddInt32(&dmp.currentSize, 1)
	atomic.AddUint64(&dmp.metrics.TotalReceived, 1)
	
	if newSize > atomic.LoadInt32(&dmp.metrics.PeakSize) {
		atomic.StoreInt32(&dmp.metrics.PeakSize, newSize)
	}
	
	return nil
}

func (dmp *DynamicMemPool) evictLowPriority() bool {
	if dmp.priorityQueue.Len() == 0 {
		return false
	}
	
	// Remove the lowest priority transaction
	lowestPriorityTx := heap.Pop(dmp.priorityQueue).(*Transaction)
	delete(dmp.transactions, lowestPriorityTx.Hash)
	
	// Remove from pending slice
	for i, tx := range dmp.pending {
		if tx.Hash == lowestPriorityTx.Hash {
			dmp.pending = append(dmp.pending[:i], dmp.pending[i+1:]...)
			break
		}
	}
	
	atomic.AddInt32(&dmp.currentSize, -1)
	atomic.AddUint64(&dmp.metrics.TotalRemoved, 1)
	atomic.AddUint64(&dmp.metrics.Evictions, 1)
	
	return true
}

func (dmp *DynamicMemPool) RemoveTransaction(hash string) bool {
	dmp.mu.Lock()
	defer dmp.mu.Unlock()
	
	tx, exists := dmp.transactions[hash]
	if !exists {
		return false
	}
	
	delete(dmp.transactions, hash)
	atomic.AddInt32(&dmp.currentSize, -1)
	atomic.AddUint64(&dmp.metrics.TotalRemoved, 1)
	
	// Remove from pending slice
	for i, pendingTx := range dmp.pending {
		if pendingTx.Hash == hash {
			dmp.pending = append(dmp.pending[:i], dmp.pending[i+1:]...)
			break
		}
	}
	
	// Note: Cannot efficiently remove from priority queue without rebuilding
	// This is acceptable for our use case
	
	return true
}

func (dmp *DynamicMemPool) GetPending(limit int) []*Transaction {
	dmp.mu.RLock()
	defer dmp.mu.RUnlock()
	
	if limit <= 0 || limit > len(dmp.pending) {
		limit = len(dmp.pending)
	}
	
	result := make([]*Transaction, limit)
	copy(result, dmp.pending[:limit])
	return result
}

func (dmp *DynamicMemPool) GetPriorityPending(limit int) []*Transaction {
	dmp.mu.RLock()
	defer dmp.mu.RUnlock()
	
	if limit <= 0 || limit > dmp.priorityQueue.Len() {
		limit = dmp.priorityQueue.Len()
	}
	
	result := make([]*Transaction, limit)
	
	// Create a copy of the priority queue to avoid modifying the original
	tempQueue := make(TxPriorityQueue, dmp.priorityQueue.Len())
	copy(tempQueue, *dmp.priorityQueue)
	heap.Init(&tempQueue)
	
	for i := 0; i < limit; i++ {
		result[i] = heap.Pop(&tempQueue).(*Transaction)
	}
	
	return result
}

func (dmp *DynamicMemPool) GetTransaction(hash string) (*Transaction, bool) {
	dmp.mu.RLock()
	defer dmp.mu.RUnlock()
	tx, exists := dmp.transactions[hash]
	return tx, exists
}

func (dmp *DynamicMemPool) GetMetrics() *MemPoolMetrics {
	return &MemPoolMetrics{
		TotalReceived: atomic.LoadUint64(&dmp.metrics.TotalReceived),
		TotalRemoved:  atomic.LoadUint64(&dmp.metrics.TotalRemoved),
		CurrentSize:   atomic.LoadInt32(&dmp.currentSize),
		PeakSize:      atomic.LoadInt32(&dmp.metrics.PeakSize),
		Rejections:    atomic.LoadUint64(&dmp.metrics.Rejections),
		Evictions:     atomic.LoadUint64(&dmp.metrics.Evictions),
	}
}

// ==================== ZK PRIVACY CIRCUITS ====================
type PrivateTransactionCircuit struct {
	SenderPrivateKey   frontend.Variable `gnark:"senderPrivKey,secret"`
	SenderBalance      frontend.Variable `gnark:"senderBalance,secret"`
	ReceiverAddress    frontend.Variable `gnark:"receiverAddr,secret"`
	TransferAmount     frontend.Variable `gnark:"amount,secret"`
	SenderNonce        frontend.Variable `gnark:"nonce,secret"`
	SenderCommitment   frontend.Variable `gnark:"senderCommit,public"`
	ReceiverCommitment frontend.Variable `gnark:"receiverCommit,public"`
	NullifierHash      frontend.Variable `gnark:"nullifier,public"`
	RootHash           frontend.Variable `gnark:"rootHash,public"`
	Fee                frontend.Variable `gnark:"fee,secret"`
	Timestamp          frontend.Variable `gnark:"timestamp,secret"`
}

func (circuit *PrivateTransactionCircuit) Define(api frontend.API) error {
	// Initialize curve and hash functions
	curve, err := gnarkeddsa.NewEdCurve(api, ecc.BN254)
	if err != nil {
		return err
	}
	
	params := curve.Params()
	basePoint := gnarkeddsa.Point{X: params.Base[0], Y: params.Base[1]}
	
	// Verify sender public key derivation
	senderPubKey := curve.ScalarMul(basePoint, circuit.SenderPrivateKey)
	
	// Verify sender has sufficient balance (amount + fee)
	totalCost := api.Add(circuit.TransferAmount, circuit.Fee)
	api.AssertIsLessOrEqual(totalCost, circuit.SenderBalance)
	
	// Verify sender commitment
	senderHash, _ := mimc.NewMiMC(api)
	senderHash.Write(circuit.SenderPrivateKey, circuit.SenderNonce, circuit.Timestamp)
	computedSenderCommit := senderHash.Sum()
	api.AssertIsEqual(computedSenderCommit, circuit.SenderCommitment)
	
	// Verify receiver commitment
	receiverHash, _ := mimc.NewMiMC(api)
	receiverHash.Write(circuit.ReceiverAddress, circuit.TransferAmount, circuit.Timestamp)
	computedReceiverCommit := receiverHash.Sum()
	api.AssertIsEqual(computedReceiverCommit, circuit.ReceiverCommitment)
	
	// Verify nullifier (prevent double spends)
	nullifierHash, _ := mimc.NewMiMC(api)
	nullifierHash.Write(circuit.SenderPrivateKey, circuit.SenderNonce, circuit.TransferAmount, circuit.Timestamp)
	computedNullifier := nullifierHash.Sum()
	api.AssertIsEqual(computedNullifier, circuit.NullifierHash)
	
	// Verify merkle root inclusion (simplified)
	merkleHash, _ := mimc.NewMiMC(api)
	merkleHash.Write(computedSenderCommit, circuit.SenderBalance, circuit.Timestamp)
	leafHash := merkleHash.Sum()
	api.AssertIsEqual(leafHash, circuit.RootHash)
	
	// Additional constraints for security
	api.AssertIsDifferent(circuit.TransferAmount, 0) // Non-zero amount
	api.AssertIsLessOrEqual(circuit.Fee, circuit.TransferAmount) // Reasonable fee
	
	return nil
}

type ZKKSPPrivacyProver struct {
	ProvingKey       groth16.ProvingKey
	VerifyingKey     groth16.VerifyingKey
	ConstraintSystem constraint.ConstraintSystem
	MerkleTree       *MerkleTree
	Nullifiers       map[string]bool
	Commitments      map[string]bool
	ProofCache       *ProofCache
	WorkerPool       *AdaptiveWorkerPool
	mu               sync.RWMutex
	metrics          *ZKProverMetrics
}

type PrivateTransactionProof struct {
	Proof              groth16.Proof `json:"proof"`
	SenderCommitment   *big.Int      `json:"senderCommit"`
	ReceiverCommitment *big.Int      `json:"receiverCommit"`
	NullifierHash      *big.Int      `json:"nullifier"`
	RootHash           *big.Int      `json:"rootHash"`
	Timestamp          int64         `json:"timestamp"`
	PublicSignals      []string      `json:"publicSignals"`
}

type ShieldedTransaction struct {
	Commitment    string                   `json:"commitment"`
	Proof         *PrivateTransactionProof `json:"proof"`
	EncryptedData []byte                   `json:"encryptedData"`
	Nullifier     string                   `json:"nullifier"`
	Amount        *big.Int                 `json:"amount"`
	Fee           *big.Int                 `json:"fee"`
	Timestamp     int64                    `json:"timestamp"`
}

type ProofCache struct {
	cache     map[string]groth16.Proof
	maxSize   int
	evictions uint64
	hits      uint64
	misses    uint64
	mu        sync.RWMutex
}

type ZKProverMetrics struct {
	ProofsGenerated uint64
	ProofsVerified  uint64
	ProofTimeAvg    time.Duration
	CacheHits       uint64
	CacheMisses     uint64
	Errors          uint64
}

func NewProofCache(maxSize int) *ProofCache {
	return &ProofCache{
		cache:   make(map[string]groth16.Proof),
		maxSize: maxSize,
	}
}

func (pc *ProofCache) Get(key string) (groth16.Proof, bool) {
	pc.mu.RLock()
	defer pc.mu.RUnlock()
	proof, exists := pc.cache[key]
	if exists {
		atomic.AddUint64(&pc.hits, 1)
	} else {
		atomic.AddUint64(&pc.misses, 1)
	}
	return proof, exists
}

func (pc *ProofCache) Set(key string, proof groth16.Proof) {
	pc.mu.Lock()
	defer pc.mu.Unlock()
	
	if len(pc.cache) >= pc.maxSize {
		// Simple eviction: remove random element
		for k := range pc.cache {
			delete(pc.cache, k)
			atomic.AddUint64(&pc.evictions, 1)
			break
		}
	}
	pc.cache[key] = proof
}

func (pc *ProofCache) GetMetrics() (hits, misses, evictions uint64) {
	pc.mu.RLock()
	defer pc.mu.RUnlock()
	return atomic.LoadUint64(&pc.hits), atomic.LoadUint64(&pc.misses), atomic.LoadUint64(&pc.evictions)
}

func NewZKKSPPrivacyProver() (*ZKKSPPrivacyProver, error) {
	// Compile the circuit
	circuit := &PrivateTransactionCircuit{}
	ccs, err := frontend.Compile(ecc.BN254.ScalarField(), r1cs.NewBuilder, circuit)
	if err != nil {
		return nil, fmt.Errorf("failed to compile circuit: %v", err)
	}
	
	// Generate proving and verification keys
	pk, vk, err := groth16.Setup(ccs)
	if err != nil {
		return nil, fmt.Errorf("failed to setup keys: %v", err)
	}
	
	return &ZKKSPPrivacyProver{
		ProvingKey:       pk,
		VerifyingKey:     vk,
		ConstraintSystem: ccs,
		MerkleTree:       NewMerkleTree(32), // 32-level merkle tree
		Nullifiers:       make(map[string]bool),
		Commitments:      make(map[string]bool),
		ProofCache:       NewProofCache(ProofCacheSize),
		WorkerPool:       NewAdaptiveWorkerPool(ProofWorkers, ProofWorkers*2),
		metrics:          &ZKProverMetrics{},
	}, nil
}

func (p *ZKKSPPrivacyProver) CreateShieldedTransaction(
	senderPrivKey *big.Int, 
	senderBalance *big.Int, 
	receiverAddr common.Address, 
	amount *big.Int, 
	fee *big.Int,
	nonce uint64,
) (*ShieldedTransaction, error) {
	
	startTime := time.Now()
	
	p.mu.Lock()
	defer p.mu.Unlock()
	
	// Validate inputs
	if senderPrivKey == nil || senderBalance == nil || amount == nil || fee == nil {
		return nil, fmt.Errorf("invalid input parameters")
	}
	
	if amount.Cmp(big.NewInt(0)) <= 0 {
		return nil, fmt.Errorf("amount must be positive")
	}
	
	if senderBalance.Cmp(new(big.Int).Add(amount, fee)) < 0 {
		return nil, fmt.Errorf("insufficient balance")
	}
	
	receiverInt := new(big.Int).SetBytes(receiverAddr.Bytes())
	nonceInt := big.NewInt(int64(nonce))
	timestamp := big.NewInt(time.Now().Unix())
	
	// Compute commitments and nullifier
	senderCommit := computeCommitmentHash(senderPrivKey, nonceInt, timestamp)
	receiverCommit := computeCommitmentHash(receiverInt, amount, timestamp)
	nullifier := computeNullifierHash(senderPrivKey, nonceInt, amount, timestamp)
	nullifierHex := hex.EncodeToString(nullifier.Bytes())
	
	// Check for double spend
	if p.Nullifiers[nullifierHex] {
		atomic.AddUint64(&p.metrics.Errors, 1)
		return nil, fmt.Errorf("double-spend detected")
	}
	
	// Get current merkle root
	rootHash := p.MerkleTree.GetRoot()
	
	// Create circuit assignment
	assignment := &PrivateTransactionCircuit{
		SenderPrivateKey:   frontend.Variable(senderPrivKey),
		SenderBalance:      frontend.Variable(senderBalance),
		ReceiverAddress:    frontend.Variable(receiverInt),
		TransferAmount:     frontend.Variable(amount),
		SenderNonce:        frontend.Variable(nonceInt),
		SenderCommitment:   frontend.Variable(senderCommit),
		ReceiverCommitment: frontend.Variable(receiverCommit),
		NullifierHash:      frontend.Variable(nullifier),
		RootHash:           frontend.Variable(rootHash),
		Fee:                frontend.Variable(fee),
		Timestamp:          frontend.Variable(timestamp),
	}
	
	// Generate witness
	w, err := frontend.NewWitness(assignment, ecc.BN254.ScalarField())
	if err != nil {
		atomic.AddUint64(&p.metrics.Errors, 1)
		return nil, fmt.Errorf("failed to create witness: %v", err)
	}
	
	// Generate proof
	proof, err := groth16.Prove(p.ConstraintSystem, p.ProvingKey, w)
	if err != nil {
		atomic.AddUint64(&p.metrics.Errors, 1)
		return nil, fmt.Errorf("failed to generate proof: %v", err)
	}
	
	// Cache the proof
	p.ProofCache.Set(nullifierHex, proof)
	
	// Create private transaction proof
	privProof := &PrivateTransactionProof{
		Proof:              proof,
		SenderCommitment:   senderCommit,
		ReceiverCommitment: receiverCommit,
		NullifierHash:      nullifier,
		RootHash:           rootHash,
		Timestamp:          time.Now().Unix(),
		PublicSignals: []string{
			senderCommit.String(),
			receiverCommit.String(),
			nullifier.String(),
			rootHash.String(),
		},
	}
	
	// Encrypt transaction data
	encryptedData, err := encryptTransactionData(receiverAddr, amount, fee, nonce)
	if err != nil {
		atomic.AddUint64(&p.metrics.Errors, 1)
		return nil, fmt.Errorf("failed to encrypt data: %v", err)
	}
	
	// Update state
	p.Nullifiers[nullifierHex] = true
	p.Commitments[hex.EncodeToString(senderCommit.Bytes())] = true
	
	// Update metrics
	atomic.AddUint64(&p.metrics.ProofsGenerated, 1)
	proofTime := time.Since(startTime)
	p.metrics.ProofTimeAvg = time.Duration(
		(int64(p.metrics.ProofTimeAvg)*int64(atomic.LoadUint64(&p.metrics.ProofsGenerated)-1) + proofTime.Nanoseconds()) /
		int64(atomic.LoadUint64(&p.metrics.ProofsGenerated)),
	)
	
	return &ShieldedTransaction{
		Commitment:    hex.EncodeToString(senderCommit.Bytes()),
		Proof:         privProof,
		EncryptedData: encryptedData,
		Nullifier:     nullifierHex,
		Amount:        amount,
		Fee:           fee,
		Timestamp:     time.Now().Unix(),
	}, nil
}

func (p *ZKKSPPrivacyProver) VerifyShieldedTransaction(stx *ShieldedTransaction) (bool, error) {
	startTime := time.Now()
	
	p.mu.RLock()
	defer p.mu.RUnlock()
	
	// Check nullifier
	if p.Nullifiers[stx.Nullifier] {
		atomic.AddUint64(&p.metrics.Errors, 1)
		return false, fmt.Errorf("nullifier already used")
	}
	
	// Try to get proof from cache
	if cachedProof, exists := p.ProofCache.Get(stx.Nullifier); exists {
		stx.Proof.Proof = cachedProof
	}
	
	// Create public witness
	publicAssignment := &PrivateTransactionCircuit{
		SenderCommitment:   frontend.Variable(stx.Proof.SenderCommitment),
		ReceiverCommitment: frontend.Variable(stx.Proof.ReceiverCommitment),
		NullifierHash:      frontend.Variable(stx.Proof.NullifierHash),
		RootHash:           frontend.Variable(stx.Proof.RootHash),
	}
	
	publicWitness, err := frontend.NewWitness(publicAssignment, ecc.BN254.ScalarField(), frontend.PublicOnly())
	if err != nil {
		atomic.AddUint64(&p.metrics.Errors, 1)
		return false, fmt.Errorf("failed to create public witness: %v", err)
	}
	
	// Verify proof
	err = groth16.Verify(stx.Proof.Proof, p.VerifyingKey, publicWitness)
	
	// Update metrics
	atomic.AddUint64(&p.metrics.ProofsVerified, 1)
	verifyTime := time.Since(startTime)
	
	if err != nil {
		atomic.AddUint64(&p.metrics.Errors, 1)
		return false, fmt.Errorf("proof verification failed: %v", err)
	}
	
	return true, nil
}

func (p *ZKKSPPrivacyProver) GetMetrics() *ZKProverMetrics {
	return &ZKProverMetrics{
		ProofsGenerated: atomic.LoadUint64(&p.metrics.ProofsGenerated),
		ProofsVerified:  atomic.LoadUint64(&p.metrics.ProofsVerified),
		ProofTimeAvg:    p.metrics.ProofTimeAvg,
		CacheHits:       atomic.LoadUint64(&p.ProofCache.hits),
		CacheMisses:     atomic.LoadUint64(&p.ProofCache.misses),
		Errors:          atomic.LoadUint64(&p.metrics.Errors),
	}
}

// ==================== MERKLE TREE IMPLEMENTATION ====================
type MerkleTree struct {
	Levels    int
	Leaves    [][]byte
	Nodes     [][][]byte
	Root      []byte
	mu        sync.RWMutex
}

func NewMerkleTree(levels int) *MerkleTree {
	if levels < 1 {
		levels = 32
	}
	return &MerkleTree{
		Levels: levels,
		Leaves: make([][]byte, 0),
		Nodes:  make([][][]byte, levels+1),
	}
}

func (mt *MerkleTree) AddLeaf(data []byte) int {
	mt.mu.Lock()
	defer mt.mu.Unlock()
	
	// Hash the leaf data
	leafHash := sha256.Sum256(data)
	mt.Leaves = append(mt.Leaves, leafHash[:])
	
	// Recompute the tree
	mt.computeTree()
	
	return len(mt.Leaves) - 1
}

func (mt *MerkleTree) computeTree() {
	if len(mt.Leaves) == 0 {
		mt.Root = make([]byte, 32)
		return
	}
	
	// Initialize level 0 with leaves
	mt.Nodes[0] = make([][]byte, len(mt.Leaves))
	copy(mt.Nodes[0], mt.Leaves)
	
	// Compute each level
	for level := 1; level <= mt.Levels; level++ {
		prevLevel := mt.Nodes[level-1]
		currentLevel := make([][]byte, (len(prevLevel)+1)/2)
		
		for i := 0; i < len(prevLevel); i += 2 {
			if i+1 < len(prevLevel) {
				// Hash two nodes together
				combined := append(prevLevel[i], prevLevel[i+1]...)
				hash := sha256.Sum256(combined)
				currentLevel[i/2] = hash[:]
			} else {
				// Odd number of nodes, duplicate the last one
				combined := append(prevLevel[i], prevLevel[i]...)
				hash := sha256.Sum256(combined)
				currentLevel[i/2] = hash[:]
			}
		}
		
		mt.Nodes[level] = currentLevel
	}
	
	// Set the root
	if len(mt.Nodes[mt.Levels]) > 0 {
		mt.Root = mt.Nodes[mt.Levels][0]
	} else {
		mt.Root = make([]byte, 32)
	}
}

func (mt *MerkleTree) GetRoot() *big.Int {
	mt.mu.RLock()
	defer mt.mu.RUnlock()
	if len(mt.Root) == 0 {
		return big.NewInt(0)
	}
	return new(big.Int).SetBytes(mt.Root)
}

func (mt *MerkleTree) GenerateProof(leafIndex int) ([][]byte, error) {
	mt.mu.RLock()
	defer mt.mu.RUnlock()
	
	if leafIndex < 0 || leafIndex >= len(mt.Leaves) {
		return nil, fmt.Errorf("invalid leaf index")
	}
	
	proof := make([][]byte, mt.Levels)
	index := leafIndex
	
	for level := 0; level < mt.Levels; level++ {
		levelNodes := mt.Nodes[level]
		if index%2 == 0 {
			// Right sibling
			if index+1 < len(levelNodes) {
				proof[level] = levelNodes[index+1]
			} else {
				proof[level] = levelNodes[index] // Self for padding
			}
		} else {
			// Left sibling
			proof[level] = levelNodes[index-1]
		}
		index = index / 2
	}
	
	return proof, nil
}

func (mt *MerkleTree) VerifyProof(leaf []byte, proof [][]byte, root []byte) bool {
	leafHash := sha256.Sum256(leaf)
	current := leafHash[:]
	
	for i := 0; i < len(proof); i++ {
		if proof[i] == nil {
			continue
		}
		
		// Determine order based on bit position
		if (uint32(i) & (1 << uint(i))) != 0 {
			current = sha256.Sum256(append(proof[i], current...))[:]
		} else {
			current = sha256.Sum256(append(current, proof[i]...))[:]
		}
	}
	
	return bytes.Equal(current, root)
}

// ==================== ENCRYPTION & CRYPTO UTILITIES ====================
func encryptTransactionData(receiver common.Address, amount, fee *big.Int, nonce uint64) ([]byte, error) {
	// Prepare plaintext data
	plaintext := append(receiver.Bytes(), amount.Bytes()...)
	plaintext = append(plaintext, fee.Bytes()...)
	nonceBytes := make([]byte, 8)
	binary.BigEndian.PutUint64(nonceBytes, nonce)
	plaintext = append(plaintext, nonceBytes...)
	
	// Derive encryption key from receiver address
	key := sha256.Sum256(receiver.Bytes())
	
	// Use AES-GCM for authenticated encryption
	block, err := aes.NewCipher(key[:])
	if err != nil {
		return nil, err
	}
	
	gcm, err := cipher.NewGCM(block)
	if err != nil {
		return nil, err
	}
	
	// Generate random nonce
	nonce = uint64(time.Now().UnixNano())
	nonceBuf := make([]byte, gcm.NonceSize())
	binary.BigEndian.PutUint64(nonceBuf, nonce)
	
	// Encrypt
	ciphertext := gcm.Seal(nonceBuf, nonceBuf, plaintext, nil)
	return ciphertext, nil
}

func decryptTransactionData(receiver common.Address, ciphertext []byte) (common.Address, *big.Int, *big.Int, uint64, error) {
	// Derive decryption key
	key := sha256.Sum256(receiver.Bytes())
	
	block, err := aes.NewCipher(key[:])
	if err != nil {
		return common.Address{}, nil, nil, 0, err
	}
	
	gcm, err := cipher.NewGCM(block)
	if err != nil {
		return common.Address{}, nil, nil, 0, err
	}
	
	nonceSize := gcm.NonceSize()
	if len(ciphertext) < nonceSize {
		return common.Address{}, nil, nil, 0, fmt.Errorf("ciphertext too short")
	}
	
	nonce, ciphertext := ciphertext[:nonceSize], ciphertext[nonceSize:]
	plaintext, err := gcm.Open(nil, nonce, ciphertext, nil)
	if err != nil {
		return common.Address{}, nil, nil, 0, err
	}
	
	// Parse plaintext
	if len(plaintext) < 20+32+32+8 {
		return common.Address{}, nil, nil, 0, fmt.Errorf("invalid plaintext length")
	}
	
	receiverAddr := common.BytesToAddress(plaintext[:20])
	amount := new(big.Int).SetBytes(plaintext[20:52])
	fee := new(big.Int).SetBytes(plaintext[52:84])
	nonceVal := binary.BigEndian.Uint64(plaintext[84:92])
	
	return receiverAddr, amount, fee, nonceVal, nil
}

func computeCommitmentHash(values ...*big.Int) *big.Int {
	data := []byte{}
	for _, v := range values {
		data = append(data, v.Bytes()...)
	}
	hash := sha256.Sum256(data)
	return new(big.Int).SetBytes(hash[:])
}

func computeNullifierHash(values ...*big.Int) *big.Int {
	data := []byte{}
	for _, v := range values {
		data = append(data, v.Bytes()...)
	}
	// Use double hash for nullifier
	hash1 := sha256.Sum256(data)
	hash2 := sha256.Sum256(hash1[:])
	return new(big.Int).SetBytes(hash2[:])
}

// ==================== GAS ABSTRACTION SYSTEM ====================
type GasAbstractionSystem struct {
	Enabled           bool
	WhitelistedTokens map[common.Address]*GasToken
	GaslessSwapRouter common.Address
	BundleManager     *BundleManager
	SponsoredAccounts map[common.Address]bool
	FreeTxsPerDay     uint64
	DailyTxCount      map[common.Address]uint64
	LastTxDate        map[common.Address]int64
	mu                sync.RWMutex
	metrics           *GasAbstractionMetrics
}

type GasToken struct {
	Address       common.Address
	Symbol        string
	Decimals      uint8
	MinSwapAmount *big.Int
	DEXRouter     common.Address
	PriceOracle   common.Address
	Enabled       bool
	ExchangeRate  *big.Int
}

type GaslessTransaction struct {
	From        common.Address
	To          common.Address
	Value       *big.Int
	Data        []byte
	GasToken    common.Address
	TokenAmount *big.Int
	Nonce       uint64
	Signature   []byte
	ZKProof     *PrivateTransactionProof
	GasLimit    uint64
	MaxFeePerGas *big.Int
	PriorityFee *big.Int
}

type TransactionBundle struct {
	LendTx     *types.Transaction
	ApproveTx  *types.Transaction
	SwapTx     *types.Transaction
	UserTx     *types.Transaction
	BundleHash common.Hash
	Atomic     bool
	Proposer   common.Address
	Status     string
	CreatedAt  time.Time
}

type BundleManager struct {
	PendingBundles   map[common.Hash]*TransactionBundle
	CompletedBundles map[common.Hash]*TransactionBundle
	FailedBundles    map[common.Hash]error
	mu               sync.RWMutex
}

type GasAbstractionMetrics struct {
	GaslessTxsProcessed uint64
	BundlesCreated      uint64
	BundlesCompleted    uint64
	BundlesFailed       uint64
	FreeTxsUsed         uint64
	SponsoredTxs        uint64
}

func NewGasAbstractionSystem() *GasAbstractionSystem {
	return &GasAbstractionSystem{
		Enabled:           true,
		WhitelistedTokens: make(map[common.Address]*GasToken),
		SponsoredAccounts: make(map[common.Address]bool),
		FreeTxsPerDay:     10,
		DailyTxCount:      make(map[common.Address]uint64),
		LastTxDate:        make(map[common.Address]int64),
		BundleManager:     NewBundleManager(),
		GaslessSwapRouter: common.HexToAddress("0x1234567890123456789012345678901234567890"),
		metrics:           &GasAbstractionMetrics{},
	}
}

func NewBundleManager() *BundleManager {
	return &BundleManager{
		PendingBundles:   make(map[common.Hash]*TransactionBundle),
		CompletedBundles: make(map[common.Hash]*TransactionBundle),
		FailedBundles:    make(map[common.Hash]error),
	}
}

func (gas *GasAbstractionSystem) ProcessGaslessTransaction(gtx *GaslessTransaction, proposer common.Address) (*TransactionBundle, error) {
	gas.mu.Lock()
	defer gas.mu.Unlock()
	
	// Validate gas token
	token, exists := gas.WhitelistedTokens[gtx.GasToken]
	if !exists || !token.Enabled {
		return nil, fmt.Errorf("token not whitelisted or disabled")
	}
	
	// Validate token amount covers gas costs
	estimatedGasCost := new(big.Int).Mul(big.NewInt(int64(gtx.GasLimit)), gtx.MaxFeePerGas)
	tokenValue := new(big.Int).Mul(gtx.TokenAmount, token.ExchangeRate)
	if tokenValue.Cmp(estimatedGasCost) < 0 {
		return nil, fmt.Errorf("insufficient token amount for gas")
	}
	
	// Create transaction bundle
	bundle := &TransactionBundle{
		LendTx: types.NewTransaction(
			gtx.Nonce,
			gtx.From,
			big.NewInt(1000000000000000000), // 1 ETH equivalent
			21000,
			gtx.MaxFeePerGas,
			nil,
		),
		ApproveTx: types.NewTransaction(
			gtx.Nonce+1,
			gtx.GasToken,
			big.NewInt(0),
			50000,
			gtx.MaxFeePerGas,
			encodeApproveData(gas.GaslessSwapRouter, gtx.TokenAmount),
		),
		SwapTx: types.NewTransaction(
			gtx.Nonce+2,
			token.DEXRouter,
			big.NewInt(0),
			150000,
			gtx.MaxFeePerGas,
			encodeSwapData(gtx.TokenAmount, estimatedGasCost),
		),
		UserTx: types.NewTransaction(
			gtx.Nonce+3,
			gtx.To,
			gtx.Value,
			gtx.GasLimit,
			gtx.MaxFeePerGas,
			gtx.Data,
		),
		Atomic:    true,
		Proposer:  proposer,
		Status:    "pending",
		CreatedAt: time.Now(),
	}
	
	// Calculate bundle hash
	bundleData, _ := rlp.EncodeToBytes([]interface{}{
		bundle.LendTx,
		bundle.ApproveTx,
		bundle.SwapTx,
		bundle.UserTx,
	})
	bundle.BundleHash = crypto.Keccak256Hash(bundleData)
	
	// Store bundle
	gas.BundleManager.PendingBundles[bundle.BundleHash] = bundle
	atomic.AddUint64(&gas.metrics.GaslessTxsProcessed, 1)
	atomic.AddUint64(&gas.metrics.BundlesCreated, 1)
	
	return bundle, nil
}

func encodeApproveData(spender common.Address, amount *big.Int) []byte {
	// ERC20 approve function signature
	approveSig := crypto.Keccak256([]byte("approve(address,uint256)"))[:4]
	spenderPadded := common.LeftPadBytes(spender.Bytes(), 32)
	amountPadded := common.LeftPadBytes(amount.Bytes(), 32)
	
	return append(approveSig, append(spenderPadded, amountPadded...)...)
}

func encodeSwapData(amountIn, amountOutMin *big.Int) []byte {
	// Simplified swap function
	swapSig := crypto.Keccak256([]byte("swapTokensForExactETH(uint256,uint256,address[],address,uint256)"))[:4]
	amountInPadded := common.LeftPadBytes(amountIn.Bytes(), 32)
	amountOutMinPadded := common.LeftPadBytes(amountOutMin.Bytes(), 32)
	
	// Simplified parameters
	path := make([]byte, 64) // Two tokens in path
	to := common.LeftPadBytes(common.HexToAddress("0x").Bytes(), 32)
	deadline := common.LeftPadBytes(big.NewInt(time.Now().Add(10*time.Minute).Unix()).Bytes(), 32)
	
	return append(swapSig, append(amountInPadded, append(amountOutMinPadded, append(path, append(to, deadline...)...)...)...)...)
}

func (gas *GasAbstractionSystem) CheckFreeTxEligibility(addr common.Address) bool {
	gas.mu.RLock()
	defer gas.mu.RUnlock()
	
	// Check if account is sponsored
	if gas.SponsoredAccounts[addr] {
		return true
	}
	
	// Check daily free transaction limit
	today := time.Now().Unix() / 86400
	lastDate := gas.LastTxDate[addr] / 86400
	
	if today != lastDate {
		// New day, reset counter
		gas.LastTxDate[addr] = time.Now().Unix()
		gas.DailyTxCount[addr] = 0
	}
	
	return gas.DailyTxCount[addr] < gas.FreeTxsPerDay
}

func (gas *GasAbstractionSystem) IncrementFreeTx(addr common.Address) {
	gas.mu.Lock()
	defer gas.mu.Unlock()
	gas.DailyTxCount[addr]++
	atomic.AddUint64(&gas.metrics.FreeTxsUsed, 1)
}

func (gas *GasAbstractionSystem) AddSponsoredAccount(addr common.Address) {
	gas.mu.Lock()
	defer gas.mu.Unlock()
	gas.SponsoredAccounts[addr] = true
	atomic.AddUint64(&gas.metrics.SponsoredTxs, 1)
}

func (gas *GasAbstractionSystem) AddGasToken(token *GasToken) {
	gas.mu.Lock()
	defer gas.mu.Unlock()
	gas.WhitelistedTokens[token.Address] = token
}

func (gas *GasAbstractionSystem) GetMetrics() *GasAbstractionMetrics {
	return &GasAbstractionMetrics{
		GaslessTxsProcessed: atomic.LoadUint64(&gas.metrics.GaslessTxsProcessed),
		BundlesCreated:      atomic.LoadUint64(&gas.metrics.BundlesCreated),
		BundlesCompleted:    atomic.LoadUint64(&gas.metrics.BundlesCompleted),
		BundlesFailed:       atomic.LoadUint64(&gas.metrics.BundlesFailed),
		FreeTxsUsed:         atomic.LoadUint64(&gas.metrics.FreeTxsUsed),
		SponsoredTxs:        atomic.LoadUint64(&gas.metrics.SponsoredTxs),
	}
}

// ==================== ROLLUP BATCH PROCESSOR ====================
type RollupBatchProcessor struct {
	BatchSize        int
	CurrentBatch     *RollupBatch
	CompletedBatches []*RollupBatch
	StateCompressor  *StateCompressor
	ZKProver         *ZKKSPPrivacyProver
	WorkerPool       *AdaptiveWorkerPool
	BatchTimeout     time.Duration
	mu               sync.RWMutex
	metrics          *RollupMetrics
}

type RollupBatch struct {
	BatchID        uint64
	Transactions   []*types.Transaction
	StateRoot      common.Hash
	PrevStateRoot  common.Hash
	CompressedData []byte
	ZKProof        *PrivateTransactionProof
	Timestamp      uint64
	BatchHash      common.Hash
	Size           uint64
	GasUsed        uint64
}

type StateCompressor struct {
	Algorithm string
	Level     int
}

type RollupMetrics struct {
	BatchesProcessed uint64
	TransactionsInBatches uint64
	AverageBatchSize float64
	CompressionRatio float64
	ProofGenerationTime time.Duration
}

func NewRollupBatchProcessor(zkProver *ZKKSPPrivacyProver) *RollupBatchProcessor {
	return &RollupBatchProcessor{
		BatchSize:        RollupBatchSize,
		CompletedBatches: make([]*RollupBatch, 0),
		StateCompressor:  &StateCompressor{Algorithm: "gzip", Level: 6},
		ZKProver:         zkProver,
		WorkerPool:       NewAdaptiveWorkerPool(ValidationWorkers, ValidationWorkers*2),
		BatchTimeout:     BatchTimeout,
		metrics:          &RollupMetrics{},
	}
}

func (rbp *RollupBatchProcessor) AddTransaction(tx *types.Transaction) {
	rbp.mu.Lock()
	defer rbp.mu.Unlock()
	
	if rbp.CurrentBatch == nil {
		rbp.CurrentBatch = &RollupBatch{
			BatchID:      uint64(len(rbp.CompletedBatches)),
			Transactions: make([]*types.Transaction, 0),
			Timestamp:    uint64(time.Now().Unix()),
		}
	}
	
	rbp.CurrentBatch.Transactions = append(rbp.CurrentBatch.Transactions, tx)
	
	if len(rbp.CurrentBatch.Transactions) >= rbp.BatchSize {
		rbp.finalizeBatch()
	}
}

func (rbp *RollupBatchProcessor) finalizeBatch() {
	if rbp.CurrentBatch == nil || len(rbp.CurrentBatch.Transactions) == 0 {
		return
	}
	
	startTime := time.Now()
	
	// Compress batch data
	rbp.CurrentBatch.CompressedData = rbp.StateCompressor.Compress(rbp.CurrentBatch.Transactions)
	
	// Calculate state root
	rbp.CurrentBatch.StateRoot = calculateStateRootFromTxs(rbp.CurrentBatch.Transactions)
	
	// Set previous state root
	if len(rbp.CompletedBatches) > 0 {
		rbp.CurrentBatch.PrevStateRoot = rbp.CompletedBatches[len(rbp.CompletedBatches)-1].StateRoot
	}
	
	// Calculate batch hash
	batchData := append(rbp.CurrentBatch.CompressedData, rbp.CurrentBatch.StateRoot.Bytes()...)
	rbp.CurrentBatch.BatchHash = crypto.Keccak256Hash(batchData)
	
	// Calculate batch size and gas used
	rbp.CurrentBatch.Size = uint64(len(rbp.CurrentBatch.CompressedData))
	for _, tx := range rbp.CurrentBatch.Transactions {
		rbp.CurrentBatch.GasUsed += tx.Gas()
	}
	
	// Generate ZK proof for batch validity (simplified)
	// In production, this would involve complex circuit proving the state transition
	
	rbp.CompletedBatches = append(rbp.CompletedBatches, rbp.CurrentBatch)
	
	// Update metrics
	atomic.AddUint64(&rbp.metrics.BatchesProcessed, 1)
	atomic.AddUint64(&rbp.metrics.TransactionsInBatches, uint64(len(rbp.CurrentBatch.Transactions)))
	
	batchTime := time.Since(startTime)
	rbp.metrics.ProofGenerationTime = time.Duration(
		(int64(rbp.metrics.ProofGenerationTime)*int64(atomic.LoadUint64(&rbp.metrics.BatchesProcessed)-1) + batchTime.Nanoseconds()) /
		int64(atomic.LoadUint64(&rbp.metrics.BatchesProcessed)),
	)
	
	rbp.CurrentBatch = nil
}

func (sc *StateCompressor) Compress(txs []*types.Transaction) []byte {
	data := make([][]byte, len(txs))
	for i, tx := range txs {
		txData, _ := tx.MarshalBinary()
		data[i] = txData
	}
	
	switch sc.Algorithm {
	case "gzip":
		return sc.compressGzip(data)
	case "msgpack":
		compressed, _ := msgpack.Marshal(data)
		return compressed
	default:
		compressed, _ := msgpack.Marshal(data)
		return compressed
	}
}

func (sc *StateCompressor) compressGzip(data [][]byte) []byte {
	var buf bytes.Buffer
	gz, _ := gzip.NewWriterLevel(&buf, sc.Level)
	
	encoded, _ := msgpack.Marshal(data)
	gz.Write(encoded)
	gz.Close()
	
	return buf.Bytes()
}

func calculateStateRootFromTxs(txs []*types.Transaction) common.Hash {
	data := []byte{}
	for _, tx := range txs {
		data = append(data, tx.Hash().Bytes()...)
	}
	return crypto.Keccak256Hash(data)
}

func (rbp *RollupBatchProcessor) GetMetrics() *RollupMetrics {
	batchesProcessed := atomic.LoadUint64(&rbp.metrics.BatchesProcessed)
	transactionsInBatches := atomic.LoadUint64(&rbp.metrics.TransactionsInBatches)
	
	var avgBatchSize float64
	if batchesProcessed > 0 {
		avgBatchSize = float64(transactionsInBatches) / float64(batchesProcessed)
	}
	
	return &RollupMetrics{
		BatchesProcessed:     batchesProcessed,
		TransactionsInBatches: transactionsInBatches,
		AverageBatchSize:     avgBatchSize,
		CompressionRatio:     rbp.metrics.CompressionRatio,
		ProofGenerationTime:  rbp.metrics.ProofGenerationTime,
	}
}

// ==================== DYNAMIC SHARD SYSTEM ====================
type DynamicShardManager struct {
	shards          []*Shard
	shardCount      int32
	maxShards       int
	router          *ShardRouter
	loadBalancer    *ShardLoadBalancer
	crossShardPool  *CrossShardTxPool
	mu              sync.RWMutex
	metrics         *ShardMetrics
}

type Shard struct {
	ID              uint64
	StateDB         *StateDB
	MemPool         *DynamicMemPool
	Blocks          []*Block
	CurrentBlock    *Block
	WorkerPool      *AdaptiveWorkerPool
	TxCount         uint64
	Load            float64
	GasUsed         uint64
	LastBlockTime   time.Time
	mu              sync.RWMutex
}

type ShardRouter struct {
	routingTable     map[common.Address]uint64
	loadDistribution map[uint64]*ShardLoad
	mu               sync.RWMutex
}

type ShardLoad struct {
	ShardID         uint64
	TxCount         uint64
	GasUsed         uint64
	AvgResponseTime time.Duration
	LoadScore       float64
}

type ShardLoadBalancer struct {
	shardManager *DynamicShardManager
	metrics      map[uint64]*ShardLoad
	mu           sync.RWMutex
}

type CrossShardTxPool struct {
	pending map[string]*CrossShardTx
	mu      sync.RWMutex
}

type CrossShardTx struct {
	SourceShard uint64
	DestShard   uint64
	Tx          *Transaction
	Proof       []byte
	Status      string
	CreatedAt   time.Time
}

type ShardMetrics struct {
	TotalShards      int32
	ActiveShards     int32
	CrossShardTxs    uint64
	ShardScalesUp    uint32
	ShardScalesDown  uint32
	AvgShardLoad     float64
}

func NewDynamicShardManager() *DynamicShardManager {
	dsm := &DynamicShardManager{
		shards:         make([]*Shard, InitialShards),
		shardCount:     int32(InitialShards),
		maxShards:      MaxShards,
		router:         NewShardRouter(),
		crossShardPool: NewCrossShardTxPool(),
		metrics:        &ShardMetrics{},
	}
	
	for i := 0; i < InitialShards; i++ {
		dsm.shards[i] = &Shard{
			ID:         uint64(i),
			StateDB:    NewStateDB(),
			MemPool:    NewDynamicMemPool(),
			Blocks:     make([]*Block, 0),
			WorkerPool: NewAdaptiveWorkerPool(WorkerPoolSize/InitialShards, WorkerPoolSize),
			LastBlockTime: time.Now(),
		}
	}
	
	dsm.loadBalancer = &ShardLoadBalancer{
		shardManager: dsm,
		metrics:      make(map[uint64]*ShardLoad),
	}
	
	go dsm.monitorAndScale()
	return dsm
}

func NewShardRouter() *ShardRouter {
	return &ShardRouter{
		routingTable:     make(map[common.Address]uint64),
		loadDistribution: make(map[uint64]*ShardLoad),
	}
}

func NewCrossShardTxPool() *CrossShardTxPool {
	return &CrossShardTxPool{
		pending: make(map[string]*CrossShardTx),
	}
}

func (dsm *DynamicShardManager) monitorAndScale() {
	ticker := time.NewTicker(30 * time.Second)
	defer ticker.Stop()
	
	for range ticker.C {
		dsm.checkAndScale()
	}
}

func (dsm *DynamicShardManager) checkAndScale() {
	dsm.mu.RLock()
	currentShards := atomic.LoadInt32(&dsm.shardCount)
	
	var totalLoad float64
	shardLoads := make([]float64, currentShards)
	
	for i := 0; i < int(currentShards); i++ {
		shard := dsm.shards[i]
		shard.mu.RLock()
		shardLoads[i] = shard.Load
		totalLoad += shard.Load
		shard.mu.RUnlock()
	}
	
	avgLoad := totalLoad / float64(currentShards)
	dsm.mu.RUnlock()
	
	// Check for scale up
	if avgLoad > 0.8 && int(currentShards) < dsm.maxShards {
		dsm.scaleUp()
		atomic.AddUint32(&dsm.metrics.ShardScalesUp, 1)
	}
	
	// Check for scale down
	if avgLoad < 0.3 && int(currentShards) > InitialShards {
		dsm.scaleDown()
		atomic.AddUint32(&dsm.metrics.ShardScalesDown, 1)
	}
	
	// Update metrics
	atomic.StoreInt32(&dsm.metrics.TotalShards, currentShards)
	atomic.StoreInt32(&dsm.metrics.ActiveShards, currentShards)
	dsm.metrics.AvgShardLoad = avgLoad
}

func (dsm *DynamicShardManager) scaleUp() {
	dsm.mu.Lock()
	defer dsm.mu.Unlock()
	
	currentCount := int(atomic.LoadInt32(&dsm.shardCount))
	newCount := currentCount * 2
	if newCount > dsm.maxShards {
		newCount = dsm.maxShards
	}
	
	if newCount <= currentCount {
		return
	}
	
	log.Printf("Scaling UP shards: %d -> %d", currentCount, newCount)
	
	// Create new shards
	for i := currentCount; i < newCount; i++ {
		newShard := &Shard{
			ID:         uint64(i),
			StateDB:    NewStateDB(),
			MemPool:    NewDynamicMemPool(),
			Blocks:     make([]*Block, 0),
			WorkerPool: NewAdaptiveWorkerPool(WorkerPoolSize/newCount, WorkerPoolSize),
			LastBlockTime: time.Now(),
		}
		dsm.shards = append(dsm.shards, newShard)
	}
	
	atomic.StoreInt32(&dsm.shardCount, int32(newCount))
	
	// Rebalance addresses to new shards
	dsm.rebalanceShards()
}

func (dsm *DynamicShardManager) scaleDown() {
	dsm.mu.Lock()
	defer dsm.mu.Unlock()
	
	currentCount := int(atomic.LoadInt32(&dsm.shardCount))
	if currentCount <= InitialShards {
		return
	}
	
	newCount := currentCount / 2
	if newCount < InitialShards {
		newCount = InitialShards
	}
	
	log.Printf("Scaling DOWN shards: %d -> %d", currentCount, newCount)
	
	// Migrate data from shards being removed
	for i := newCount; i < currentCount; i++ {
		shard := dsm.shards[i]
		dsm.migrateShardData(shard, newCount)
		shard.WorkerPool.Stop()
	}
	
	dsm.shards = dsm.shards[:newCount]
	atomic.StoreInt32(&dsm.shardCount, int32(newCount))
}

func (dsm *DynamicShardManager) rebalanceShards() {
	// Simplified rebalancing - in production this would be more sophisticated
	dsm.router.mu.Lock()
	defer dsm.router.mu.Unlock()
	
	for addr, oldShard := range dsm.router.routingTable {
		newShard := dsm.GetOptimalShardForAddress(addr)
		if newShard != oldShard {
			dsm.router.routingTable[addr] = newShard
			// In production, would trigger state migration here
		}
	}
}

func (dsm *DynamicShardManager) migrateShardData(sourceShard *Shard, newShardCount int) {
	// Migrate accounts from source shard to other shards
	sourceShard.mu.Lock()
	defer sourceShard.mu.Unlock()
	
	for addr, account := range sourceShard.StateDB.accounts {
		newShardID := dsm.GetOptimalShardForAddress(common.HexToAddress(addr))
		if newShardID < uint64(newShardCount) {
			targetShard := dsm.shards[newShardID]
			targetShard.mu.Lock()
			targetShard.StateDB.accounts[addr] = account
			targetShard.mu.Unlock()
		}
	}
	
	// Clear source shard data
	sourceShard.StateDB.accounts = make(map[string]*Account)
	sourceShard.MemPool = NewDynamicMemPool()
}

func (dsm *DynamicShardManager) GetOptimalShardForAddress(addr common.Address) uint64 {
	addrInt := new(big.Int).SetBytes(addr.Bytes())
	shardCount := atomic.LoadInt32(&dsm.shardCount)
	return addrInt.Uint64() % uint64(shardCount)
}

func (sr *ShardRouter) GetShard(addr common.Address) uint64 {
	sr.mu.RLock()
	defer sr.mu.RUnlock()
	
	if shard, exists := sr.routingTable[addr]; exists {
		return shard
	}
	
	// Default shard assignment
	addrInt := new(big.Int).SetBytes(addr.Bytes())
	return addrInt.Uint64() % uint64(InitialShards)
}

func (sr *ShardRouter) UpdateRouting(addr common.Address, shardID uint64) {
	sr.mu.Lock()
	defer sr.mu.Unlock()
	sr.routingTable[addr] = shardID
}

func (csp *CrossShardTxPool) AddCrossShardTx(tx *CrossShardTx) {
	csp.mu.Lock()
	defer csp.mu.Unlock()
	key := fmt.Sprintf("%d-%d-%s", tx.SourceShard, tx.DestShard, tx.Tx.Hash)
	tx.CreatedAt = time.Now()
	tx.Status = "pending"
	csp.pending[key] = tx
}

func (dsm *DynamicShardManager) ProcessCrossShardTx(tx *CrossShardTx) error {
	startTime := time.Now()
	
	dsm.mu.RLock()
	if tx.SourceShard >= uint64(len(dsm.shards)) || tx.DestShard >= uint64(len(dsm.shards)) {
		dsm.mu.RUnlock()
		return fmt.Errorf("invalid shard ID")
	}
	
	sourceShard := dsm.shards[tx.SourceShard]
	destShard := dsm.shards[tx.DestShard]
	dsm.mu.RUnlock()
	
	// Lock both shards to prevent deadlocks (always lock in consistent order)
	if tx.SourceShard < tx.DestShard {
		sourceShard.mu.Lock()
		destShard.mu.Lock()
	} else {
		destShard.mu.Lock()
		sourceShard.mu.Lock()
	}
	
	defer sourceShard.mu.Unlock()
	defer destShard.mu.Unlock()
	
	// Check source balance
	sourceBalance := sourceShard.StateDB.GetBalance(tx.Tx.From)
	if sourceBalance.Cmp(tx.Tx.Value) < 0 {
		return fmt.Errorf("insufficient balance in source shard")
	}
	
	// Execute cross-shard transfer
	newSourceBalance := new(big.Int).Sub(sourceBalance, tx.Tx.Value)
	sourceShard.StateDB.SetBalance(tx.Tx.From, newSourceBalance)
	
	destBalance := destShard.StateDB.GetBalance(tx.Tx.To)
	newDestBalance := new(big.Int).Add(destBalance, tx.Tx.Value)
	destShard.StateDB.SetBalance(tx.Tx.To, newDestBalance)
	
	tx.Status = "completed"
	
	// Remove from pending pool
	dsm.crossShardPool.mu.Lock()
	key := fmt.Sprintf("%d-%d-%s", tx.SourceShard, tx.DestShard, tx.Tx.Hash)
	delete(dsm.crossShardPool.pending, key)
	dsm.crossShardPool.mu.Unlock()
	
	// Update metrics
	atomic.AddUint64(&dsm.metrics.CrossShardTxs, 1)
	
	processingTime := time.Since(startTime)
	log.Printf("Cross-shard transaction processed in %v: %s -> %s (value: %s)", 
		processingTime, tx.Tx.From, tx.Tx.To, tx.Tx.Value.String())
	
	return nil
}

func (dsm *DynamicShardManager) GetShardForAddress(addr common.Address) uint64 {
	return dsm.router.GetShard(addr)
}

func (slb *ShardLoadBalancer) UpdateMetrics(shardID uint64, txCount, gasUsed uint64, responseTime time.Duration) {
	slb.mu.Lock()
	defer slb.mu.Unlock()
	
	if _, exists := slb.metrics[shardID]; !exists {
		slb.metrics[shardID] = &ShardLoad{ShardID: shardID}
	}
	
	slb.metrics[shardID].TxCount = txCount
	slb.metrics[shardID].GasUsed = gasUsed
	slb.metrics[shardID].AvgResponseTime = responseTime
	
	// Calculate load score (simplified)
	loadScore := float64(txCount)/1000.0 + float64(gasUsed)/1000000.0 + responseTime.Seconds()
	slb.metrics[shardID].LoadScore = loadScore
}

func (slb *ShardLoadBalancer) GetLeastLoadedShard() uint64 {
	slb.mu.RLock()
	defer slb.mu.RUnlock()
	
	if len(slb.metrics) == 0 {
		return 0
	}
	
	var minLoad float64 = math.MaxFloat64
	var selectedShard uint64 = 0
	
	for shardID, load := range slb.metrics {
		if load.LoadScore < minLoad {
			minLoad = load.LoadScore
			selectedShard = shardID
		}
	}
	
	return selectedShard
}

func (csp *CrossShardTxPool) GetPending() []*CrossShardTx {
	csp.mu.RLock()
	defer csp.mu.RUnlock()
	
	result := make([]*CrossShardTx, 0, len(csp.pending))
	for _, tx := range csp.pending {
		result = append(result, tx)
	}
	return result
}

func (csp *CrossShardTxPool) CleanupStaleTxs(maxAge time.Duration) {
	csp.mu.Lock()
	defer csp.mu.Unlock()
	
	now := time.Now()
	for key, tx := range csp.pending {
		if now.Sub(tx.CreatedAt) > maxAge {
			delete(csp.pending, key)
		}
	}
}

func (dsm *DynamicShardManager) GetMetrics() *ShardMetrics {
	return &ShardMetrics{
		TotalShards:     atomic.LoadInt32(&dsm.metrics.TotalShards),
		ActiveShards:    atomic.LoadInt32(&dsm.metrics.ActiveShards),
		CrossShardTxs:   atomic.LoadUint64(&dsm.metrics.CrossShardTxs),
		ShardScalesUp:   atomic.LoadUint32(&dsm.metrics.ShardScalesUp),
		ShardScalesDown: atomic.LoadUint32(&dsm.metrics.ShardScalesDown),
		AvgShardLoad:    dsm.metrics.AvgShardLoad,
	}
}

// ==================== STATE DATABASE ====================
type StateDB struct {
	accounts map[string]*Account
	storage  map[string]map[string]common.Hash
	code     map[string][]byte
	cache    *StateCache
	mu       sync.RWMutex
}

type Account struct {
	Balance  *big.Int
	Nonce    uint64
	CodeHash []byte
	Root     common.Hash
}

type StateCache struct {
	accounts map[string]*Account
	maxSize  int
	hits     uint64
	misses   uint64
	mu       sync.RWMutex
}

type StateChange struct {
	Address     string
	OldBalance  *big.Int
	NewBalance  *big.Int
	OldNonce    uint64
	NewNonce    uint64
	BlockNumber *big.Int
}

func NewStateDB() *StateDB {
	return &StateDB{
		accounts: make(map[string]*Account),
		storage:  make(map[string]map[string]common.Hash),
		code:     make(map[string][]byte),
		cache:    NewStateCache(StateCacheSize),
	}
}

func NewStateCache(maxSize int) *StateCache {
	return &StateCache{
		accounts: make(map[string]*Account),
		maxSize:  maxSize,
	}
}

func (sdb *StateDB) GetAccount(addr string) *Account {
	sdb.mu.RLock()
	defer sdb.mu.RUnlock()
	
	if acc := sdb.cache.Get(addr); acc != nil {
		return acc
	}
	
	if acc, exists := sdb.accounts[addr]; exists {
		sdb.cache.Set(addr, acc)
		return acc
	}
	
	newAcc := &Account{
		Balance: big.NewInt(0),
		Nonce:   0,
		CodeHash: []byte{},
		Root:     common.Hash{},
	}
	sdb.accounts[addr] = newAcc
	return newAcc
}

func (sdb *StateDB) SetAccount(addr string, acc *Account) {
	sdb.mu.Lock()
	defer sdb.mu.Unlock()
	sdb.accounts[addr] = acc
	sdb.cache.Set(addr, acc)
}

func (sdb *StateDB) GetBalance(addr string) *big.Int {
	acc := sdb.GetAccount(addr)
	return new(big.Int).Set(acc.Balance)
}

func (sdb *StateDB) SetBalance(addr string, balance *big.Int) {
	acc := sdb.GetAccount(addr)
	acc.Balance = new(big.Int).Set(balance)
	sdb.SetAccount(addr, acc)
}

func (sdb *StateDB) GetNonce(addr string) uint64 {
	acc := sdb.GetAccount(addr)
	return acc.Nonce
}

func (sdb *StateDB) SetNonce(addr string, nonce uint64) {
	acc := sdb.GetAccount(addr)
	acc.Nonce = nonce
	sdb.SetAccount(addr, acc)
}

func (sdb *StateDB) GetCode(addr string) []byte {
	sdb.mu.RLock()
	defer sdb.mu.RUnlock()
	return sdb.code[addr]
}

func (sdb *StateDB) SetCode(addr string, code []byte) {
	sdb.mu.Lock()
	defer sdb.mu.Unlock()
	sdb.code[addr] = code
}

func (sdb *StateDB) GetStorage(addr, key string) common.Hash {
	sdb.mu.RLock()
	defer sdb.mu.RUnlock()
	
	if storage, exists := sdb.storage[addr]; exists {
		return storage[key]
	}
	return common.Hash{}
}

func (sdb *StateDB) SetStorage(addr, key string, value common.Hash) {
	sdb.mu.Lock()
	defer sdb.mu.Unlock()
	
	if sdb.storage[addr] == nil {
		sdb.storage[addr] = make(map[string]common.Hash)
	}
	sdb.storage[addr][key] = value
}

func (sdb *StateDB) GetRoot() []byte {
	sdb.mu.RLock()
	defer sdb.mu.RUnlock()
	
	data := []byte{}
	addresses := make([]string, 0, len(sdb.accounts))
	for addr := range sdb.accounts {
		addresses = append(addresses, addr)
	}
	sort.Strings(addresses)
	
	for _, addr := range addresses {
		acc := sdb.accounts[addr]
		data = append(data, []byte(addr)...)
		data = append(data, acc.Balance.Bytes()...)
		nonceBytes := make([]byte, 8)
		binary.BigEndian.PutUint64(nonceBytes, acc.Nonce)
		data = append(data, nonceBytes...)
		data = append(data, acc.CodeHash...)
		data = append(data, acc.Root.Bytes()...)
	}
	
	hash := sha256.Sum256(data)
	return hash[:]
}

func (sdb *StateDB) CreateStateSnapshot() map[string]*Account {
	sdb.mu.RLock()
	defer sdb.mu.RUnlock()
	
	snapshot := make(map[string]*Account)
	for addr, acc := range sdb.accounts {
		snapshot[addr] = &Account{
			Balance:  new(big.Int).Set(acc.Balance),
			Nonce:    acc.Nonce,
			CodeHash: append([]byte{}, acc.CodeHash...),
			Root:     acc.Root,
		}
	}
	return snapshot
}

func (sdb *StateDB) RestoreStateSnapshot(snapshot map[string]*Account) {
	sdb.mu.Lock()
	defer sdb.mu.Unlock()
	
	sdb.accounts = make(map[string]*Account)
	for addr, acc := range snapshot {
		sdb.accounts[addr] = &Account{
			Balance:  new(big.Int).Set(acc.Balance),
			Nonce:    acc.Nonce,
			CodeHash: append([]byte{}, acc.CodeHash...),
			Root:     acc.Root,
		}
	}
	
	// Clear cache
	sdb.cache = NewStateCache(StateCacheSize)
}

func (sc *StateCache) Get(addr string) *Account {
	sc.mu.RLock()
	defer sc.mu.RUnlock()
	
	if acc, exists := sc.accounts[addr]; exists {
		atomic.AddUint64(&sc.hits, 1)
		return acc
	}
	
	atomic.AddUint64(&sc.misses, 1)
	return nil
}

func (sc *StateCache) Set(addr string, acc *Account) {
	sc.mu.Lock()
	defer sc.mu.Unlock()
	
	if len(sc.accounts) >= sc.maxSize {
		// Simple random eviction
		for k := range sc.accounts {
			delete(sc.accounts, k)
			break
		}
	}
	
	sc.accounts[addr] = &Account{
		Balance:  new(big.Int).Set(acc.Balance),
		Nonce:    acc.Nonce,
		CodeHash: append([]byte{}, acc.CodeHash...),
		Root:     acc.Root,
	}
}

func (sc *StateCache) GetMetrics() (hits, misses uint64, size int) {
	sc.mu.RLock()
	defer sc.mu.RUnlock()
	return atomic.LoadUint64(&sc.hits), atomic.LoadUint64(&sc.misses), len(sc.accounts)
}

// ==================== BLOCKCHAIN CORE ====================
type ZKKaiBlockchain struct {
	ChainID         *big.Int
	CurrentBlock    *Block
	Blocks          []*Block
	Validators      []*Validator
	GasAbstraction  *GasAbstractionSystem
	ZKProver        *ZKKSPPrivacyProver
	RollupProcessor *RollupBatchProcessor
	MemPool         *DynamicMemPool
	StateDB         *StateDB
	BlockProducer   *BlockProducer
	Database        *leveldb.DB
	ZKSystem        *ZKProofSystem
	GasSystem       *KaiaGasSystem
	ShardManager    *DynamicShardManager
	WorkerPool      *AdaptiveWorkerPool
	mu              sync.RWMutex
	config          *ChainConfig
}

type KaiaGasSystem struct {
	FreeTxsPerDay     uint64
	SponsoredAccounts map[string]bool
	DailyTxCount      map[string]uint64
	LastTxDate        map[string]int64
	mu                sync.RWMutex
}

type ZKProofSystem struct {
	ProverURL        string
	VerifierKey      []byte
	RecursionEnabled bool
	BatchProving     bool
}

func NewZKKaiBlockchain() (*ZKKaiBlockchain, error) {
	// Create data directory
	if err := os.MkdirAll("./zkkai_data", 0755); err != nil {
		return nil, fmt.Errorf("failed to create data directory: %v", err)
	}
	
	db, err := leveldb.OpenFile("./zkkai_data", &opt.Options{
		WriteBuffer:         DBWriteBuffer,
		BlockCacheCapacity:  DBCacheSize,
		OpenFilesCacheCapacity: DBMaxOpenFiles,
		CompactionTableSize: 32 * 1024 * 1024,
	})
	if err != nil {
		return nil, fmt.Errorf("failed to open database: %v", err)
	}
	
	zkProver, err := NewZKKSPPrivacyProver()
	if err != nil {
		db.Close()
		return nil, fmt.Errorf("failed to initialize ZK prover: %v", err)
	}
	
	bc := &ZKKaiBlockchain{
		ChainID:         big.NewInt(ZKKaiChainID),
		Blocks:          make([]*Block, 0),
		Validators:      make([]*Validator, 0),
		GasAbstraction:  NewGasAbstractionSystem(),
		ZKProver:        zkProver,
		RollupProcessor: NewRollupBatchProcessor(zkProver),
		MemPool:         NewDynamicMemPool(),
		StateDB:         NewStateDB(),
		Database:        db,
		ShardManager:    NewDynamicShardManager(),
		WorkerPool:      NewAdaptiveWorkerPool(WorkerPoolSize, WorkerPoolSize*2),
		ZKSystem: &ZKProofSystem{
			ProverURL:        "http://localhost:8080",
			RecursionEnabled: true,
			BatchProving:     true,
		},
		GasSystem: &KaiaGasSystem{
			FreeTxsPerDay:     10,
			SponsoredAccounts: make(map[string]bool),
			DailyTxCount:      make(map[string]uint64),
			LastTxDate:        make(map[string]int64),
		},
		config: GetChainConfig(),
	}
	
	// Load or create genesis block
	genesisBlock, err := bc.loadGenesisBlock()
	if err != nil {
		bc.Database.Close()
		return nil, fmt.Errorf("failed to load genesis block: %v", err)
	}
	
	bc.Blocks = append(bc.Blocks, genesisBlock)
	bc.CurrentBlock = genesisBlock
	
	// Load state
	if err := bc.LoadState(); err != nil {
		log.Printf("Warning: Failed to load state: %v", err)
	}
	
	bc.BlockProducer = NewBlockProducer(bc)
	
	return bc, nil
}

func (bc *ZKKaiBlockchain) loadGenesisBlock() (*Block, error) {
	// Try to load from database
	key := []byte("block:genesis")
	data, err := bc.Database.Get(key, nil)
	if err == nil {
		var block Block
		if err := json.Unmarshal(data, &block); err != nil {
			return nil, err
		}
		return &block, nil
	}
	
	// Create new genesis block
	genesisBlock := bc.createGenesisBlock()
	
	// Save to database
	blockData, err := json.Marshal(genesisBlock)
	if err != nil {
		return nil, err
	}
	
	if err := bc.Database.Put(key, blockData, nil); err != nil {
		return nil, err
	}
	
	return genesisBlock, nil
}

func (bc *ZKKaiBlockchain) createGenesisBlock() *Block {
	// Pre-fund some accounts for testing
	prefundedAccounts := map[string]*big.Int{
		"0x0000000000000000000000000000000000000001": big.NewInt(1000000000000000000), // 1 ZKK
		"0x0000000000000000000000000000000000000002": big.NewInt(1000000000000000000), // 1 ZKK
		"0x0000000000000000000000000000000000000003": big.NewInt(1000000000000000000), // 1 ZKK
	}
	
	// Initialize state with prefunded accounts
	for addr, balance := range prefundedAccounts {
		bc.StateDB.SetBalance(addr, balance)
	}
	
	stateRoot := bc.StateDB.GetRoot()
	
	return &Block{
		Number:       big.NewInt(0),
		Hash:         "0x0000000000000000000000000000000000000000000000000000000000000000",
		ParentHash:   "0x0000000000000000000000000000000000000000000000000000000000000000",
		Timestamp:    uint64(time.Now().Unix()),
		GasLimit:     BlockGasLimit,
		GasUsed:      0,
		BaseFee:      big.NewInt(BaseGasPrice),
		StateRoot:    hex.EncodeToString(stateRoot),
		Transactions: make([]*Transaction, 0),
		ShieldedTxs:  make([]*ShieldedTransaction, 0),
		ZKProofs:     make([]*PrivateTransactionProof, 0),
		Miner:        "0x0000000000000000000000000000000000000000",
		Nonce:        0,
		Difficulty:   big.NewInt(1),
		TotalDifficulty: big.NewInt(1),
		ExtraData:    []byte("zkKaia Genesis Block"),
		LogsBloom:    "0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
		ReceiptsRoot: "0x0000000000000000000000000000000000000000000000000000000000000000",
		TransactionsRoot: "0x0000000000000000000000000000000000000000000000000000000000000000",
		Uncles:       make([]string, 0),
		Size:         1000,
	}
}

func (bc *ZKKaiBlockchain) AddTransaction(tx *Transaction) error {
	// Validate transaction
	if tx.From == "" {
		return fmt.Errorf("missing sender address")
	}
	if tx.Value == nil {
		return fmt.Errorf("missing value")
	}
	if tx.GasPrice == nil {
		return fmt.Errorf("missing gas price")
	}
	if tx.Gas == 0 {
		return fmt.Errorf("missing gas limit")
	}
	
	// Check nonce
	currentNonce := bc.StateDB.GetNonce(tx.From)
	if tx.Nonce < currentNonce {
		return fmt.Errorf("nonce too low")
	}
	
	// Check balance
	balance := bc.StateDB.GetBalance(tx.From)
	totalCost := new(big.Int).Add(tx.Value, new(big.Int).Mul(big.NewInt(int64(tx.Gas)), tx.GasPrice))
	if balance.Cmp(totalCost) < 0 {
		return fmt.Errorf("insufficient balance")
	}
	
	// Route to appropriate shard
	shardID := bc.ShardManager.GetShardForAddress(common.HexToAddress(tx.From))
	shard := bc.ShardManager.shards[shardID]
	
	return shard.MemPool.AddTransaction(tx)
}

func (bc *ZKKaiBlockchain) ProcessShieldedTransaction(stx *ShieldedTransaction) error {
	// Verify the ZK proof
	valid, err := bc.ZKProver.VerifyShieldedTransaction(stx)
	if err != nil {
		return err
	}
	if !valid {
		return fmt.Errorf("invalid shielded transaction proof")
	}
	
	bc.mu.Lock()
	defer bc.mu.Unlock()
	
	// Add to current block
	bc.CurrentBlock.ShieldedTxs = append(bc.CurrentBlock.ShieldedTxs, stx)
	
	return nil
}

func (bc *ZKKaiBlockchain) SaveBlock(block *Block) error {
	bc.mu.Lock()
	defer bc.mu.Unlock()
	
	blockData, err := json.Marshal(block)
	if err != nil {
		return err
	}
	
	key := []byte(fmt.Sprintf("block:%s", block.Hash))
	return bc.Database.Put(key, blockData, nil)
}

func (bc *ZKKaiBlockchain) LoadBlock(hash string) (*Block, error) {
	key := []byte(fmt.Sprintf("block:%s", hash))
	data, err := bc.Database.Get(key, nil)
	if err != nil {
		return nil, err
	}
	
	var block Block
	if err := json.Unmarshal(data, &block); err != nil {
		return nil, err
	}
	
	return &block, nil
}

func (bc *ZKKaiBlockchain) SaveState() error {
	bc.mu.Lock()
	defer bc.mu.Unlock()
	
	stateData, err := json.Marshal(bc.StateDB.accounts)
	if err != nil {
		return err
	}
	
	return bc.Database.Put([]byte("state:latest"), stateData, nil)
}

func (bc *ZKKaiBlockchain) LoadState() error {
	data, err := bc.Database.Get([]byte("state:latest"), nil)
	if err != nil {
		if err == leveldb.ErrNotFound {
			return nil
		}
		return err
	}
	
	var accounts map[string]*Account
	if err := json.Unmarshal(data, &accounts); err != nil {
		return err
	}
	
	bc.StateDB.accounts = accounts
	return nil
}

func (bc *ZKKaiBlockchain) GetBlockByNumber(number uint64) *Block {
	bc.mu.RLock()
	defer bc.mu.RUnlock()
	
	if number < uint64(len(bc.Blocks)) {
		return bc.Blocks[number]
	}
	return nil
}

func (bc *ZKKaiBlockchain) GetBlockByHash(hash string) *Block {
	bc.mu.RLock()
	defer bc.mu.RUnlock()
	
	for _, block := range bc.Blocks {
		if block.Hash == hash {
			return block
		}
	}
	return nil
}

func (bc *ZKKaiBlockchain) GetTransaction(hash string) (*Transaction, *Block, error) {
	bc.mu.RLock()
	defer bc.mu.RUnlock()
	
	for _, block := range bc.Blocks {
		for _, tx := range block.Transactions {
			if tx.Hash == hash {
				return tx, block, nil
			}
		}
	}
	return nil, nil, fmt.Errorf("transaction not found")
}

// ==================== BLOCK PRODUCER ====================
type BlockProducer struct {
	blockchain  *ZKKaiBlockchain
	miningPool  *MiningPool
	stateDB     *StateDB
	running     bool
	stopChan    chan struct{}
	workerPool  *AdaptiveWorkerPool
}

type MiningPool struct {
	Miners    []*Miner
	Config    *MiningConfig
	IsMining  bool
	mu        sync.RWMutex
}

type Miner struct {
	Address      common.Address `json:"address"`
	Stake        *big.Int       `json:"stake"`
	Active       bool           `json:"active"`
	BlocksMined  uint64         `json:"blocksMined"`
	TotalRewards *big.Int       `json:"totalRewards"`
}

type MiningConfig struct {
	Difficulty    *big.Int
	BlockTime     time.Duration
	MinerReward   *big.Int
	MinStake      *big.Int
	ValidatorSetSize int
}

func NewBlockProducer(bc *ZKKaiBlockchain) *BlockProducer {
	return &BlockProducer{
		blockchain: bc,
		miningPool: &MiningPool{
			Miners:   make([]*Miner, 0),
			IsMining: false,
			Config: &MiningConfig{
				Difficulty:    big.NewInt(1000000),
				BlockTime:     BlockTime,
				MinerReward:   big.NewInt(1000000000000000000), // 1 ZKK
				MinStake:      big.NewInt(32000000000000000000), // 32 ZKK
				ValidatorSetSize: ValidatorSetSize,
			},
		},
		stateDB:    bc.StateDB,
		running:    false,
		stopChan:   make(chan struct{}),
		workerPool: NewAdaptiveWorkerPool(ValidationWorkers, ValidationWorkers*2),
	}
}

func (bp *BlockProducer) Start() {
	bp.running = true
	bp.miningPool.IsMining = true
	go bp.produceBlocks()
	log.Println("Block producer started")
}

func (bp *BlockProducer) Stop() {
	bp.running = false
	bp.miningPool.IsMining = false
	close(bp.stopChan)
	bp.workerPool.Stop()
	log.Println("Block producer stopped")
}

func (bp *BlockProducer) produceBlocks() {
	ticker := time.NewTicker(BlockTime)
	defer ticker.Stop()
	
	for {
		select {
		case <-ticker.C:
			if bp.running && bp.miningPool.IsMining {
				bp.workerPool.Submit(func() {
					bp.createBlock()
				})
			}
		case <-bp.stopChan:
			return
		}
	}
}

func (bp *BlockProducer) createBlock() {
	bp.blockchain.mu.Lock()
	defer bp.blockchain.mu.Unlock()
	
	// Get pending transactions from all shards
	var allPendingTxs []*Transaction
	shardCount := atomic.LoadInt32(&bp.blockchain.ShardManager.shardCount)
	
	for i := 0; i < int(shardCount); i++ {
		shard := bp.blockchain.ShardManager.shards[i]
		shardTxs := shard.MemPool.GetPriorityPending(MaxBatchSize / int(shardCount))
		allPendingTxs = append(allPendingTxs, shardTxs...)
	}
	
	if len(allPendingTxs) == 0 {
		return
	}
	
	startTime := time.Now()
	
	// Create new block
	newBlock := &Block{
		Number:       new(big.Int).Add(bp.blockchain.CurrentBlock.Number, big.NewInt(1)),
		ParentHash:   bp.blockchain.CurrentBlock.Hash,
		Timestamp:    uint64(time.Now().Unix()),
		GasLimit:     BlockGasLimit,
		GasUsed:      0,
		BaseFee:      big.NewInt(BaseGasPrice),
		Transactions: allPendingTxs,
		ShieldedTxs:  bp.blockchain.CurrentBlock.ShieldedTxs,
		ZKProofs:     make([]*PrivateTransactionProof, 0),
		Nonce:        uint64(time.Now().UnixNano()),
		Difficulty:   bp.miningPool.Config.Difficulty,
		TotalDifficulty: new(big.Int).Add(bp.blockchain.CurrentBlock.TotalDifficulty, bp.miningPool.Config.Difficulty),
		ExtraData:    []byte("zkKaia Block"),
		LogsBloom:    "0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
		ReceiptsRoot: "0x0000000000000000000000000000000000000000000000000000000000000000",
		TransactionsRoot: "0x0000000000000000000000000000000000000000000000000000000000000000",
		Uncles:       make([]string, 0),
	}
	
	// Process transactions
	var validTxs []*Transaction
	totalGasUsed := uint64(0)
	
	for _, tx := range allPendingTxs {
		if totalGasUsed+tx.Gas > BlockGasLimit {
			break
		}
		
		fromBalance := bp.stateDB.GetBalance(tx.From)
		toBalance := bp.stateDB.GetBalance(tx.To)
		
		totalCost := new(big.Int).Add(tx.Value, new(big.Int).Mul(big.NewInt(int64(tx.Gas)), tx.GasPrice))
		
		if fromBalance.Cmp(totalCost) >= 0 {
			// Execute transaction
			newFromBalance := new(big.Int).Sub(fromBalance, totalCost)
			newToBalance := new(big.Int).Add(toBalance, tx.Value)
			
			bp.stateDB.SetBalance(tx.From, newFromBalance)
			bp.stateDB.SetBalance(tx.To, newToBalance)
			bp.stateDB.SetNonce(tx.From, bp.stateDB.GetNonce(tx.From)+1)
			
			tx.Status = 1
			tx.BlockHash = newBlock.Hash
			tx.BlockNumber = newBlock.Number
			tx.TransactionIndex = uint64(len(validTxs))
			
			validTxs = append(validTxs, tx)
			totalGasUsed += tx.Gas
			newBlock.GasUsed += tx.Gas
		} else {
			tx.Status = 0
		}
	}
	
	newBlock.Transactions = validTxs
	
	// Calculate block hash
	blockData := fmt.Sprintf("%s%d%d%s", newBlock.ParentHash, newBlock.Timestamp, newBlock.Nonce, hex.EncodeToString(bp.stateDB.GetRoot()))
	newBlock.Hash = fmt.Sprintf("0x%x", sha256.Sum256([]byte(blockData)))
	
	// Update state root
	stateRoot := bp.stateDB.GetRoot()
	newBlock.StateRoot = hex.EncodeToString(stateRoot)
	
	// Calculate receipts root (simplified)
	receiptsData := []byte{}
	for _, tx := range validTxs {
		receiptsData = append(receiptsData, []byte(tx.Hash)...)
	}
	newBlock.ReceiptsRoot = fmt.Sprintf("0x%x", sha256.Sum256(receiptsData))
	
	// Calculate transactions root (simplified)
	txsData := []byte{}
	for _, tx := range validTxs {
		txsData = append(txsData, []byte(tx.Hash)...)
	}
	newBlock.TransactionsRoot = fmt.Sprintf("0x%x", sha256.Sum256(txsData))
	
	// Estimate block size
	blockJSON, _ := json.Marshal(newBlock)
	newBlock.Size = uint64(len(blockJSON))
	
	// Add to blockchain
	bp.blockchain.Blocks = append(bp.blockchain.Blocks, newBlock)
	bp.blockchain.CurrentBlock = newBlock
	
	// Save to storage
	if err := bp.blockchain.SaveBlock(newBlock); err != nil {
		log.Printf("Failed to save block: %v", err)
	}
	
	if err := bp.blockchain.SaveState(); err != nil {
		log.Printf("Failed to save state: %v", err)
	}
	
	// Remove processed transactions from mempools
	for _, tx := range validTxs {
		shardID := bp.blockchain.ShardManager.GetShardForAddress(common.HexToAddress(tx.From))
		shard := bp.blockchain.ShardManager.shards[shardID]
		shard.MemPool.RemoveTransaction(tx.Hash)
	}
	
	processingTime := time.Since(startTime)
	
	// Update shard metrics
	for i := 0; i < int(shardCount); i++ {
		shard := bp.blockchain.ShardManager.shards[i]
		shard.mu.Lock()
		shard.TxCount += uint64(len(validTxs))
		shard.GasUsed += newBlock.GasUsed
		shard.Load = float64(shard.MemPool.currentSize) / float64(shard.MemPool.maxSize)
		shard.LastBlockTime = time.Now()
		shard.mu.Unlock()
		
		bp.blockchain.ShardManager.loadBalancer.UpdateMetrics(
			shard.ID,
			shard.TxCount,
			shard.GasUsed,
			processingTime,
		)
	}
	
	log.Printf("Block #%d created with %d transactions (gas used: %d, processing time: %v)", 
		newBlock.Number.Uint64(), len(validTxs), newBlock.GasUsed, processingTime)
}

// ==================== WALLET MANAGEMENT ====================
type WalletManager struct {
	KeyStore   *keystore.KeyStore
	Wallets    map[string]*Wallet
	HDWallets  map[string]*HDWallet
	blockchain *ZKKaiBlockchain
	mu         sync.RWMutex
}

type Wallet struct {
	Address    common.Address    `json:"address"`
	PublicKey  string            `json:"publicKey"`
	PrivateKey *ecdsa.PrivateKey `json:"-"`
	Balance    *big.Int          `json:"balance"`
	Nonce      uint64            `json:"nonce"`
}

type HDWallet struct {
	Mnemonic   string    `json:"mnemonic"`
	Seed       []byte    `json:"-"`
	Accounts   []*Wallet `json:"accounts"`
	Path       string    `json:"path"`
}

func NewWalletManager(bc *ZKKaiBlockchain) *WalletManager {
	// Create keystore directory
	os.MkdirAll("./keystore", 0700)
	
	return &WalletManager{
		KeyStore:   keystore.NewKeyStore("./keystore", keystore.StandardScryptN, keystore.StandardScryptP),
		Wallets:    make(map[string]*Wallet),
		HDWallets:  make(map[string]*HDWallet),
		blockchain: bc,
	}
}

func (wm *WalletManager) CreateWallet() (*Wallet, error) {
	privateKey, err := crypto.GenerateKey()
	if err != nil {
		return nil, fmt.Errorf("failed to generate private key: %v", err)
	}
	
	publicKey := privateKey.Public().(*ecdsa.PublicKey)
	address := crypto.PubkeyToAddress(*publicKey)
	
	wallet := &Wallet{
		Address:    address,
		PublicKey:  hex.EncodeToString(crypto.FromECDSAPub(publicKey)),
		PrivateKey: privateKey,
		Balance:    big.NewInt(0),
		Nonce:      0,
	}
	
	wm.mu.Lock()
	wm.Wallets[address.Hex()] = wallet
	wm.mu.Unlock()
	
	// Update balance and nonce from blockchain
	if err := wm.UpdateWalletState(address.Hex()); err != nil {
		log.Printf("Warning: Failed to update wallet state: %v", err)
	}
	
	return wallet, nil
}

func (wm *WalletManager) GetWallet(address string) (*Wallet, error) {
	wm.mu.RLock()
	defer wm.mu.RUnlock()
	
	wallet, exists := wm.Wallets[address]
	if !exists {
		return nil, fmt.Errorf("wallet not found: %s", address)
	}
	
	return wallet, nil
}

func (wm *WalletManager) UpdateWalletState(address string) error {
	wallet, err := wm.GetWallet(address)
	if err != nil {
		return err
	}
	
	wallet.Balance = wm.blockchain.StateDB.GetBalance(address)
	wallet.Nonce = wm.blockchain.StateDB.GetNonce(address)
	
	return nil
}

func (wm *WalletManager) ImportWallet(privateKeyHex string) (*Wallet, error) {
	privateKey, err := crypto.HexToECDSA(privateKeyHex)
	if err != nil {
		return nil, fmt.Errorf("invalid private key: %v", err)
	}
	
	publicKey := privateKey.Public().(*ecdsa.PublicKey)
	address := crypto.PubkeyToAddress(*publicKey)
	
	wallet := &Wallet{
		Address:    address,
		PublicKey:  hex.EncodeToString(crypto.FromECDSAPub(publicKey)),
		PrivateKey: privateKey,
		Balance:    big.NewInt(0),
		Nonce:      0,
	}
	
	wm.mu.Lock()
	wm.Wallets[address.Hex()] = wallet
	wm.mu.Unlock()
	
	if err := wm.UpdateWalletState(address.Hex()); err != nil {
		log.Printf("Warning: Failed to update wallet state: %v", err)
	}
	
	return wallet, nil
}

func (wm *WalletManager) SignTransaction(tx *Transaction, wallet *Wallet) error {
	if wallet.PrivateKey == nil {
		return fmt.Errorf("wallet private key not available")
	}
	
	// Create Ethereum transaction
	ethTx := types.NewTransaction(
		tx.Nonce,
		common.HexToAddress(tx.To),
		tx.Value,
		tx.Gas,
		tx.GasPrice,
		tx.Input,
	)
	
	// Sign transaction
	signer := types.NewEIP155Signer(wm.blockchain.ChainID)
	signedTx, err := types.SignTx(ethTx, signer, wallet.PrivateKey)
	if err != nil {
		return fmt.Errorf("failed to sign transaction: %v", err)
	}
	
	// Update transaction with signature
	if v, r, s := signedTx.RawSignatureValues(); v != nil && r != nil && s != nil {
		tx.V = v
		tx.R = r
		tx.S = s
		tx.Type = uint8(signedTx.Type())
		tx.ChainID = wm.blockchain.ChainID
	}
	
	// Set transaction hash
	tx.Hash = signedTx.Hash().Hex()
	
	return nil
}

func (wm *WalletManager) ExportWallet(address string) (string, error) {
	wallet, err := wm.GetWallet(address)
	if err != nil {
		return "", err
	}
	
	if wallet.PrivateKey == nil {
		return "", fmt.Errorf("private key not available for export")
	}
	
	return hex.EncodeToString(crypto.FromECDSA(wallet.PrivateKey)), nil
}

// ==================== CHAIN CONFIGURATION ====================
type ChainConfig struct {
	ChainID           *big.Int `json:"chainId"`
	NetworkID         *big.Int `json:"networkId"`
	Name              string   `json:"name"`
	ShortName         string   `json:"shortName"`
	Consensus         string   `json:"consensus"`
	BlockPeriod       uint64   `json:"blockPeriod"`
	BlockGasLimit     uint64   `json:"blockGasLimit"`
	EpochLength       uint64   `json:"epochLength"`
	ValidatorMinStake *big.Int `json:"validatorMinStake"`
	ZKProofTimeWindow uint64   `json:"zkProofTimeWindow"`
	MaxBlockSize      uint64   `json:"maxBlockSize"`
	ValidityProofs    bool     `json:"validityProofs"`
	Recursion         bool     `json:"recursion"`
	GasPrice          *big.Int `json:"gasPrice"`
	GasAbstraction    bool     `json:"gasAbstraction"`
	GasSponsorship    bool     `json:"gasSponsorship"`
	FreeTxsPerDay     uint64   `json:"freeTxsPerDay"`
	NativeCurrency    struct {
		Name     string `json:"name"`
		Symbol   string `json:"symbol"`
		Decimals uint8  `json:"decimals"`
	} `json:"nativeCurrency"`
	RPC       []string `json:"rpc"`
	Faucets   []string `json:"faucets"`
	Explorers []struct {
		Name     string `json:"name"`
		URL      string `json:"url"`
		Standard string `json:"standard"`
	} `json:"explorers"`
}

func GetChainConfig() *ChainConfig {
	config := &ChainConfig{
		ChainID:           big.NewInt(ZKKaiChainID),
		NetworkID:         big.NewInt(ZKKaiChainID),
		Name:              "zkKaia Chain",
		ShortName:         "zkkai",
		Consensus:         "Istanbul BFT + ZK Rollup",
		BlockPeriod:       12,
		BlockGasLimit:     BlockGasLimit,
		EpochLength:       86400,
		ValidatorMinStake: big.NewInt(32000000000000000000), // 32 ZKK
		ZKProofTimeWindow: 300,
		MaxBlockSize:      2097152,
		ValidityProofs:    true,
		Recursion:         true,
		GasPrice:          big.NewInt(BaseGasPrice),
		GasAbstraction:    true,
		GasSponsorship:    true,
		FreeTxsPerDay:     10,
		RPC: []string{
			"http://localhost:8545",
			"ws://localhost:8546",
		},
		Faucets: []string{
			"http://localhost:8080/faucet",
		},
	}
	
	config.NativeCurrency.Name = "zkKaia"
	config.NativeCurrency.Symbol = "ZKK"
	config.NativeCurrency.Decimals = 18
	
	config.Explorers = []struct {
		Name     string `json:"name"`
		URL      string `json:"url"`
		Standard string `json:"standard"`
	}{
		{Name: "zkKaia Explorer", URL: "http://localhost:4000", Standard: "EIP3091"},
	}
	
	return config
}

// ==================== RPC SERVER ====================
type RPCServer struct {
	blockchain *ZKKaiBlockchain
	router     *mux.Router
	upgrader   websocket.Upgrader
	clients    map[*websocket.Conn]bool
	mu         sync.RWMutex
}

func NewRPCServer(bc *ZKKaiBlockchain) *RPCServer {
	return &RPCServer{
		blockchain: bc,
		router:     mux.NewRouter(),
		upgrader: websocket.Upgrader{
			ReadBufferSize:  ReadBufferSize,
			WriteBufferSize: WriteBufferSize,
			CheckOrigin:     func(r *http.Request) bool { return true },
		},
		clients: make(map[*websocket.Conn]bool),
	}
}

func (rpc *RPCServer) Start(port string) error {
	rpc.setupRoutes()
	
	server := &http.Server{
		Addr:         ":" + port,
		Handler:      rpc.router,
		ReadTimeout:  30 * time.Second,
		WriteTimeout: 30 * time.Second,
		IdleTimeout:  120 * time.Second,
	}
	
	log.Printf("RPC Server starting on port %s", port)
	return server.ListenAndServe()
}

func (rpc *RPCServer) setupRoutes() {
	rpc.router.Use(corsMiddleware)
	rpc.router.Use(loggingMiddleware)
	
	// Health and info endpoints
	rpc.router.HandleFunc("/", rpc.handleRoot).Methods("GET")
	rpc.router.HandleFunc("/health", rpc.handleHealth).Methods("GET")
	rpc.router.HandleFunc("/config", rpc.handleGetConfig).Methods("GET")
	
	// Ethereum-compatible JSON-RPC endpoints
	rpc.router.HandleFunc("/eth_blockNumber", rpc.handleBlockNumber).Methods("POST")
	rpc.router.HandleFunc("/eth_getBlockByNumber", rpc.handleGetBlockByNumber).Methods("POST")
	rpc.router.HandleFunc("/eth_getBlockByHash", rpc.handleGetBlockByHash).Methods("POST")
	rpc.router.HandleFunc("/eth_getBalance", rpc.handleGetBalance).Methods("POST")
	rpc.router.HandleFunc("/eth_getTransactionCount", rpc.handleGetTransactionCount).Methods("POST")
	rpc.router.HandleFunc("/eth_sendRawTransaction", rpc.handleSendRawTransaction).Methods("POST")
	rpc.router.HandleFunc("/eth_getTransactionReceipt", rpc.handleGetTransactionReceipt).Methods("POST")
	rpc.router.HandleFunc("/eth_call", rpc.handleCall).Methods("POST")
	rpc.router.HandleFunc("/eth_estimateGas", rpc.handleEstimateGas).Methods("POST")
	rpc.router.HandleFunc("/eth_gasPrice", rpc.handleGasPrice).Methods("POST")
	rpc.router.HandleFunc("/eth_chainId", rpc.handleChainId).Methods("POST")
	rpc.router.HandleFunc("/eth_getLogs", rpc.handleGetLogs).Methods("POST")
	
	// zkKaia-specific endpoints
	rpc.router.HandleFunc("/zk_sendShieldedTransaction", rpc.handleSendShieldedTransaction).Methods("POST")
	rpc.router.HandleFunc("/zk_getProofStatus", rpc.handleGetProofStatus).Methods("POST")
	rpc.router.HandleFunc("/zk_getNullifiers", rpc.handleGetNullifiers).Methods("POST")
	rpc.router.HandleFunc("/zk_verifyProof", rpc.handleVerifyProof).Methods("POST")
	
	// Kaia-specific endpoints
	rpc.router.HandleFunc("/kai_getShardInfo", rpc.handleGetShardInfo).Methods("POST")
	rpc.router.HandleFunc("/kai_getMetrics", rpc.handleGetMetrics).Methods("POST")
	rpc.router.HandleFunc("/kai_createWallet", rpc.handleCreateWallet).Methods("POST")
	rpc.router.HandleFunc("/kai_getWallet", rpc.handleGetWallet).Methods("POST")
	
	// WebSocket endpoint
	rpc.router.HandleFunc("/ws", rpc.handleWebSocket)
}

func corsMiddleware(next http.Handler) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.Header().Set("Access-Control-Allow-Origin", "*")
		w.Header().Set("Access-Control-Allow-Methods", "GET, POST, OPTIONS")
		w.Header().Set("Access-Control-Allow-Headers", "Content-Type, Authorization")
		
		if r.Method == "OPTIONS" {
			w.WriteHeader(http.StatusOK)
			return
		}
		
		next.ServeHTTP(w, r)
	})
}

func loggingMiddleware(next http.Handler) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		start := time.Now()
		next.ServeHTTP(w, r)
		log.Printf("%s %s %v", r.Method, r.URL.Path, time.Since(start))
	})
}

// RPC handler implementations (abbreviated for brevity)
func (rpc *RPCServer) handleRoot(w http.ResponseWriter, r *http.Request) {
	json.NewEncoder(w).Encode(map[string]string{
		"name":    "zkKaia RPC Server",
		"version": "1.0.0",
		"status":  "running",
	})
}

func (rpc *RPCServer) handleHealth(w http.ResponseWriter, r *http.Request) {
	w.WriteHeader(http.StatusOK)
	json.NewEncoder(w).Encode(map[string]interface{}{
		"status":      "healthy",
		"blockNumber": rpc.blockchain.CurrentBlock.Number.String(),
		"chainId":     rpc.blockchain.ChainID.String(),
		"peers":       0, // Would be actual peer count in production
	})
}

func (rpc *RPCServer) handleGetConfig(w http.ResponseWriter, r *http.Request) {
	config := GetChainConfig()
	json.NewEncoder(w).Encode(map[string]interface{}{
		"config": config,
	})
}

func (rpc *RPCServer) handleBlockNumber(w http.ResponseWriter, r *http.Request) {
	rpc.blockchain.mu.RLock()
	blockNumber := rpc.blockchain.CurrentBlock.Number
	rpc.blockchain.mu.RUnlock()
	
	json.NewEncoder(w).Encode(map[string]string{
		"result": fmt.Sprintf("0x%x", blockNumber.Uint64()),
	})
}

func (rpc *RPCServer) handleGetBlockByNumber(w http.ResponseWriter, r *http.Request) {
	var req struct {
		Number string `json:"number"`
		Full   bool   `json:"full"`
	}
	
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}
	
	var blockNum uint64
	if req.Number == "latest" {
		rpc.blockchain.mu.RLock()
		blockNum = rpc.blockchain.CurrentBlock.Number.Uint64()
		rpc.blockchain.mu.RUnlock()
	} else {
		var err error
		blockNum, err = strconv.ParseUint(strings.TrimPrefix(req.Number, "0x"), 16, 64)
		if err != nil {
			json.NewEncoder(w).Encode(map[string]interface{}{
				"error": "invalid block number",
			})
			return
		}
	}
	
	block := rpc.blockchain.GetBlockByNumber(blockNum)
	if block == nil {
		json.NewEncoder(w).Encode(map[string]interface{}{
			"error": "block not found",
		})
		return
	}
	
	json.NewEncoder(w).Encode(map[string]interface{}{
		"result": block,
	})
}

func (rpc *RPCServer) handleGetBlockByHash(w http.ResponseWriter, r *http.Request) {
	var req struct {
		Hash string `json:"hash"`
		Full bool   `json:"full"`
	}
	
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}
	
	block := rpc.blockchain.GetBlockByHash(req.Hash)
	if block == nil {
		json.NewEncoder(w).Encode(map[string]interface{}{
			"error": "block not found",
		})
		return
	}
	
	json.NewEncoder(w).Encode(map[string]interface{}{
		"result": block,
	})
}

func (rpc *RPCServer) handleGetBalance(w http.ResponseWriter, r *http.Request) {
	var req struct {
		Address string `json:"address"`
		Block   string `json:"block"`
	}
	
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}
	
	balance := rpc.blockchain.StateDB.GetBalance(req.Address)
	
	json.NewEncoder(w).Encode(map[string]string{
		"result": fmt.Sprintf("0x%x", balance),
	})
}

func (rpc *RPCServer) handleGetTransactionCount(w http.ResponseWriter, r *http.Request) {
	var req struct {
		Address string `json:"address"`
		Block   string `json:"block"`
	}
	
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}
	
	nonce := rpc.blockchain.StateDB.GetNonce(req.Address)
	
	json.NewEncoder(w).Encode(map[string]string{
		"result": fmt.Sprintf("0x%x", nonce),
	})
}

func (rpc *RPCServer) handleSendRawTransaction(w http.ResponseWriter, r *http.Request) {
	var req struct {
		Data string `json:"data"`
	}
	
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}
	
	txData, err := hex.DecodeString(strings.TrimPrefix(req.Data, "0x"))
	if err != nil {
		json.NewEncoder(w).Encode(map[string]interface{}{
			"error": "invalid hex data",
		})
		return
	}
	
	ethTx := new(types.Transaction)
	if err := ethTx.UnmarshalBinary(txData); err != nil {
		json.NewEncoder(w).Encode(map[string]interface{}{
			"error": "invalid transaction data",
		})
		return
	}
	
	signer := types.NewEIP155Signer(rpc.blockchain.ChainID)
	sender, err := types.Sender(signer, ethTx)
	if err != nil {
		json.NewEncoder(w).Encode(map[string]interface{}{
			"error": "invalid signature",
		})
		return
	}
	
	tx := &Transaction{
		Hash:     ethTx.Hash().Hex(),
		From:     sender.Hex(),
		To:       ethTx.To().Hex(),
		Value:    ethTx.Value(),
		Gas:      ethTx.Gas(),
		GasPrice: ethTx.GasPrice(),
		Input:    ethTx.Data(),
		Nonce:    ethTx.Nonce(),
		Status:   0,
	}
	
	if err := rpc.blockchain.AddTransaction(tx); err != nil {
		json.NewEncoder(w).Encode(map[string]interface{}{
			"error": err.Error(),
		})
		return
	}
	
	json.NewEncoder(w).Encode(map[string]string{
		"result": tx.Hash,
	})
	
	rpc.BroadcastNewTransaction(tx)
}

func (rpc *RPCServer) handleGetTransactionReceipt(w http.ResponseWriter, r *http.Request) {
	var req struct {
		Hash string `json:"hash"`
	}
	
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}
	
	tx, block, err := rpc.blockchain.GetTransaction(req.Hash)
	if err != nil {
		json.NewEncoder(w).Encode(map[string]interface{}{
			"result": nil,
		})
		return
	}
	
	receipt := map[string]interface{}{
		"transactionHash":   tx.Hash,
		"transactionIndex":  fmt.Sprintf("0x%x", tx.TransactionIndex),
		"blockNumber":       fmt.Sprintf("0x%x", block.Number.Uint64()),
		"blockHash":         block.Hash,
		"from":              tx.From,
		"to":                tx.To,
		"cumulativeGasUsed": fmt.Sprintf("0x%x", tx.Gas),
		"gasUsed":           fmt.Sprintf("0x%x", tx.Gas),
		"contractAddress":   nil,
		"logs":              []interface{}{},
		"logsBloom":         "0x00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
		"status":            fmt.Sprintf("0x%x", tx.Status),
	}
	
	json.NewEncoder(w).Encode(map[string]interface{}{
		"result": receipt,
	})
}

func (rpc *RPCServer) handleCall(w http.ResponseWriter, r *http.Request) {
	var req struct {
		To   string `json:"to"`
		Data string `json:"data"`
	}
	
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}
	
	// Simplified call implementation
	json.NewEncoder(w).Encode(map[string]string{
		"result": "0x",
	})
}

func (rpc *RPCServer) handleEstimateGas(w http.ResponseWriter, r *http.Request) {
	// Simplified gas estimation
	json.NewEncoder(w).Encode(map[string]string{
		"result": "0x5208", // 21000 gas
	})
}

func (rpc *RPCServer) handleGasPrice(w http.ResponseWriter, r *http.Request) {
	json.NewEncoder(w).Encode(map[string]string{
		"result": fmt.Sprintf("0x%x", BaseGasPrice),
	})
}

func (rpc *RPCServer) handleChainId(w http.ResponseWriter, r *http.Request) {
	json.NewEncoder(w).Encode(map[string]string{
		"result": fmt.Sprintf("0x%x", ZKKaiChainID),
	})
}

func (rpc *RPCServer) handleGetLogs(w http.ResponseWriter, r *http.Request) {
	// Simplified logs implementation
	json.NewEncoder(w).Encode(map[string]interface{}{
		"result": []interface{}{},
	})
}

func (rpc *RPCServer) handleSendShieldedTransaction(w http.ResponseWriter, r *http.Request) {
	var stx ShieldedTransaction
	
	if err := json.NewDecoder(r.Body).Decode(&stx); err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}
	
	if err := rpc.blockchain.ProcessShieldedTransaction(&stx); err != nil {
		json.NewEncoder(w).Encode(map[string]interface{}{
			"error": err.Error(),
		})
		return
	}
	
	json.NewEncoder(w).Encode(map[string]interface{}{
		"result": map[string]string{
			"commitment": stx.Commitment,
			"nullifier":  stx.Nullifier,
			"status":     "pending",
		},
	})
}

func (rpc *RPCServer) handleGetProofStatus(w http.ResponseWriter, r *http.Request) {
	var req struct {
		Nullifier string `json:"nullifier"`
	}
	
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}
	
	rpc.blockchain.ZKProver.mu.RLock()
	exists := rpc.blockchain.ZKProver.Nullifiers[req.Nullifier]
	rpc.blockchain.ZKProver.mu.RUnlock()
	
	status := "not_found"
	if exists {
		status = "verified"
	}
	
	json.NewEncoder(w).Encode(map[string]interface{}{
		"result": map[string]string{
			"nullifier": req.Nullifier,
			"status":    status,
		},
	})
}

func (rpc *RPCServer) handleGetNullifiers(w http.ResponseWriter, r *http.Request) {
	rpc.blockchain.ZKProver.mu.RLock()
	nullifiers := make([]string, 0, len(rpc.blockchain.ZKProver.Nullifiers))
	for n := range rpc.blockchain.ZKProver.Nullifiers {
		nullifiers = append(nullifiers, n)
	}
	rpc.blockchain.ZKProver.mu.RUnlock()
	
	json.NewEncoder(w).Encode(map[string]interface{}{
		"result": nullifiers,
	})
}

func (rpc *RPCServer) handleVerifyProof(w http.ResponseWriter, r *http.Request) {
	var stx ShieldedTransaction
	
	if err := json.NewDecoder(r.Body).Decode(&stx); err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}
	
	valid, err := rpc.blockchain.ZKProver.VerifyShieldedTransaction(&stx)
	
	json.NewEncoder(w).Encode(map[string]interface{}{
		"result": map[string]interface{}{
			"valid": valid,
			"error": func() string {
				if err != nil {
					return err.Error()
				}
				return ""
			}(),
		},
	})
}

func (rpc *RPCServer) handleGetShardInfo(w http.ResponseWriter, r *http.Request) {
	shardCount := atomic.LoadInt32(&rpc.blockchain.ShardManager.shardCount)
	
	shards := make([]map[string]interface{}, shardCount)
	for i := int32(0); i < shardCount; i++ {
		shard := rpc.blockchain.ShardManager.shards[i]
		shard.mu.RLock()
		shards[i] = map[string]interface{}{
			"id":       shard.ID,
			"txCount":  shard.TxCount,
			"load":     shard.Load,
			"gasUsed":  shard.GasUsed,
			"blocks":   len(shard.Blocks),
			"lastBlockTime": shard.LastBlockTime.Format(time.RFC3339),
		}
		shard.mu.RUnlock()
	}
	
	json.NewEncoder(w).Encode(map[string]interface{}{
		"result": map[string]interface{}{
			"shardCount": shardCount,
			"shards":     shards,
		},
	})
}

func (rpc *RPCServer) handleGetMetrics(w http.ResponseWriter, r *http.Request) {
	workerMetrics := rpc.blockchain.WorkerPool.GetMetrics()
	mempoolMetrics := rpc.blockchain.MemPool.GetMetrics()
	zkMetrics := rpc.blockchain.ZKProver.GetMetrics()
	shardMetrics := rpc.blockchain.ShardManager.GetMetrics()
	rollupMetrics := rpc.blockchain.RollupProcessor.GetMetrics()
	gasMetrics := rpc.blockchain.GasAbstraction.GetMetrics()
	
	json.NewEncoder(w).Encode(map[string]interface{}{
		"result": map[string]interface{}{
			"workerPool": map[string]interface{}{
				"tasksProcessed":   workerMetrics.TasksProcessed,
				"tasksQueued":      workerMetrics.TasksQueued,
				"activeWorkers":    workerMetrics.ActiveWorkers,
				"queueUtilization": workerMetrics.QueueUtilization,
				"scaleEvents":      workerMetrics.ScaleEvents,
			},
			"mempool": map[string]interface{}{
				"totalReceived": mempoolMetrics.TotalReceived,
				"totalRemoved":  mempoolMetrics.TotalRemoved,
				"currentSize":   mempoolMetrics.CurrentSize,
				"peakSize":      mempoolMetrics.PeakSize,
				"rejections":    mempoolMetrics.Rejections,
				"evictions":     mempoolMetrics.Evictions,
			},
			"zkProver": map[string]interface{}{
				"proofsGenerated": zkMetrics.ProofsGenerated,
				"proofsVerified":  zkMetrics.ProofsVerified,
				"proofTimeAvg":    zkMetrics.ProofTimeAvg.String(),
				"cacheHits":       zkMetrics.CacheHits,
				"cacheMisses":     zkMetrics.CacheMisses,
				"errors":          zkMetrics.Errors,
			},
			"sharding": map[string]interface{}{
				"totalShards":     shardMetrics.TotalShards,
				"activeShards":    shardMetrics.ActiveShards,
				"crossShardTxs":   shardMetrics.CrossShardTxs,
				"shardScalesUp":   shardMetrics.ShardScalesUp,
				"shardScalesDown": shardMetrics.ShardScalesDown,
				"avgShardLoad":    shardMetrics.AvgShardLoad,
			},
			"rollup": map[string]interface{}{
				"batchesProcessed":      rollupMetrics.BatchesProcessed,
				"transactionsInBatches": rollupMetrics.TransactionsInBatches,
				"averageBatchSize":      rollupMetrics.AverageBatchSize,
				"compressionRatio":      rollupMetrics.CompressionRatio,
				"proofGenerationTime":   rollupMetrics.ProofGenerationTime.String(),
			},
			"gasAbstraction": map[string]interface{}{
				"gaslessTxsProcessed": gasMetrics.GaslessTxsProcessed,
				"bundlesCreated":      gasMetrics.BundlesCreated,
				"bundlesCompleted":    gasMetrics.BundlesCompleted,
				"bundlesFailed":       gasMetrics.BundlesFailed,
				"freeTxsUsed":         gasMetrics.FreeTxsUsed,
				"sponsoredTxs":        gasMetrics.SponsoredTxs,
			},
			"blockchain": map[string]interface{}{
				"blockNumber": rpc.blockchain.CurrentBlock.Number.String(),
				"blockCount":  len(rpc.blockchain.Blocks),
				"chainId":     rpc.blockchain.ChainID.String(),
			},
		},
	})
}

func (rpc *RPCServer) handleCreateWallet(w http.ResponseWriter, r *http.Request) {
	walletManager := NewWalletManager(rpc.blockchain)
	wallet, err := walletManager.CreateWallet()
	if err != nil {
		json.NewEncoder(w).Encode(map[string]interface{}{
			"error": err.Error(),
		})
		return
	}
	
	json.NewEncoder(w).Encode(map[string]interface{}{
		"result": map[string]string{
			"address":   wallet.Address.Hex(),
			"publicKey": wallet.PublicKey,
		},
	})
}

func (rpc *RPCServer) handleGetWallet(w http.ResponseWriter, r *http.Request) {
	var req struct {
		Address string `json:"address"`
	}
	
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}
	
	walletManager := NewWalletManager(rpc.blockchain)
	wallet, err := walletManager.GetWallet(req.Address)
	if err != nil {
		json.NewEncoder(w).Encode(map[string]interface{}{
			"error": err.Error(),
		})
		return
	}
	
	json.NewEncoder(w).Encode(map[string]interface{}{
		"result": map[string]interface{}{
			"address": wallet.Address.Hex(),
			"balance": wallet.Balance.String(),
			"nonce":   wallet.Nonce,
		},
	})
}

func (rpc *RPCServer) handleWebSocket(w http.ResponseWriter, r *http.Request) {
	conn, err := rpc.upgrader.Upgrade(w, r, nil)
	if err != nil {
		log.Printf("WebSocket upgrade error: %v", err)
		return
	}
	
	rpc.mu.Lock()
	rpc.clients[conn] = true
	rpc.mu.Unlock()
	
	defer func() {
		rpc.mu.Lock()
		delete(rpc.clients, conn)
		rpc.mu.Unlock()
		conn.Close()
	}()
	
	for {
		var msg map[string]interface{}
		err := conn.ReadJSON(&msg)
		if err != nil {
			break
		}
		
		// Handle WebSocket messages
		if method, ok := msg["method"].(string); ok {
			switch method {
			case "subscribe":
				// Handle subscription requests
				rpc.handleWebSocketSubscribe(conn, msg)
			case "ping":
				// Respond to ping
				conn.WriteJSON(map[string]interface{}{
					"method": "pong",
					"id":     msg["id"],
				})
			default:
				conn.WriteJSON(map[string]interface{}{
					"error": "unknown method",
					"id":    msg["id"],
				})
			}
		}
	}
}

func (rpc *RPCServer) handleWebSocketSubscribe(conn *websocket.Conn, msg map[string]interface{}) {
	// Handle subscription logic
	params, _ := msg["params"].([]interface{})
	
	if len(params) > 0 {
		if topic, ok := params[0].(string); ok {
			// Subscribe to topic (newBlocks, newTransactions, etc.)
			conn.WriteJSON(map[string]interface{}{
				"method": "subscription",
				"id":     msg["id"],
				"result": fmt.Sprintf("subscribed to %s", topic),
			})
		}
	}
}

func (rpc *RPCServer) BroadcastNewTransaction(tx *Transaction) {
	rpc.mu.RLock()
	defer rpc.mu.RUnlock()
	
	message := map[string]interface{}{
		"method": "newTransaction",
		"params": tx,
	}
	
	for client := range rpc.clients {
		go func(client *websocket.Conn) {
			client.WriteJSON(message)
		}(client)
	}
}

func (rpc *RPCServer) BroadcastNewBlock(block *Block) {
	rpc.mu.RLock()
	defer rpc.mu.RUnlock()
	
	message := map[string]interface{}{
		"method": "newBlock",
		"params": block,
	}
	
	for client := range rpc.clients {
		go func(client *websocket.Conn) {
			client.WriteJSON(message)
		}(client)
	}
}

// ==================== MAIN FUNCTION ====================
func main() {
	// Configure runtime
	runtime.GOMAXPROCS(runtime.NumCPU())
	
	log.Println("Initializing zkKaia Blockchain...")
	log.Printf("Go version: %s", runtime.Version())
	log.Printf("CPU cores: %d", runtime.NumCPU())
	
	// Initialize blockchain
	blockchain, err := NewZKKaiBlockchain()
	if err != nil {
		log.Fatalf("Failed to initialize blockchain: %v", err)
	}
	
	log.Printf("zkKaia Blockchain initialized - Chain ID: %d", blockchain.ChainID.Int64())
	log.Printf("Genesis block: %s", blockchain.Blocks[0].Hash)
	
	// Start block producer
	blockchain.BlockProducer.Start()
	log.Println("Block producer started")
	
	// Initialize wallet manager
	walletManager := NewWalletManager(blockchain)
	log.Println("Wallet manager initialized")
	
	// Create demo wallet
	wallet, err := walletManager.CreateWallet()
	if err != nil {
		log.Fatalf("Failed to create wallet: %v", err)
	}
	log.Printf("Demo wallet created: %s", wallet.Address.Hex())
	
	// Fund demo wallet
	blockchain.StateDB.SetBalance(wallet.Address.Hex(), big.NewInt(1000000000000000000))
	log.Printf("Funded demo wallet with 1 ZKK")
	
	// Initialize RPC server
	rpcServer := NewRPCServer(blockchain)
	
	// Display chain configuration
	config := GetChainConfig()
	configJSON, _ := json.MarshalIndent(config, "", "  ")
	log.Printf("Chain Configuration:\n%s", string(configJSON))
	
	// Display startup information
	log.Println("Starting RPC server on port 8545...")
	log.Println("Endpoints available:")
	log.Println("  - HTTP: http://localhost:8545")
	log.Println("  - WebSocket: ws://localhost:8545/ws")
	log.Println("  - Health: http://localhost:8545/health")
	log.Println("\nScalability Features:")
	log.Printf("  - Dynamic Sharding: %d initial shards (max %d)", InitialShards, MaxShards)
	log.Printf("  - Adaptive Worker Pool: %d workers", WorkerPoolSize)
	log.Printf("  - Dynamic MemPool: %d initial capacity (max %d)", InitialMemPoolSize, MaxMemPoolSize)
	log.Println("  - Parallel Transaction Processing")
	log.Println("  - ZK Privacy with Groth16 Proofs")
	log.Println("  - Gas Abstraction & Sponsorship")
	log.Println("  - Rollup Batch Processing")
	log.Println("\nzkKaia is ready! ")
	
	// Start RPC server
	if err := rpcServer.Start("8545"); err != nil {
		log.Fatalf("RPC server failed: %v", err)
	}
}
